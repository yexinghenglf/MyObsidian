<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Study]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>Study</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Tue, 19 Mar 2024 12:41:47 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Tue, 19 Mar 2024 12:40:43 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[函数的定义]]></title><description><![CDATA[<a class="tag" href="?query=tag:scala" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scala</a> <a class="tag" href="?query=tag:面向对象" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#面向对象</a> 
 <br><a href=".?query=tag:scala" class="tag" target="_blank" rel="noopener">#scala</a> <a href=".?query=tag:面向对象" class="tag" target="_blank" rel="noopener">#面向对象</a> <br>
<br><a data-tooltip-position="top" aria-label="基本形式" data-href="#基本形式" href="\#基本形式" class="internal-link" target="_self" rel="noopener">基本形式</a>
<br><a data-tooltip-position="top" aria-label="带有多参数列表的⽅法" data-href="#带有多参数列表的⽅法" href="\#带有多参数列表的⽅法" class="internal-link" target="_self" rel="noopener">带有多参数列表的⽅法</a>
<br><br><a data-tooltip-position="top" aria-label="Scala_01-没有答案.pdf > page=59&amp;selection=6,2,6,4" data-href="Scala_01-没有答案.pdf#page=59&amp;selection=6,2,6,4" href="\大数据\PDF\Scala_01-没有答案.pdf#page=59&amp;selection=6,2,6,4" class="internal-link" target="_self" rel="noopener">Scala_01-没有答案, page 59</a><br><br>Scaladef 方法名称（参数列表）：返回值类型 = 方法体
def add(x: Int, y: Int): Int = x + y 
println(add(1, 2)) // 3 
//也可以定义成 //def add(x: Int, y: Int) = x + y 
//或者 
//def add(x: Int, y: Int){x + y}
//省略“=”时始终返回Unit
⼀定要⽤⼤括号把⽅法体括起来
复制<br><br>Scaladef addThenMultiply(x: Int, y: Int)(multiplier: Int):
Int = (x + y) * multiplier 
println(addThenMultiply(1, 2)(3)) // 
复制<br>Danger
空参⽅法可以作为最终表达式出现，实际上是⽅法调⽤，只不过Scala规定 空参⽅法的调⽤可以省略 ( ) 。但是⽆参⽅法不允许使⽤ ( ) 调⽤，建议当成 普通变量使⽤。
<br>小结

<br>⽅法的返回值类型可以不写，编译器可以⾃动推断出来，但是对于递 归函数，必须指定返回类型 
<br>⽅法的返回值默认是⽅法体中最后⼀⾏表达式 的值，当然也可以⽤ return来执⾏返回值，但不推荐这么做。 
<br>若使⽤return来指定函数的返回值，scala的类型推断将会失效，要 显式指定返回值类型。 
<br>⽅法也可以没有返回值（返回值是Unit）省略=即可

<br><br> val 变量 = (函数参数列表)=&gt;函数<br><br>Scalaval f1 = ((a: Int, b: Int) =&gt; a + b) 
val f2 = (a: Int, b: Int) =&gt; a + b val 
f3 = (_: Int) + (_: Int) val 
f4: (Int, Int) =&gt; Int = (_ + _)
复制<br><br>Scalaval f1:((Int,Int)=&gt;Int)={(x,y)=&gt;x+y} //表示式返回值为函数 
val f2:(Int,Int)=&gt;Int =(x,y)=&gt;x+y
复制<br><br>Scalaval f1 = new Function2[Int, Int, Int] { 
def apply(x: Int, y: Int): Int = if (x &lt; y) y else x
}
复制<br><br>Scala(x: Int) =&gt; x + 1 
var inc = (x:Int) =&gt; x+100 
//变量inc现在是⼀种可以像函数那样使⽤的函数 
var x = inc(7)-1 
//还可以如下定义具有多个参数的函数： 
var mul = (x: Int, y: Int) =&gt; x*y 
//变量mul现在是可以像函数那样使⽤的函数 
println(mul(3, 4)) 
//也可以定义不带参数的函数 
var userDir = () =&gt; { System.getProperty("user.dir") }
//变量userDir现在是可以像函数那样使⽤的函数 
println( userDir )
复制<br><br>Scalaobject FuncDemo3 {
//实现阶乘功能3！=3*2*1 4！=3！*4 
//通过定义⼀个递归实现
val factorial:Int=&gt;Int=(n)=&gt;{ 
if(n&lt;=1) 
1 
else 
n *factorial(n-1) 
} 
def main(args: Array[String]): Unit = {
println(factorial(10))
 } 

}
复制<br><br>Scalaval getTheAnswer = () =&gt; 42 
println(getTheAnswer()) // 42
复制<br><br>Danger
⽅法不能做为最终表达式出现，⽽函数可以
⽅法名是⽅法调⽤，⽽函数名只是代表函数对象本身
⽅法可以转换为函数，所以⼀般也不必严格区分 

<br>⾃动转换：在需要函数的地⽅，如果传递⼀个⽅法，Scala能够⾃动把⽅ 法转换为函数 
<br>⼿动转换

<br><a data-tooltip-position="top" aria-label="Scala_01-没有答案.pdf > page=66&amp;selection=139,0,161,4" data-href="Scala_01-没有答案.pdf#page=66&amp;selection=139,0,161,4" href="\大数据\PDF\Scala_01-没有答案.pdf#page=66&amp;selection=139,0,161,4" class="internal-link" target="_self" rel="noopener">Scala_01-没有答案, page 66</a>]]></description><link>大数据\scala\scala函数.html</link><guid isPermaLink="false">大数据/scala/scala函数.md</guid><pubDate>Mon, 16 Oct 2023 15:07:01 GMT</pubDate></item><item><title><![CDATA[scala集合]]></title><description><![CDATA[<a class="tag" href="?query=tag:scala" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scala</a> 
 <br><a href=".?query=tag:scala" class="tag" target="_blank" rel="noopener">#scala</a>
<a data-tooltip-position="top" aria-label="Scala_02-没有答案.pdf > page=2&amp;selection=14,0,14,1" data-href="Scala_02-没有答案.pdf#page=2&amp;selection=14,0,14,1" href="\大数据\PDF\Scala_02-没有答案.pdf#page=2&amp;selection=14,0,14,1" class="internal-link" target="_self" rel="noopener">Scala_02-没有答案, page 2</a><br>scala集合两个主要的包:<br>Scala# 不可变集合 
scala.collection.immutable (Scala默认采⽤不可变集合)
# 可变集合 
scala.collection.mutable
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310131745099.png" referrerpolicy="no-referrer"><br>粗线是系统推荐用的序列<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310131746490.png" referrerpolicy="no-referrer"><br><br>
Array
定⻓数组，数组不可扩容scala.Array
ArrayBuffer
🍕变⻓数组，数组可扩容 scala.collection.mutable.ArrayBuffe
<br><a data-tooltip-position="top" aria-label="Scala_02-没有答案.pdf > page=6&amp;selection=6,0,24,35" data-href="Scala_02-没有答案.pdf#page=6&amp;selection=6,0,24,35" href="\大数据\PDF\Scala_02-没有答案.pdf#page=6&amp;selection=6,0,24,35" class="internal-link" target="_self" rel="noopener">Scala_02-没有答案, page 6</a><br><br>Danger
使⽤new关键字创建⼀个定⻓数组
var arr=new Array[Int]（3）
直接使⽤Array创建并初始化⼀个数组，注意不再使⽤new关键字。
var arr=Array(1,2,3)
<br>扩展
为什么定⻓数组是可变集合？
Array本身不属于scala集合成员，从前⾯类继承图中也可发现这⼀点， 在可变集合图中IndexedSeq有⼀条虚线指向了Array，说明并不是直接继 承关系。
⼀般将Array归为集合是因为Scala默认将Array隐式转换为 WrappedArray，⽽WrappedArray实现了IndexedSeq特质。
从这⼀点上来说，String与WrappedString也有异曲同⼯之妙，可以发 现在不可变集合中，String与IndexedSeq也是虚线连接，也就是说在 Scala中，String可以当集合处理
<br><br>Scalaprintln("*****************创建变长数组*******************")
  val arr5 = ArrayBuffer[Int]()
  //添加元素
  arr5.append(2)
  arr5.append(4,4,5,6,7,8,9)
  arr5.insert(1,12)
  arr5 += 1 //末尾追加元素
  arr5 += (4,5,6,7,6)
  arr5 ++= Array(4,5,6) //添加另一个数组整体的元素要使用++=
  println(arr5)
  //删除
  arr5.remove(1)
  println("*****************数组练习*******************")
  /*
  现在有一个数组,元素:1-9
  1.对每个元素*2后输出
  2.将所有元素先求奇数,在*2输出
   */
  //1.对每个元素*2后输出
  for (elem &lt;- 1.to(9)) {
    println(elem*2)
  }
  val v1 = for (elem &lt;- 1 to 9) yield elem*2
  println(s"v1:${v1.toBuffer}")
  //2.将所有元素先求奇数,在*2输出
  val v2 = for (elem &lt;- 1 to 9; if elem % 2 != 0) yield elem*2
  println(s"v2:${v2.toBuffer}")
  println("*****************二维数组*******************")
  //第一种方式:不规则二维数组
  val arr6 = new Array[Array[Int]](10)
  arr6(0) = Array(1,2)
  arr6(1) = Array(3,4,5,6)
  //第二种方法
  val arr7 = Array.ofDim[Int](3,3)
  //遍历二维数组
  arr7.foreach(x=&gt;x.foreach(println(_)))
  println("*****************数组--高阶函数filter   map*******************")
  //filter:过滤
  //map:将原数组通过设置得到新的数组,但是元素个数与原数组一致
  //  1.对每个元素*2后输出
  val arr8 = Array(1,2,3,4,5,6,7)
  val v3 = arr8.map(_*2)
  println(s"v3:${v3.toBuffer}")
  //  2.将所有元素先求奇数,在*2输出
  val  v4:Array[Int] = arr8.filter(_ % 2 != 0).map(_*2)
  println(s"v4:${v4.toBuffer}")
  println("*****************数组常用方法*******************")
  val arr = Array(9,1,3,4,2,7,5,6,8)
  println(arr.sum) // 数组求和:45
  println(arr.max) // 数组求最大值:9
  println(arr.min) // 数组求最小值:1
  println(arr.sorted.toBuffer)// 数组排序:默认升序，返回一个新的数组
  println(arr.sortWith(_&lt;_).toBuffer)// 可以自定义排序规则，返回一个新的数组
复制<br><br>Scala Tuple表示固定元素的组合，元组可以装着多个不同类型的值，是不 同类型的值的聚集。Tuple是Scala中⾮常重要的⼀种数据结构，后⾯会⼤ 量使⽤。其特点包括：<br>重点
1.最多⽀持22个元素组合，分别对应类型Tuple1~Tuple2
2.相应也称为⼀ 元组（⼀般不⽤）、⼆元组、三元组... 2.元组可以容纳不同类型的元素
3.元组不可变
⼆元组可以表示Map中的⼀个元素，Map是K/V对偶的集合，对偶 是元组的最简单形式。
<br>访问组元：使⽤ 1 ， _2 ， _3 等形式访问组元，注意下标从1开始<br>Scalaobject Tuple04 extends  App {
  println("*************************创建元组***************************")
  val t1 = (1,"bing",20,100) //四元组
  val t2,(a1,a2,a3,a4) = (1,"bing",20,100)
  val t3 = new Tuple4[Int,String,Int,Double](1,"bing",20,100)

  //元素访问
  //下标
  println(t1.productElement(0))
  //元组的特定方法,通过_值,从1开始
  t1._4
  println("*************************遍历元组***************************")
  for (elem &lt;- t1.productIterator) {println(elem)}
  t1.productIterator.foreach(x=&gt;println(x))
  t1.productIterator.foreach(println(_))
  println("*************************元组拉链***************************")
  val arr1 = Array("hadoop","spark","flink")
  val arr2 = Array(1,2,3)
  val t4:Array[(String,Int)] = arr1.zip(arr2)
  t4.foreach(print(_))//(hadoop,1)(spark,2)(flink,3)
  println()
  //当拉链的两边元素个数不一样时,以少的为标准,多的删除
  val arr3 = Array(1,2,3,4,5)
  val t5:Array[(String,Int)] = arr1.zip(arr3)
  t5.foreach(print(_))
  println()
  val arr33 = Array(1,2)
  //这里的0是当生成的元组中0下标元素缺失补充的内容
  //这里的1是当生成的元组中1下标元素缺失补充的内容
  val t6:Array[(Any,Int)] = arr1.zipAll(arr33,0,1)
  // 
  t6.foreach(print(_))
  println()
  
  //直接跟数组的下标组成元组
  val t7:Array[(Any,Int)] = arr1.zipWithIndex
  //默认使用下标
  val t8:Array[(String,Int)] = arr1.zip(Stream from 2)
  //是从 2 下标开始
  t7.foreach(print(_))//(hadoop,0)(spark,1)(flink,2)
  println()
  t8.foreach(print(_))//(hadoop,2)(spark,3)(flink,4)
  println()
  println("*************************元组-unzip***************************")
  val t9:(Array[Any],Array[Int]) = t7.unzip
  t9._1.foreach(println(_))
  println("*************************创建元组***************************")

}
复制<br><br>在Scala中，把哈希表这种数据结构叫做映射。Scala中的Map存储的内容是 键值对(key-value)，Map区分可变Map (scala.collection.mutable.Map) 和 不可变Map(scala.collection.immutable.Map) 。不可变的Map（仅 TreeMap）⽀持有序，⽽可变的Map是⽆序的。<br>重点
Map中的元素为⼆元组，可⽤两种⽅式表示
<br>Scala("a",1)
"a" -&gt; 1
复制<br>Scalapackage scala03

import scala.collection.immutable.TreeMap

object Map01  extends  App{
  println("***************************构建Map*****************")
  //直接使用Map,默认是scala.collection.immutable.Map
  val m1 = Map("hadoop"-&gt;1,"spark"-&gt;20,"flink"-&gt;3)
  val m2 = Map(("hadoop",1),("spark",2),("flink",3))
  //构建可变的
  val m3 = scala.collection.mutable.Map(("hadoop",1),("spark",2),("flink",3))

  //取值
  println(m1("hadoop"))

  println("***************************遍历Map*****************")
  //通过keySet
  for (elem &lt;- m1.keySet) {println(s"key:$elem   value:${m1(elem)}")}
  //通过keys
  m1.keys.foreach(elem=&gt;println(s"key:$elem   value:${m1(elem)}"))
  //通过values
  m1.values.foreach(value=&gt;println(s"value:$value"))
  //直接遍历Map
  for (elem &lt;- m1) {println(s"key:${elem._1}   value:${elem._2}")}
  for ((k,v) &lt;- m1) {println(s"key:$k   value:$v")}

  println("***************************其他的Map*****************")
  println(s"m1:$m1")
  println(s"m2:$m2")
  println(s"m3:$m3")
  //只有不可变Map的TreeMap才会排序
  val m4 = TreeMap(("hadoop",1),("spark",2),("flink",3))
  println(s"m4:$m4")
  val m5 = scala.collection.mutable.TreeMap(("hadoop",1),("spark",2),("wlink",3))
  println(s"m5:$m5")
  println("***************************构建Map*****************")
  println(m1.getClass)//class scala.collection.immutable.Map$Map3---HashMap
  println(m4.getClass)//class scala.collection.immutable.TreeMap
  println(m3.getClass)//class scala.collection.mutable.HashMap
  println("***************************构建Map*****************")
  println("***************************构建Map*****************")
}

复制<br><br>HashMap是最常⽤的数据结构之⼀，查找和增删元素具有O(1) 的时间复杂度。同时HashMap也是默认的Map类型。<br>Scala//可变
Map scala&gt; var map4 = mutable.Map( ("A", 1), ("B", "北京"), ("C", 3) ) 
map4: scala.collection.mutable.Map[String,Any] = Map(A -&gt; 1, C -&gt; 3, B -&gt; 北京) 
scala&gt; map4.getClass 
res208: Class[_ &lt;: 
scala.collection.mutable.Map[String,Any]] = class scala.collection.mutable.HashMap 
//不可变
Map scala&gt; var map5 = Map( ("A", 1), ("B", "北京"), ("C", 3) ,("D",4),("E",5)) 
map5: scala.collection.immutable.Map[String,Any] = Map(E -&gt; 5, A -&gt; 1, B -&gt; 北京, C -&gt; 3, D -&gt; 4) 

scala&gt; map5.getClass 
res211: Class[_ &lt;: 
scala.collection.immutable.Map[String,Any]] = class scala.collection.immutable.HashMap$HashTrieMap
复制<br>可变HashMap操作示例:<br>Scalaimport scala.collection.mutable
object MutMapDemo extends App{
val map1 = new mutable.HashMap[String, Int]()
map1("spark") = 1
map1 += (("hadoop", 2))
map1.put("storm", 3) 
println(map1)
//从map中移除元素 
map1 -= "spark" 
map1.remove("hadoop") 
println(map1)
复制<br><br>返回按特定顺序排列的元素集合<br>Scalaimport scala.collection.immutable.TreeMap 
var tm = TreeMap(3 -&gt; 'x', 1 -&gt; 'x', 4 -&gt; 'x') 
tm += (2 -&gt; 'x') 
tm //Map(1 -&gt; x, 2 -&gt; x, 3 -&gt; x, 4 -&gt; x)
复制<br><br>Scala 列表类似于数组，它们所有元素的类型都相同，但是它们也有所不同：列表是不可变的，值⼀旦被定义了就不能改变，其次列表是链表结构，⽽数组不是<br>重点
1.列表中的元素类型必须相同。
2.列表是有序的。
3.列表是不可变的，内容及⻓度都不可变。
<br>Scala
object List05 extends App {
  println("********************创建List***************************")
  //空列表
  val list1 = Nil
  val list2 = List()

  val fruit = List("apple","orange","banana")
  println(fruit)
  val fruit1 = List("apple1","orange1","banana1")
  val fruit2:List[List[String]] = List(fruit,fruit1)
  println(fruit2)
  /*
  1.list是以Nil结尾
  2.默认从右到左结合
   */
  val fruit3 = "apple"::("orange"::("banana"::Nil))
  println(fruit3)
  val fruit4:List[String] = "apple"::"orange"::fruit
  println(fruit4)//List(apple, orange, apple, orange, banana)
  val fruit5:List[java.io.Serializable] = "apple"::"orange"::fruit::Nil//List(apple, orange, List(apple, orange, banana))
  println(fruit5)

  println("********************遍历List***************************")
  fruit.foreach(println(_))
  //转成字符串
  println(fruit.mkString(","))
  println("********************List--高阶函数***************************")
  //reduce  计算
  val l1 = List(1,2,3,4,5)
  //求和
  val v1 = l1.reduce(_+_)
  println(v1)
  //求差
  //从左往右运算
  val v2 = l1.reduce(_-_)//-13
  println(v2)
  //从左往右运算
  val v3 = l1.reduceLeft(_-_)//-13
  println(v3)
  //从右往左运算
  val v4 = l1.reduceRight(_-_)//3  (1-(2-(3-(4-5)))
  println(v4)
  //扩展
  def test(a:Int)(b:Int)(c:Int) ={}  //多参数列表函数--柯里化
  //fold
  //第一个参数表示与当前集合中元素同类型的一个值
  //第二个参数表示实际的运算规则
  val  v5 = l1.fold(10)((x,y)=&gt;x+y)//25
  println(v5)
  //map
  fruit.map(_.length).mkString(",").foreach(print(_))//5,6,6
  //List(e, l, p, p, a),List(e, g, n, a, r, o),List(a, n, a, n, a, b)
  fruit.map(_.toList.reverse).mkString(",").foreach(print(_))
  println()
  fruit.map(x=&gt;{
    val s = x.split("")
    (s(0),s(1),s(2))
  }).mkString(",").foreach(print(_))//(a,p,p),(o,r,a),(b,a,n)
  println()
  //返回的是从左到右找到的第一个符合条件的值
  List(6,2,4,6,8,3,5,7).find(_ % 2 == 0).foreach(println(_))
  //takewhile  从左到右取值,一直取到不符合条件终止
  //dropwhile  从左到右将符合条件的丢弃,一直取到不符合条件终止
  println("********************创建List***************************")
  println("********************创建List***************************")
}

复制<br><br>Scalapackage scala02
//求单词数量
object Wordcount06 extends App {
  val lists = List("App extends scala.AnyRef with scala.DelayedInit App extends scala.AnyRef with scala.DelayedInit","main should not be overridden App extends scala.AnyRef with scala.DelayedInit","the delayedInit mechanism will disappear")
  //1.完成单词的拆解

  //val lists = List("App","extends","scala.AnyRef","with","scala.DelayedInit","main","should","not","be","overridden")
  //第一种:使用flatten + map
  val words:List[Array[String]] = lists.map(_.split(" "))
  //words.foreach(x=&gt;println(x.toBuffer))
  //flattem   压平
  val words01:List[String] = words.flatten
  words01.mkString(",").foreach(x=&gt;print(x))
  //第二种:flatMap==flatten + map
  lists.flatMap(_.split(" "))
  //2.让数据形成kv的形式  app 1
  println()
  //(App,1),(extends,1),(scala.AnyRef,1),(with,1)
  val tuple:List[(String,Int)]  = words01.map((_,1))
  tuple.mkString(",").foreach(print(_))
  //3.根据key分组
  println()
  //scala.AnyRef -&gt; List((scala.AnyRef,1), (scala.AnyRef,1), (scala.AnyRef,1))
  val groups:Map[String,List[(String,Int)]] = tuple.groupBy(_._1)
  groups.mkString(",").foreach(print(_))

  //4.对值内的元素数量进行求和---mapValues(直接对值操作)
  println()
  //scala.AnyRef -&gt; 3
  val temp:Map[String,Int] = groups.mapValues(_.size)
  temp.mkString(",").foreach(print(_))

  //5.排序
  //scala中sortBy默认只能是升序,spark中才能灵活使用
  println()
  val sort1:List[(String,Int)] = temp.toList.sortBy(_._2).reverse
  sort1.mkString(",").foreach(print(_))

  //6.topN
  println()
  val three = sort1 take 3
  val three1 = sort1.takeRight(3)//取最右边的三个
  three.mkString(",").foreach(print(_))

  println()
  //简化版
  val three2 = lists.map(_.split(" ")).flatten.map((_,1)).groupBy(_._1).mapValues(_.size).toList.sortBy(_._2).reverse.take(3)
  three2.mkString(",").foreach(print(_))
}

复制<br><br>Scalapackage scala02
/**
 * 创建10个整数的随机列表，分别求奇数位与偶数位之和。
 *
 * 如：List(10, 9, 8, 7, 6, 5, 4, 3, 2, 1)：奇数位之各为10+8+6+4+2，偶数位之和为9+7+5+3+1
 *
 * 分析
 *
 * # 生成随机数集合
 * # 考虑使用zipWithIndex实现，注意index以0开始，变换使满足题意
 * # 拉链后按index奇偶分组，然后组内求和
 *
 * 关键代码
 */
object Example07 {
  def main(args: Array[String]): Unit = {
    //1.先得到10个随机数
    val mynum = for (elem &lt;- 1.to(10)) yield (Math.random()*100).toInt
    //2.操作
    //(12,0),(34,1),(45,2)
    //1 -&gt; Vector((71,1), (4,3), (89,5), (5,7), (74,9)),0 -&gt; Vector((9,0), (9,2), (62,4), (33,6), (12,8))
    //1 -&gt; 296,0 -&gt; 298
    val temp = mynum.zipWithIndex.groupBy(x=&gt;x._2 % 2).map(kvs=&gt;{(kvs._1,kvs._2.unzip._1.sum)}).foreach(x=&gt;{
      if (x._1 == 1)
        println(s"偶数列的和是${x._2}")
      else
        println(s"奇数列的和是${x._2}")
    })
    //temp.mkString(",").foreach(print(_))
  }

}
复制<br><br>Set中的元素不可重复，⽆序（TreeSet除外）。<br>Scalapackage scala03
/*
Set:无序,不可重复的
 */
object Set02 extends App {
  println("************************构建Set**********************")
  val s1 = Set(1,2,4,5,6,8,9,2,46,3)
  val s2 = Set(4,2,7,0,54,3,5,6,8,5)
  println(s"s1:$s1")
  println("***********************操作Set**********************")
  //交集--两个都有
  println(s1.&amp;(s2))//Set(5, 6, 2, 3, 8, 4)
  println(s1.intersect(s2))
  //差集--s1有,s2没有
  println(s1 &amp;~ s2)//Set(46, 1, 9)
  println(s1.diff(s2))
  //并集--s1和s2所有的元素,但是会去重
  println(s1 union(s2))//Set(0, 5, 46, 1, 6, 9, 2, 54, 7, 3, 8, 4)
  println("************************常用Set**********************")
  println(s1.max)
  println(s1.min)
  println(s1.head)
  println(s1.tail)
  println(s1.isEmpty)
  println(s1.count(_&gt;0))
  println("************************扩展Set**********************")
  //并行集合
  val result1 = (1 to 100000).map{case _ =&gt; Thread.currentThread().getName}.distinct
  val result2 = (1 to 100000).par.map{case _ =&gt; Thread.currentThread().getName}.distinct
  println(result1)//Vector(main)
  println(result2)//ParVector(scala-execution-context-global-11, scala-execution-context-global-12)
  println("************************构建Set**********************")
  println("************************构建Set**********************")
  println("************************构建Set**********************")

}
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310141003189.png" referrerpolicy="no-referrer"><br><br>重点
Scala集合有三个基本操作：
head返回集合第⼀个元素
tail返回⼀个集合，包含除了第⼀元素之外的其他元素
isEmpty在集合为空时返回true
]]></description><link>大数据\scala\scala集合.html</link><guid isPermaLink="false">大数据/scala/scala集合.md</guid><pubDate>Sat, 14 Oct 2023 02:04:00 GMT</pubDate><enclosure url="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310131745099.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310131745099.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Hadoop 3.3.6配置文件]]></title><description><![CDATA[<a class="tag" href="?query=tag:hdfs" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hdfs</a> 
 <br><br><a href=".?query=tag:hdfs" class="tag" target="_blank" rel="noopener">#hdfs</a>
 提示：这篇教程对于 linux 安装 Hadoop 要写完成前置配置： 服务器创建、固定 IP、防火墙关闭、Hadoop 用户创建、SSH 免密、JDK 部署等操作
例如：JDK 部署等操作   <br><br><br><br>在网上找了好久好久的3.3.6或者其他的配置文件教程，都不是很满意，于是就自己总结写了这一篇有关3.3.6的配置文件
先看我的文件地址与说明
jdk 文件路径与 hadoop 都是在 /export/server/
<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/3a34ff41cbf1407a9b8ac991d9e8fa9a.png" referrerpolicy="no-referrer"><br><br>提示：以下是本篇文章正文内容，下面案例可供参考<br><br>现在就正式开始： 通过 rz 命令上传
<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/a8e9a6842ad54c5c9434d9651568d082.png" referrerpolicy="no-referrer"><br>
<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/df10cdbf1ecc42a295e2b5b623874345.png" referrerpolicy="no-referrer">
解压缩安装包到
 /export/server/ 中 tar -zxvf hadoop-3.3.6.tar.gz -C /export/server
<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/b3e851ac5dbc48808299b769c81391d4.png" referrerpolicy="no-referrer">
创建软链接方便后续配置操作<br>Linuxcd /export/server
ln -s /export/server/hadoop-3.3.6 hadoop
复制<br><br>接下来就是真正的开始配置：先cd<br>Linuxcd hadoop/etc/hadoop/
复制<br><br> 这里根据自己的主机来 我是 One Two Three<br>Linux主机1
主机2
主机3
复制<br> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/ad2b48bbdcbb4b73abc3a776e2f758eb.png" referrerpolicy="no-referrer"><br><br>文件环境变量更具自己的来<br>Linuxexport JAVA_HOME=/export/server/jdk
export HADOOP_HOME=/export/server/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_LOG_DIR=$HADOOP_HOME/logs
复制<br><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/8ebdf2086175455fa47f2e9eeaf830b7.png" referrerpolicy="no-referrer"> <br><br>注意下面要改自己的主机名<br>Linux&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;fs.defaultFS&lt;/name&gt;
&lt;value&gt;hdfs://自己的主机1:8020&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;io.file.buffer.size&lt;/name&gt;
&lt;value&gt;131072&lt;/value&gt;             
&lt;/property&gt;
&lt;/configuration&gt;
复制<br><br>注意下面要改自己的主机名<br>Linux&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
&lt;value&gt;0.0.0.0:9870&lt;/value&gt;                                       
&lt;description&gt; The address and the base port where the dfs namenode web ui will listen on.  
&lt;/description&gt; 
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt;
&lt;value&gt;700&lt;/value&gt; 
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
&lt;value&gt;/data/nn&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.hosts&lt;/name&gt;
&lt;value&gt;主机1，主机2，主机3&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.blocksize&lt;/name&gt;
&lt;value&gt;268435456&lt;/value&gt; 
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;
&lt;value&gt;100&lt;/value&gt; 
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
&lt;value&gt;/data/dn&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
复制<br>因为在这里配置了：<br><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/77bc7b7f550c429e9cc75bc812091591.png" referrerpolicy="no-referrer"><br>
<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/f242d54fdd65455c8b2fdeb44b9ae782.png" referrerpolicy="no-referrer">现在要创建这两个目录，上一步退出就可以直接创建，主机 123 三个都要分别创建。主机23 先创建后面分发直接用：
<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/9ea4336c20e741b2a5f2a30e640c896a.png" referrerpolicy="no-referrer"><br><br>Linux&lt;configuration&gt; 
&lt;property&gt;
&lt;name&gt;mapreduce.framework.name&lt;/name&gt; 
&lt;value&gt;yarn&lt;/value&gt; 
&lt;/property&gt;
&lt;property&gt; 
&lt;name&gt;mapreduce.application.classpath&lt;/name&gt;
&lt;value&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
复制<br><br>Linux&lt;configuration&gt; 
&lt;property&gt; 
&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;                                  
&lt;/property&gt; 
&lt;property&gt;     
&lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt; &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;  
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;  
yarn.scheduler.minimum-allocation-mb
&lt;value&gt;2048&lt;/value&gt; 
&lt;/property&gt; 
&lt;/configuration&gt;
复制<br><br><br>先cd到hadoop的文件下<br>Linuxcd /export/server
 
scp -r hadoop-3.3.4 主机2:`pwd`/
 
scp -r hadoop-3.3.4 主机3:`pwd`/
复制<br><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/9c4929882611474b93efa022c6b24b96.png" referrerpolicy="no-referrer">   <br><br>主机2，3先cd在创建<br>Linuxcd /export/server/

ln -s /export/server/hadoop-3.3.6 hadoop
复制<br><br>在前置的java环境变量下配置Hadoop环境变量<br>Linuxvim /etc/profile

export HADOOP_HOME=/export/server/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin      
复制<br><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/71401cafefa24a64aac96f5254c133b6.png" referrerpolicy="no-referrer"><br><br>主机 123 都执行： 以 root 权限 给 hadoop 用户配置相关权限（没有 hadoop 用户先创建一个也要无密互联 ssh）<br>Linuxchown -R hadoop:hadoop /data
 
chown -R hadoop:hadoop /export
复制<br><br><br>Linux su - hadoop
复制<br><br>Linuxhdfs namenode -format
复制<br><br>Linux一键启动：
start-all.sh

分开启动：
start-hdfs.sh

start-yarn.sh
复制<br><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/cb8ead962568471a93c0e140a82bfba5.png" referrerpolicy="no-referrer"><br>
启动成功后有六个
<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/66d2e84d5b86478487b0d448b35c0a67.png" referrerpolicy="no-referrer"><br><br><br><br>Linux&lt; property&gt;
    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
    &lt;value&gt;0.0.0.0:9870&lt;/value&gt;   
    &lt;description&gt;
The address and the base port where the dfs namenode web ui will listen on.
    &lt;/description&gt;
&lt; /property&gt;   
复制<br><br>Linux&lt; property&gt;
	&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
	&lt;value&gt;0.0.0.0:8088&lt;/value&gt;   
&lt; /property&gt;
复制<br><br><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/17695a5444064f0b8fba9c8b11aac6dc.png" referrerpolicy="no-referrer">
<img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/48bed0c15ac4428081554f8281cf71b6.png" referrerpolicy="no-referrer">]]></description><link>大数据\Hadoop3.3.6.html</link><guid isPermaLink="false">大数据/Hadoop3.3.6.md</guid><pubDate>Mon, 16 Oct 2023 14:17:41 GMT</pubDate><enclosure url="https://img-blog.csdnimg.cn/3a34ff41cbf1407a9b8ac991d9e8fa9a.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/3a34ff41cbf1407a9b8ac991d9e8fa9a.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[HDFS]]></title><description><![CDATA[<a class="tag" href="?query=tag:hdfs" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hdfs</a> <a class="tag" href="?query=tag:yarn" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#yarn</a> <a class="tag" href="?query=tag:hive" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hive</a> <a class="tag" href="?query=tag:数据库" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#数据库</a> <a class="tag" href="?query=tag:zookeeper" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#zookeeper</a> <a class="tag" href="?query=tag:mapreduce" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#mapreduce</a> 
 <br>
<br><a data-tooltip-position="top" aria-label="1. 初识" data-href="#1. 初识" href="\#1._初识" class="internal-link" target="_self" rel="noopener">1. 初识</a>
<br><a data-tooltip-position="top" aria-label="1. 块" data-href="#1. 块" href="\#1._块" class="internal-link" target="_self" rel="noopener">1. 块</a>
<br><a data-tooltip-position="top" aria-label="1. 数据的读写流程" data-href="#1. 数据的读写流程" href="\#1._数据的读写流程" class="internal-link" target="_self" rel="noopener">1. 数据的读写流程</a>
<br><a data-tooltip-position="top" aria-label="1. 核心架构" data-href="#1. 核心架构" href="\#1._核心架构" class="internal-link" target="_self" rel="noopener">1. 核心架构</a>

<br><a data-tooltip-position="top" aria-label="1. 核心架构 > 1. YARN的架构有哪2个角色？" data-href="#1. 核心架构#1. YARN的架构有哪2个角色？" href="\#1._核心架构#1._YARN的架构有哪2个角色？" class="internal-link" target="_self" rel="noopener">1. YARN的架构有哪2个角色？</a>
<br><a data-tooltip-position="top" aria-label="1. 核心架构 > 2. 两个角色各自的功能是什么？" data-href="#1. 核心架构#2. 两个角色各自的功能是什么？" href="\#1._核心架构#2._两个角色各自的功能是什么？" class="internal-link" target="_self" rel="noopener">2. 两个角色各自的功能是什么？</a>
<br><a data-tooltip-position="top" aria-label="1. 核心架构 > 3. 什么是YARN的容器？" data-href="#1. 核心架构#3. 什么是YARN的容器？" href="\#1._核心架构#3._什么是YARN的容器？" class="internal-link" target="_self" rel="noopener">3. 什么是YARN的容器？</a>


<br><a data-tooltip-position="top" aria-label="2. 辅助架构" data-href="#2. 辅助架构" href="\#2._辅助架构" class="internal-link" target="_self" rel="noopener">2. 辅助架构</a>

<br><a data-tooltip-position="top" aria-label="2. 辅助架构 > 1. Web应用代理（Web Application Proxy)" data-href="#2. 辅助架构#1. Web应用代理（Web Application Proxy)" href="\#2._辅助架构#1._Web应用代理（Web_Application_Proxy)" class="internal-link" target="_self" rel="noopener">1. Web应用代理（Web Application Proxy)</a>
<br><a data-tooltip-position="top" aria-label="2. 辅助架构 > 2. JobHistoryServer历史服务器" data-href="#2. 辅助架构#2. JobHistoryServer历史服务器" href="\#2._辅助架构#2._JobHistoryServer历史服务器" class="internal-link" target="_self" rel="noopener">2. JobHistoryServer历史服务器</a>


<br><a data-tooltip-position="top" aria-label="测试" data-href="#测试" href="\#测试" class="internal-link" target="_self" rel="noopener">测试</a>

<br><a data-tooltip-position="top" aria-label="测试 > 1. 圆周率" data-href="#测试#1. 圆周率" href="\#测试#1._圆周率" class="internal-link" target="_self" rel="noopener">1. 圆周率</a>
<br><a data-tooltip-position="top" aria-label="测试 > 2. 统计单词" data-href="#测试#2. 统计单词" href="\#测试#2._统计单词" class="internal-link" target="_self" rel="noopener">2. 统计单词</a>
<br><a data-tooltip-position="top" aria-label="测试 > 3. 蒙特卡罗算法求PI的基础原理" data-href="#测试#3. 蒙特卡罗算法求PI的基础原理" href="\#测试#3._蒙特卡罗算法求PI的基础原理" class="internal-link" target="_self" rel="noopener">3. 蒙特卡罗算法求PI的基础原理</a>


<br><a data-tooltip-position="top" aria-label="Hive测试" data-href="#Hive测试" href="\#Hive测试" class="internal-link" target="_self" rel="noopener">Hive测试</a>
<br><a data-tooltip-position="top" aria-label="1. ZooKeeper是什么" data-href="#1. ZooKeeper是什么" href="\#1._ZooKeeper是什么" class="internal-link" target="_self" rel="noopener">1. ZooKeeper是什么</a>
<br><a data-tooltip-position="top" aria-label="2.  Zookeeper的特点" data-href="#2.  Zookeeper的特点" href="\#2.__Zookeeper的特点" class="internal-link" target="_self" rel="noopener">2.  Zookeeper的特点</a>
<br><a data-tooltip-position="top" aria-label="3. Zookeeper的数据模型" data-href="#3. Zookeeper的数据模型" href="\#3._Zookeeper的数据模型" class="internal-link" target="_self" rel="noopener">3. Zookeeper的数据模型</a>
<br><a data-tooltip-position="top" aria-label="MapReduce的核心思想" data-href="#MapReduce的核心思想" href="\#MapReduce的核心思想" class="internal-link" target="_self" rel="noopener">MapReduce的核心思想</a>
<br><a data-tooltip-position="top" aria-label="MapReduce的阶段分类" data-href="#MapReduce的阶段分类" href="\#MapReduce的阶段分类" class="internal-link" target="_self" rel="noopener">MapReduce的阶段分类</a>

<br><a data-tooltip-position="top" aria-label="MapReduce的阶段分类 > 第一阶段: Map:" data-href="#MapReduce的阶段分类#第一阶段: Map:" href="\#MapReduce的阶段分类#第一阶段:_Map:" class="internal-link" target="_self" rel="noopener">第一阶段: Map:</a>
<br><a data-tooltip-position="top" aria-label="MapReduce的阶段分类 > 第二阶段: Reduce" data-href="#MapReduce的阶段分类#第二阶段: Reduce" href="\#MapReduce的阶段分类#第二阶段:_Reduce" class="internal-link" target="_self" rel="noopener">第二阶段: Reduce</a>


<br><a data-tooltip-position="top" aria-label="MapReduce的编程规范" data-href="#MapReduce的编程规范" href="\#MapReduce的编程规范" class="internal-link" target="_self" rel="noopener">MapReduce的编程规范</a>
- <a data-tooltip-position="top" aria-label="第二阶段: Reduce > Mapper" data-href="#第二阶段: Reduce#Mapper" href="\#第二阶段:_Reduce#Mapper" class="internal-link" target="_self" rel="noopener">Mapper</a>
- <a data-tooltip-position="top" aria-label="第二阶段: Reduce > Reducer" data-href="#第二阶段: Reduce#Reducer" href="\#第二阶段:_Reduce#Reducer" class="internal-link" target="_self" rel="noopener">Reducer</a>
- <a data-tooltip-position="top" aria-label="第二阶段: Reduce > Driver" data-href="#第二阶段: Reduce#Driver" href="\#第二阶段:_Reduce#Driver" class="internal-link" target="_self" rel="noopener">Driver</a>
<br><br><a href=".?query=tag:hdfs" class="tag" target="_blank" rel="noopener">#hdfs</a> <br><br>HDFS只是分布式文件管理系统中的一种
可以理解为一份数据切割为多份
有多少份就要上传几次数据<br>缺点

但是增删查改不是很方便
不适合低延迟数据访问
无法高效存储大量小文件
不支持多用户写入及任意修改文件

<br><br>优点
支持大规模文件存储
简化系统设计
适合数据备份
<br><br>Tip<br>1.客户端向NameNode发起请求
2.NameNode审核权限、剩余空间后，满足条件允许写入，并告知客户端写入的DataNode地址
3.客户端向指定的DataNode发送数据包
4.被写入数据的DataNode同时完成数据副本的复制工作，将其接收的数据分发给其它DataNode
5.如上图，DataNode1复制给DataNode2,然后基于DataNode2复制给Datanode3和DataNode4
6.写入完成客户端通知NameNode,NameNode做元数据记录工作
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310101917054.png" referrerpolicy="no-referrer"><br><br>计算就是根据内容分析获取想要的数据
分布式 存储就是靠 数量 取胜
也就是：
分散 -&gt; 汇总模式
例如 我国的人口普查<br><br><a href=".?query=tag:yarn" class="tag" target="_blank" rel="noopener">#yarn</a>
<img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310111325477.png" referrerpolicy="no-referrer"><br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310081519119.png" referrerpolicy="no-referrer"><br><br><br>Node
主（Master):ResourceManager
从（5lave):NodeManager
<img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310081529279.png" referrerpolicy="no-referrer">
<br><br>Node
ResourceManager:管理、统筹并分配整个集群的资源
NodeManager:管理、分配单个服务器的资源，即创建管理容器，由容器提供资源供程序使用
<br><br>Node
容器（Container)是YARN的NodeManager在所属服务器上分配资源的手段
创建一个资源容器，即由NodeManager占用这部分资源
然后应用程序运行在NodeManager创建的这个容器内
应用程序无法突破容器的资源限制
<br><br><br>代理服务器默认集成在了ResourceManager中
也可以将其分离出来单独启动，如果要分离代理服务器<br>例如
警告用户正在访问一个不受信任的站点
剥离用户访问的Cookie等
<br><br>历史服务器的功能很简单：记录历史运行的程序的信息以及产生的日志并提供WEBUI站点供用户使用浏览器查看<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310081538282.png" referrerpolicy="no-referrer"><br>总结
YARN的架构有哪些角色
核心角色：ResourceManager和NodeManager
辅助角色：ProxyServer,保障WEBU访问的安全性
辅助角色：JobHistoryserver,记录历史程序运行信息和日志
<br><br><br>首先由 One 来启动
start-all.sh  或者 start-hdfs.sh  和  start-yarn.sh
以及启动我们需要的日志结点端口<br>Linuxmapred --daemon(start|stop) historyserver
yarn --daemon start timelineserver
复制<br>当结点都启动好一共8个
接下来就开始 运行<br>Linuxcd /export/server/hadoop/share/hadoop/mapreduce/
hadoop jar hadoop-mapreduce-examples-3.3.6.jar pi 3 1000
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310081738240.png" referrerpolicy="no-referrer"><br>Warning
参数pi表示要运行的Java类，这里表示运行jar包中的求pi程序<br>
参数3，表示设置几个map任务<br>
参数1000，表示模拟求PI的样本数（越大求的PI越准确，但是速度越慢）
<br><br>准备一份数据文件，并上传到 HDFS 中。<br>文件内容
hadoop it bigdata hello world
hello bigdata hdfs
it is hadoop hdfs
hdfs mapreduce yarn
hadoop yarn
<br>将上面内容保存到Linux中为words.txt文件，并上传到HDFS<br>Linuxhadoop fs -mkdir -p /input
hadoop fs -mkdir /output
hadoop fs -ls /
hadoop fs -put words.txt /input/
hadoop fs -ls /input/
hadoop jar /export/server/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount hdfs://One:8020/input hdfs://Two:8020/output/wc
复制<br>Warning
hdfs://Two:8020/output/wc
里面的 wc在开始的 HDFS 没有进行创建 否则会报错 
<br>执行完成后，可以查看HDFS上的输出结果<br>Linuxhadoop fs -ls /output/wc
hadoop fs -cat /output/wc/*
复制<br><img src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310081744467.png" referrerpolicy="no-referrer"><br><br>Monte Carlo算法的基本思想是：以模拟的”实验”形式、以大量随机样本的统计形式，来得到问题的求解。
比如，求圆周率，以数学的方式是非常复杂的，但是我们可以以简单的形式去求解<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310081927215.png" referrerpolicy="no-referrer">
如图，我们在正方形内，随机落点
统计落在1/4圆内的点和总点数量的比例即可得到1/4的PI
最终乘以4即可得到PI
比如，红色点的数量比全部点的数量，结果是0.765
那么乘以四可以得到3.06.
3.06就是求得的PI
所以，此方法，需要大量的样本（落点）,样本越多越精准<br>Python代码实现如下<br>Python import random
 sample_num=int(input("请输入样本数："))
 inner_point=0
 pi=0
 for i in range(sample_num):
     a = random . uniform ( 0 , 1 )
     b=random.uniform(0,1)
     if a*a + b*b ≤ 1:
         inner_point=inner_point+1
 pi=(inner_point/sample_num)*4
 print(f'样本数：{sample_num},pi近似等于：{pi}')
复制<br><br><a href=".?query=tag:hive" class="tag" target="_blank" rel="noopener">#hive</a> <a href=".?query=tag:数据库" class="tag" target="_blank" rel="noopener">#数据库</a> <br>Tip
Apache Hive是一款分布式SQL计算的工具，其主要功能是：
将SQL语句翻译成MapReduce程序运行
<br>Success
使用Hive处理数据的好处
操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）
底层执行MapReduce,可以完成分布式海量数据的SQL处理
<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310081937944.png" referrerpolicy="no-referrer">
<img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310081938832.png" referrerpolicy="no-referrer"><br>元数据存储
通常是存储在关系数据库如mysql/derby中。Hive中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等）,表的数据所在目录等。
--Hive提供了Metastore服务进程提供元数据管理功能<br>Driver 驱动程序，包括语法解析器、计划编译器、优化器、执行器
完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS中，并在随后有执行引擎调用执行。<br>用户接口
包括CLI、JDBC/ODBC、WebGUI。其中，CLI(command line interface)为shell命令行；Hive中的Thrift服务器允许外部客户端通过网络与Hive进行交互，类似于JDBC或ODBC协议。WebGUI是通过浏览器访问Hive。
--Hive提供了 Hive Shell、ThriftServer等服务进程向用户提供操作接口<br>Danger
 nohup bin/hive --service metastore &gt;&gt; logs/metastore.log 2&gt;&amp;1 &amp;
<br><br>启动代码<br>Danger
一定要在hive里面启动
<br>Linuxstart-all.sh
nohup bin/hive --service metastore &gt;&gt; logs/metastore.log 2&gt;&amp;1 &amp;
nohup bin/hive --service hiveserver2 &gt;&gt; logs/hiveserver2.log 2&gt;&amp;1 &amp;
mapred --daemon start historyserver
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310082151727.png" referrerpolicy="no-referrer"><br>确保如上图<br>开始<br>Linuxcd /export/server/hive
bin /hive
复制<br>进入到Hive Shell环境中，可以直接执行SQL语句。  <br>创建表<br>LinuxCREATE TABLE test(id INT, name STRING, gender STRING);
show tables;
复制<br>插入数据<br>Linuxinsert into test values(1,'mm','nan');
INSERT INTO test VALUES(2,'王力红','男'),(3,'李华','女');
复制<br>执行非常非常慢！！！！！！！！！！！！！！！！！！！！！！！！！<br>查询数据<br>LinxuSELECT gender, COUNT(*) AS cnt FROM test GROUP BY gender;
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310082153097.png" referrerpolicy="no-referrer"><br>验证Hive的数据存储<br>
Hive的数据存储在HDFS的：/user/hive/warehouse中
<img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310082153824.png" referrerpolicy="no-referrer"><br><br><a href=".?query=tag:zookeeper" class="tag" target="_blank" rel="noopener">#zookeeper</a><br><br>Note

<br>zookeeper是⼀个为分布式应⽤程序提供的⼀个分布式开源协调服务框 架。是Google的Chubby的⼀个开源实现，是Hadoop和Hbase的重要组件。 主要⽤于解决分布式集群中应⽤系统的⼀致性问题。
<br>提供了基于类似Unix系统的⽬录节点树⽅式的数据存储。 
<br>可⽤于维护和监控存储的数据的状态的变化，通过监控这些数据状态的变 化，从⽽达到基于数据的集群管理 
<br>提供了⼀组原语(机器指令)，提供了java和c语⾔的接⼝

<br><br>Note

<br>也是⼀个分布式集群，⼀个领导者(leader),多个跟随者(follower). 
<br>集群中只要有半数以上的节点存活，Zookeeper集群就能正常服务。
<br>全局数据⼀致性：每个server保存⼀份相同的数据副本，client⽆论 连接到哪个server,数据都是⼀致的。 
<br>更新请求按顺序进⾏：来⾃同⼀个client的更新请求按其发送顺序依次 执⾏
<br>数据更新的原⼦性：⼀次数据的更新要么成功，要么失败 
<br>数据的实时性：在⼀定时间范围内，client能读到最新数据。

<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310102139647.png" referrerpolicy="no-referrer"><br><br><img src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310102139214.png" referrerpolicy="no-referrer"><br><br><a href=".?query=tag:mapreduce" class="tag" target="_blank" rel="noopener">#mapreduce</a>
MapReduce主要解决的是分布式文件存储系统上，数据的分布式计算的问题。在上述导读部分我们介绍过一个 WordCount的案例，就是一个非常典型的分布式计算的案例。如果我们将所有的需要处理的数据移动到一个节点 上进行处理，那么只是在数据传输的过程中就得消耗大量的时间，而且还可能在一台节点存不下这大量的数据。就 算是能够存储下，也能够接受数据移动所带来的时间消耗，集群中其他节点的计算资源也都是在闲置的，不能高效 率的利用集群。<br><br>Danger

<br>MapReduce设计的一个理念是“计算向数据靠拢”（移动计算），而不是“数据向计算靠拢”（移动数据） 
<br>将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，移动到有数据存储的集群节 点上，一是可以减少节点间的数据移动开销。二是在存储节点上可以并行计算，大大提高计算效率问题。
<br>MapReduce一个完整的运算分为Map和Reduce两个部分。Map会处理本节点的原始数据，产生的数据会临 时存储到本地磁盘。Reduce会跨节点fetch属于自己的数据，并进行处理，产生的数据会存储到HDFS上。

<br><img src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310111509411.png" referrerpolicy="no-referrer"><br><br><br>第一阶段，也称之为Map阶段。这个阶段会有若干个MapTask实例，完全并行运行，互不相干。每个MapTask会 读取分析一个InputSplit(输入分片，简称分片)对应的原始数据。计算的结果数据会临时保存到所在节点的本地磁盘 里。
该阶段的编程模型中会有一个map函数需要开发人员重写，map函数的输入是一个&lt;key,value&gt;对,map函数的输出 也是一个&lt;key,value&gt;对,key和value的类型需要开发人员指定。参考下图：
复制<br><img src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310111513108.png" referrerpolicy="no-referrer"><br><br>第二阶段，也称为Reduce阶段。这个阶段会有若干个ReduceTask实例并发运行，互不相干。但是他们的数据依赖 于上一个阶段的所有maptask并发实例的输出。一个ReudceTask会从多个MapTask运行节点上fetch自己要处理的 分区数据。经过处理后，输出到HDFS上。 
该阶段的编程模型中有一个reduce函数需要开发人员重写，reduce函数的输入也是一个&lt;key，value&gt;对，reduce 函数的输出也是一个&lt;key，value&gt;对。这里要强调的是，reduce的输入其实就是map的输出，只不过map的输出 经过shuffle技术后变成了&lt;key,List&lt;Value&gt;&gt;而已。参考下图：
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310111514531.png" referrerpolicy="no-referrer"><br><br><br>Danger

<br>自定义类，继承 org.apache.hadoop.mapreduce.Mapper 类型 
<br>定义K1,V1,K2,V2的泛型（K1,V1是Mapper的输入数据类型，K2,V2是Mapper的输出数据类型） 
<br>重写map方法（处理逻辑）

<br><br>Danger

<br>自定义类，继承 org.apache.hadoop.mapreduce.Reducer 类型
<br>定义K2,V2,K3,V3的泛型（K2,V2是Reducer的输入数据类型，K3,V3是Reducer的输出数据类型） 
<br>重写reduce方法的处理逻辑

<br><br>Danger
 MapReduce的程序，需要进行执行之前的属性配置与任务的提交，这些操作都需要在Driver类中来完成
]]></description><link>大数据\HDFS.html</link><guid isPermaLink="false">大数据/HDFS.md</guid><pubDate>Mon, 16 Oct 2023 15:09:07 GMT</pubDate><enclosure url="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310101917054.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310101917054.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Hive SQL 语法大全]]></title><description><![CDATA[<a class="tag" href="?query=tag:hive" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hive</a> <a class="tag" href="?query=tag:数据库" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#数据库</a> 
 <br><br><a href=".?query=tag:hive" class="tag" target="_blank" rel="noopener">#hive</a>  <a href=".?query=tag:数据库" class="tag" target="_blank" rel="noopener">#数据库</a> <br><br>SqlCREATE DATABASE [IF NOT EXISTS] db_name [LOCATION] 'path';
SELECT expr, ... FROM tbl ORDER BY col_name [ASC | DESC]
(A | B | C)
复制<br>如上语法，在语法描述中出现：<br>
<br>
[]，表示可选，如上[LOCATION] 表示可写、可不写

<br>
|，表示或，如上ASC | DESC，表示二选一

<br>
...，表示序列，即未完结，如上SELECT expr, ... 表示在SELECT后可以跟多个expr（查询表达式），以逗号隔开

<br>
()，表示必填，如上(A | B | C)表示此处必填，填入内容在A、B、C中三选一

<br><br><br>SqlCREATE DATABASE [IF NOT EXISTS] db_name [LOCATION 'path'] [COMMENT database_comment];
复制<br>
<br>
IF NOT EXISTS，如存在同名数据库不执行任何操作，否则执行创建数据库操作

<br>
[LOCATION]，自定义数据库存储位置，如不填写，默认数据库在HDFS的路径为：/user/hive/warehouse

<br>
[COMMENT database_comment]，可选，数据库注释

<br><br>SqlDROP DATABASE [IF EXISTS] db_name [CASCADE];
复制<br>
<br>[IF EXISTS]，可选，如果存在此数据库执行删除，不存在不执行任何操作
<br>[CASCADE]，可选，级联删除，即数据库内存在表，使用CASCADE可以强制删除数据库
<br><br>SqlALTER DATABASE database_name SET LOCATION hdfs_path;
复制<br>不会在HDFS对数据库所在目录进行改名，只是修改location后，新创建的表在新的路径，旧的不变<br><br>SqlUSE db_name;
复制<br>
<br>选择数据库后，后续SQL操作基于当前选择的库执行
<br>如不使用use，默认在default库执行
<br>若想切换回使用default库<br>SqlUSE DEFAULT;
复制<br>查询当前USE的数据库<br>SqlSELECT current_database();
复制<br><br><br><br><br>SqlCREATE [EXTERNAL] TABLE tb_name
	(col_name col_type [COMMENT col_comment], ......)
	[COMMENT tb_comment]
	[PARTITIONED BY(col_name, col_type, ......)]
	[CLUSTERED BY(col_name, col_type, ......) INTO num BUCKETS]
	[ROW FORMAT DELIMITED FIELDS TERMINATED BY '']
	[LOCATION 'path']
复制<br>
<br>
[EXTERNAL]，外部表，需搭配

<br>
[ROW FORMAT DELIMITED FIELDS TERMINATED BY '']指定列分隔符

<br>
[LOCATION 'path']表数据路径

<br>
外部表示意
SqlCREATE EXTERNAL TABLE test_ext(id int) COMMENT 'external table' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LOCATION 'hdfs://node1:8020/tmp/test_ext';
复制



<br>
[COMMENT tb_comment]表注释，可选

<br>
[PARTITIONED BY(col_name, col_type, ......)]基于列分区
Sql-- 分区表示意
CREATE TABLE test_ext(id int) COMMENT 'partitioned table' PARTITION BY(year string, month string, day string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
复制

<br>
[CLUSTERED BY(col_name, col_type, ......)]基于列分桶
SqlCREATE TABLE course (c_id string,c_name string,t_id string) CLUSTERED BY(c_id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
复制

<br><br>SqlCREATE TABLE tbl_name LIKE other_tbl;
复制<br><br>SqlCREATE TABLE tbl_name AS SELECT ...;
复制<br><br>SqlDROP TABLE tbl;
复制<br><br>重命名<br>SqlALTER TABLE old RENAME TO new;
复制<br>修改属性<br>SqlALTER TABLE tbl SET TBLPROPERTIES(key=value);
-- 常用属性
("EXTERNAL"="TRUE") -- 内外部表，TRUE表示外部表
('comment' = new_comment) -- 修改表注释
-- 其余属性参见
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-listTableProperties
复制<br><br>创建分区表<br>Sql-- 分区表示意
CREATE TABLE test_ext(id int) COMMENT 'partitioned table' PARTITION BY(year string, month string, day string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
复制<br>添加分区<br>SqlALTER TABLE tablename ADD PARTITION (partition_key='partition_value', ......);
复制<br>修改分区值<br>SqlALTER TABLE tablename PARTITION (partition_key='old_partition_value') RENAME TO PARTITION (partition_key='new_partition_value');
复制<br>注意<br>只会在元数据中修改，不会同步修改HDFS路径吗，如：<br>
<br>原分区路径为：/user/hive/warehouse/test.db/test_table/month=201910，分区名：month='201910'
<br>将分区名修改为：201911后，分区所在路径不变，依旧是：/user/hive/warehouse/test.db/test_table/month=201910
<br>如果希望修改分区名后，同步修改HDFS的路径，并保证正常可用，需要：<br>
<br>在元数据库中：找到SDS表 -&gt; 找到LOCATION列 -&gt; 找到对应分区的路径记录进行修改

<br>如将记录的：/user/hive/warehouse/test.db/test_table/month=201910 修改为：/user/hive/warehouse/test.db/test_table/month=201911


<br>在HDFS中，同步修改文件夹名

<br>如将文件夹：/user/hive/warehouse/test.db/test_table/month=201910 修改为：/user/hive/warehouse/test.db/test_table/month=201911


<br>删除分区<br>SqlALTER TABLE tablename DROP PARTITION (partition_key='partition_value');
复制<br>
删除分区后，只是在元数据中删除，即删除元数据库中：

<br>PARTITION表
<br>SDS表

相关记录
分区所在的HDFS文件夹依旧保留
<br>加载数据<br>LOAD DATA<br>SqlLOAD DATA [LOCAL] INPATH 'path' INTO TABLE tbl PARTITION(partition_key='partition_value');
复制<br>INSERT SELECT<br>SqlINSERT (OVERWRITE | INTO) TABLE tbl PARTITION(partition_key='partition_value') SELECT ... FROM ...;
复制<br><br>建表<br>SqlCREATE TABLE course (c_id string,c_name string,t_id string) 
	[PARTITION(partition_key='partition_value')] 
	CLUSTERED BY(c_id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
复制<br>
<br>CLUSTERED BY(col) 指定分桶列
<br>INTO 3 BUCKETS，设定3个桶
<br>
分桶表需要开启：
set hive.enforce.bucketing=true;
设置自动匹配桶数量的reduces task数量
<br>数据加载<br>SqlINSERT (OVERWRITE | INTO) TABLE tbl 
	[PARTITION(partition_key='partition_value')] 
	SELECT ... FROM ... CLUSTER BY(col);
复制<br>分桶表无法使用LOAD DATA进行数据加载<br><br>LOAD DATA<br>将数据文件加载到表<br>SqlLOAD DATA [LOCAL] INPATH 'path' INTO TABLE tbl [PARTITION(partition_key='partition_value')];	-- 指定分区可选
复制<br>INSERT SELECT<br>将其它表数据，加载到目标表<br>SqlINSERT (OVERWRITE | INTO) TABLE tbl 
	[PARTITION(partition_key='partition_value')] 		-- 指定分区，可选
	SELECT ... FROM ... [CLUSTER BY(col)];				-- 指定分桶列，可选
复制<br><br>INSERT OVERWRITE SELECT<br>SqlINSERT OVERWRITE [LOCAL] DIRECTORY ‘path’ 				-- LOCAL可选，带LOCAL导出Linux本地，不带LOCAL导出到HDFS
	[ROW FORMAT DELIMITED FIELDS TERMINATED BY '']		-- 可选，自定义列分隔符
	SELECT ... FROM ...;
复制<br>bin/hive<br>
<br>bin/hive -e 'sql' &gt; export_file 将sql结果重定向到导出文件中
<br>bin/hive -f 'sql_script_file' &gt; export_file 将sql脚本执行的结果重定向到导出文件中
<br><br><br><br><br>Sqlcreate database itheima;
use itheima;
CREATE TABLE itheima.orders (
    orderId bigint COMMENT '订单id',
    orderNo string COMMENT '订单编号',
    shopId bigint COMMENT '门店id',
    userId bigint COMMENT '用户id',
    orderStatus tinyint COMMENT '订单状态 -3:用户拒收 -2:未付款的订单 -1：用户取消 0:待发货 1:配送中 2:用户确认收货',
    goodsMoney double COMMENT '商品金额',
    deliverMoney double COMMENT '运费',
    totalMoney double COMMENT '订单金额（包括运费）',
    realTotalMoney double COMMENT '实际订单金额（折扣后金额）',
    payType tinyint COMMENT '支付方式,0:未知;1:支付宝，2：微信;3、现金；4、其他',
    isPay tinyint COMMENT '是否支付 0:未支付 1:已支付',
    userName string COMMENT '收件人姓名',
    userAddress string COMMENT '收件人地址',
    userPhone string COMMENT '收件人电话',
    createTime timestamp COMMENT '下单时间',
    payTime timestamp COMMENT '支付时间',
    totalPayFee int COMMENT '总支付金额'
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

load data local inpath '/home/hadoop/itheima_orders.txt' into table itheima.orders;

CREATE TABLE itheima.users (
    userId int,
    loginName string,
    loginSecret int,
    loginPwd string,
    userSex tinyint,
    userName string,
    trueName string,
    brithday date,
    userPhoto string,
    userQQ string,
    userPhone string,
    userScore int,
    userTotalScore int,
    userFrom tinyint,
    userMoney double,
    lockMoney double,
    createTime timestamp,
    payPwd string,
    rechargeMoney double
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

load data local inpath '/home/hadoop/itheima_users.txt' into table itheima.users;

-- 查询全表数据
SELECT * FROM itheima.orders;

-- 查询单列信息
SELECT orderid, userid, totalmoney FROM itheima.orders o ;

-- 查询表有多少条数据
SELECT COUNT(*) FROM itheima.orders;

-- 过滤广东省的订单
SELECT * FROM itheima.orders WHERE useraddress LIKE '%广东%';

-- 找出广东省单笔营业额最大的订单
SELECT * FROM itheima.orders WHERE useraddress LIKE '%广东%'
ORDER BY totalmoney DESC LIMIT 1;

-- 统计未支付、已支付各自的人数
SELECT ispay, COUNT(*) FROM itheima.orders o GROUP BY ispay ;

-- 在已付款的订单中，统计每个用户最高的一笔消费金额
SELECT userid, MAX(totalmoney) FROM itheima.orders WHERE ispay = 1 GROUP BY userid;
-- 统计每个用户的平均订单消费额
SELECT userid, AVG(totalmoney) FROM itheima.orders GROUP BY userid;
-- 统计每个用户的平均订单消费额，并过滤大于10000的数据
SELECT userid, AVG(totalmoney) AS avg_money FROM itheima.orders GROUP BY userid HAVING avg_money &gt; 10000;

-- 订单表和用户表JOIN 找出用户username
SELECT o.orderid, o.userid, u.username FROM itheima.orders o JOIN itheima.users u ON o.userid = u.userid;
SELECT o.orderid, o.userid, u.username FROM itheima.orders o LEFT JOIN itheima.users u ON o.userid = u.userid;

复制<br><br><img alt="image-20230224234706719" src="https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2023/02/24/20230224234706.png" referrerpolicy="no-referrer"><br><img alt="image-20230224234719463" src="https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2023/02/24/20230224234719.png" referrerpolicy="no-referrer"><br><img alt="image-20230224234733895" src="https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2023/02/24/20230224234733.png" referrerpolicy="no-referrer"><br>Sql-- 查找广东省数据
SELECT * FROM itheima.orders WHERE useraddress RLIKE '.*广东.*';
-- 查找用户地址是：xx省 xx市 xx区
SELECT * FROM itheima.orders WHERE useraddress RLIKE '..省 ..市 ..区';
-- 查找用户姓为：张、王、邓
SELECT * FROM itheima.orders WHERE username RLIKE '[张王邓]\\S+';
-- 查找手机号符合：188****0*** 规则
SELECT * FROM itheima.orders WHERE userphone RLIKE '188\\S{4}0[0-9]{3}';
复制<br><br>SqlCREATE TABLE itheima.course(
c_id string, 
c_name string, 
t_id string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

LOAD DATA LOCAL INPATH '/home/hadoop/course.txt' INTO TABLE itheima.course;
-- 基础UNION
SELECT * FROM itheima.course WHERE t_id = '周杰轮'
	UNION
SELECT * FROM itheima.course WHERE t_id = '王力鸿';
-- 去重演示
SELECT * FROM itheima.course
	UNION
SELECT * FROM itheima.course;
-- 不去重
SELECT * FROM itheima.course
	UNION ALL
SELECT * FROM itheima.course;
-- UNION写在FROM中 UNION写在子查询中
SELECT t_id, COUNT(*) FROM 
(
	SELECT * FROM itheima.course WHERE t_id = '周杰轮'
		UNION ALL
	SELECT * FROM itheima.course WHERE t_id = '王力鸿' 
) AS u GROUP BY t_id;

-- 用于INSERT SELECT
INSERT OVERWRITE TABLE itheima.course2
SELECT * FROM itheima.course 
	UNION
SELECT * FROM itheima.course;
复制<br><br>Sql# 随机桶抽取， 分配桶是有规则的
# 可以按照列的hash取模分桶
# 按照完全随机分桶
-- 其它条件不变的话，每一次运行结果一致
select username, orderId, totalmoney FROM itheima.orders 
	tablesample(bucket 3 out of 10 on username);
	
-- 完全随机，每一次运行结果不同
select * from itheima.orders 
	tablesample(bucket 3 out of 10 on rand());
	


# 数据块抽取，按顺序抽取，每次条件不变，抽取结果不变
-- 抽取100条
select * from itheima.orders
	tablesample(100 rows);
	
-- 取1%数据
select * from itheima.orders
	tablesample(1 percent);
	
-- 取 1KB数据
select * from itheima.orders
	tablesample(1K);

复制<br><br>虚拟列是Hive内置的可以在查询语句中使用的特殊标记，可以查询数据本身的详细参数。<br>Hive目前可用3个虚拟列：<br>Shell- INPUT__FILE__NAME，显示数据行所在的具体文件
- BLOCK__OFFSET__INSIDE__FILE，显示数据行所在文件的偏移量
- ROW__OFFSET__INSIDE__BLOCK，显示数据所在HDFS块的偏移量
  此虚拟列需要设置：SET hive.exec.rowoffset=true 才可使用
复制<br>SqlSET hive.exec.rowoffset=true;

SELECT orderid, username, INPUT__FILE__NAME, BLOCK__OFFSET__INSIDE__FILE, ROW__OFFSET__INSIDE__BLOCK FROM itheima.orders;

SELECT *, BLOCK__OFFSET__INSIDE__FILE FROM itheima.orders WHERE BLOCK__OFFSET__INSIDE__FILE &lt; 1000;

SELECT orderid, username, INPUT__FILE__NAME, BLOCK__OFFSET__INSIDE__FILE, ROW__OFFSET__INSIDE__BLOCK FROM itheima.orders_bucket;

SELECT INPUT__FILE__NAME, COUNT(*) FROM itheima.orders_bucket GROUP BY INPUT__FILE__NAME;
复制<br><br>数值、集合、转换、日期函数<br>Sql-- 查看所有可用函数
show functions;
-- 查看函数使用方式
describe function extended count;
-- 数值函数
-- round 取整，设置小数精度
select round(3.1415926);		-- 取整(四舍五入)
select round(3.1415926, 4);		-- 设置小数精度4位(四舍五入)
-- 随机数
select rand();					-- 完全随机
select rand(3);					-- 设置随机数种子，设置种子后每次运行结果一致的
-- 绝对值
select abs(-3);
-- 求PI
select pi();

-- 集合函数
-- 求元素个数
select size(work_locations) from test_array;
select size(members) from test_map;
-- 取出map的全部key
select map_keys(members) from test_map;
-- 取出map的全部value
select map_values(members) from test_map;
-- 查询array内是否包含指定元素，是就返回True
select * from test_array where ARRAY_CONTAINS(work_locations, 'tianjin');
-- 排序
select *, sort_array(work_locations) from test_array;


-- 类型转换函数
-- 转二进制
select binary('hadoop');
-- 自由转换，类型转换失败报错或返回NULL
select cast('1' as bigint);

-- 日期函数
-- 当前时间戳
select current_timestamp();
-- 当前日期
select current_date();
-- 时间戳转日期
select to_date(current_timestamp());
-- 年月日季度等
select year('2020-01-11');
select month('2020-01-11');
select day('2020-01-11');
select quarter('2020-05-11');
select dayofmonth('2020-05-11');
select hour('2020-05-11 10:36:59');
select minute('2020-05-11 10:36:59');
select second('2020-05-11 10:36:59');
select weekofyear('2020-05-11 10:36:59');
-- 日期之间的天数
select datediff('2022-12-31', '2019-12-31');
-- 日期相加、相减
select date_add('2022-12-31', 5);
select date_sub('2022-12-31', 5);

复制<br><br>准备数据<br>Sql-- 创建数据库
create database db_msg;
-- 选择数据库
use db_msg;

-- 如果表已存在就删除
drop table if exists db_msg.tb_msg_source ;
-- 建表
create table db_msg.tb_msg_source(
    msg_time string comment "消息发送时间",
    sender_name string comment "发送人昵称",
    sender_account string comment "发送人账号",
    sender_sex string comment "发送人性别",
    sender_ip string comment "发送人ip地址",
    sender_os string comment "发送人操作系统",
    sender_phonetype string comment "发送人手机型号",
    sender_network string comment "发送人网络类型",
    sender_gps string comment "发送人的GPS定位",
    receiver_name string comment "接收人昵称",
    receiver_ip string comment "接收人IP",
    receiver_account string comment "接收人账号",
    receiver_os string comment "接收人操作系统",
    receiver_phonetype string comment "接收人手机型号",
    receiver_network string comment "接收人网络类型",
    receiver_gps string comment "接收人的GPS定位",
    receiver_sex string comment "接收人性别",
    msg_type string comment "消息类型",
    distance string comment "双方距离",
    message string comment "消息内容"
);

-- 上传数据到HDFS(Linux命令)
hadoop fs -mkdir -p /chatdemo/data
hadoop fs -put chat_data-30W.csv /chatdemo/data/

-- 加载数据到表中，基于HDFS加载
load data inpath '/chatdemo/data/chat_data-30W.csv' into table tb_msg_source;

-- 验证数据加载
select * from tb_msg_source tablesample(100 rows);
-- 验证一下表的数量
select count(*) from tb_msg_source;
复制<br>ETL清洗转换<br>Sqlcreate table db_msg.tb_msg_etl(
    msg_time string comment "消息发送时间",
    sender_name string comment "发送人昵称",
    sender_account string comment "发送人账号",
    sender_sex string comment "发送人性别",
    sender_ip string comment "发送人ip地址",
    sender_os string comment "发送人操作系统",
    sender_phonetype string comment "发送人手机型号",
    sender_network string comment "发送人网络类型",
    sender_gps string comment "发送人的GPS定位",
    receiver_name string comment "接收人昵称",
    receiver_ip string comment "接收人IP",
    receiver_account string comment "接收人账号",
    receiver_os string comment "接收人操作系统",
    receiver_phonetype string comment "接收人手机型号",
    receiver_network string comment "接收人网络类型",
    receiver_gps string comment "接收人的GPS定位",
    receiver_sex string comment "接收人性别",
    msg_type string comment "消息类型",
    distance string comment "双方距离",
    message string comment "消息内容",
    msg_day string comment "消息日",
    msg_hour string comment "消息小时",
    sender_lng double comment "经度",
    sender_lat double comment "纬度"
);

INSERT OVERWRITE TABLE db_msg.tb_msg_etl
SELECT 
	*, 
	DATE(msg_time) AS msg_day, 
	HOUR(msg_time) AS msg_hour, 
	SPLIT(sender_gps, ',')[0] AS sender_lng, 
	SPLIT(sender_gps, ',')[1] AS sender_lat
FROM db_msg.tb_msg_source
WHERE LENGTH(sender_gps) &gt; 0;
复制<br>指标计算<br>需求1<br>Sql--保存结果表
CREATE TABLE IF NOT EXISTS tb_rs_total_msg_cnt 
COMMENT "每日消息总量" AS 
SELECT 
    msg_day, 
    COUNT(*) AS total_msg_cnt 
FROM db_msg.tb_msg_etl 
GROUP BY msg_day;
复制<br>需求2<br>Sql--保存结果表
CREATE TABLE IF NOT EXISTS tb_rs_hour_msg_cnt 
COMMENT "每小时消息量趋势" AS  
SELECT  
    msg_hour, 
    COUNT(*) AS total_msg_cnt, 
    COUNT(DISTINCT sender_account) AS sender_user_cnt, 
    COUNT(DISTINCT receiver_account) AS receiver_user_cnt
FROM db_msg.tb_msg_etl GROUP BY msg_hour;
复制<br>需求3<br>SqlCREATE TABLE IF NOT EXISTS tb_rs_loc_cnt
COMMENT '今日各地区发送消息总量' AS 
SELECT 
    msg_day,  
    sender_lng, 
    sender_lat, 
    COUNT(*) AS total_msg_cnt 
FROM db_msg.tb_msg_etl
GROUP BY msg_day, sender_lng, sender_lat;
复制<br>需求4<br>Sql--保存结果表
CREATE TABLE IF NOT EXISTS tb_rs_user_cnt
COMMENT "今日发送消息人数、接受消息人数" AS
SELECT 
msg_day, 
COUNT(DISTINCT sender_account) AS sender_user_cnt, 
COUNT(DISTINCT receiver_account) AS receiver_user_cnt
FROM db_msg.tb_msg_etl
GROUP BY msg_day;
复制<br>需求5<br>Sql--保存结果表
CREATE TABLE IF NOT EXISTS db_msg.tb_rs_s_user_top10
COMMENT "发送消息条数最多的Top10用户" AS
SELECT 
    sender_name AS username, 
    COUNT(*) AS sender_msg_cnt 
FROM db_msg.tb_msg_etl 
GROUP BY sender_name 
ORDER BY sender_msg_cnt DESC 
LIMIT 10;
复制<br>需求6<br>SqlCREATE TABLE IF NOT EXISTS db_msg.tb_rs_r_user_top10
COMMENT "接收消息条数最多的Top10用户" AS
SELECT 
receiver_name AS username, 
COUNT(*) AS receiver_msg_cnt 
FROM db_msg.tb_msg_etl 
GROUP BY receiver_name 
ORDER BY receiver_msg_cnt DESC 
LIMIT 10;
复制<br>需求7<br>SqlCREATE TABLE IF NOT EXISTS db_msg.tb_rs_sender_phone
COMMENT "发送人的手机型号分布" AS
SELECT 
    sender_phonetype, 
    COUNT(sender_account) AS cnt 
FROM db_msg.tb_msg_etl 
GROUP BY sender_phonetype;
复制<br>需求8<br>Sql--保存结果表
CREATE TABLE IF NOT EXISTS db_msg.tb_rs_sender_os
COMMENT "发送人的OS分布" AS
SELECT
    sender_os, 
    COUNT(sender_account) AS cnt 
FROM db_msg.tb_msg_etl 
GROUP BY sender_os
复制<br><br>Sql-- 在Hive的MySQL元数据库中执行
use hive;

1).修改字段注释字符集

alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;
2).修改表注释字符集

alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;
3).修改分区表参数，以支持分区键能够用中文表示

alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;
alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;
4).修改索引注解

mysql&gt;alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;
复制]]></description><link>大数据\Hive-SQL语法大全.html</link><guid isPermaLink="false">大数据/Hive-SQL语法大全.md</guid><pubDate>Mon, 16 Oct 2023 15:08:41 GMT</pubDate><enclosure url="https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2023/02/24/20230224234706.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://image-set.oss-cn-zhangjiakou.aliyuncs.com/img-out/2023/02/24/20230224234706.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[日记模板]]></title><description><![CDATA[ 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>&lt;%
let url = '<a rel="noopener" class="external-link" href="https://www.tianqi.com/fuquan/'" target="_blank">https://www.tianqi.com/fuquan/'</a>
let res = await request({url: url,method: "GET"});
res = res.replace(/\n/g,'')
let dqwd = /(\d+)&lt;/b&gt;℃&lt;/i&gt;&lt;/p&gt;/.exec(res)
let tqwdfw = /(.?)&lt;/b&gt;(.?)&lt;/span&gt;/.exec(res)
let sdfxzwx = /(.?)&lt;/b&gt;(.?)&lt;/b&gt;(.?)&lt;/b&gt;&lt;/dd&gt;/.exec(res)
let kqzlrcrl = /<br>await fetch('<a rel="noopener" class="external-link" href="https://v1.hitokoto.cn/?c=d&amp;c=h&amp;c=i&amp;c=j'" target="_blank">https://v1.hitokoto.cn/?c=d&amp;c=h&amp;c=i&amp;c=j'</a>)
.then(response =&gt; response.json())
.then(data =&gt; {
一言 = data.hitokoto
来源 = data.from
作者 = data.from_who === null ? '佚名' : data.from_who
})
-%&gt;<br>一言
 &lt;% 一言 %&gt;  —— 《&lt;% 来源 %&gt;》 · &lt;% 作者 %&gt;
<br><br><br><br><br>&lt;&lt; <a data-href="<% tp.date.now(&quot;YYYY-MM-DD&quot;, -1) %>" href="\3C20tp.date.now(22YYYY-MM-DD22,20-1)203E" class="internal-link" target="_self" rel="noopener">&lt;% tp.date.now("YYYY-MM-DD", -1) %&gt;</a> | <a data-href="<% tp.date.now(&quot;YYYY-MM-DD&quot;, 1) %>" href="\3C20tp.date.now(22YYYY-MM-DD22,201)203E" class="internal-link" target="_self" rel="noopener">&lt;% tp.date.now("YYYY-MM-DD", 1) %&gt;</a> &gt;&gt;]]></description><link>模板\日记模板.html</link><guid isPermaLink="false">模板/日记模板.md</guid><pubDate>Thu, 11 Jan 2024 14:35:35 GMT</pubDate></item><item><title><![CDATA[ColorfulClock]]></title><description><![CDATA[ 
 <br>Noneconst [date, setDate] = useState(new Date)

useEffect(()=&gt;{
	const timerId = setInterval(()=&gt;{
		setDate(new Date)
	},1000)
	return ()=&gt;{clearInterval(timerId)}
})

//moment.locale('en-us');
moment.locale('zh-cn');
let formatDate = moment().format("dddd-MMMM-D-H-mm-ss-a").split("-")
let secProgress = formatDate[5] / 60
let minProgress = (formatDate[4]) / 60
let hourProgress = (formatDate[3]) / 24
let dayProgress = (formatDate[2]) / 31
//console.log(formatDate[2]/31)

return (
&lt;div id="clock" className="progress-clock"&gt;
	&lt;button className="progress-clock__time-date" data-group="d" type="button"&gt;
		&lt;small data-unit="w"&gt;{formatDate[0]}&lt;/small&gt;&lt;br/&gt;
		&lt;span data-unit="mo"&gt;{formatDate[1]}&lt;/span&gt;
		&lt;span data-unit="d"&gt;{formatDate[2]}&lt;/span&gt;
	&lt;/button&gt;
	&lt;button className="progress-clock__time-digit" data-unit="h" data-group="h" type="button"&gt;{formatDate[3]}&lt;/button&gt;&lt;span className="progress-clock__time-colon"&gt;:&lt;/span&gt;&lt;button className="progress-clock__time-digit" data-unit="m" data-group="m" type="button"&gt;{formatDate[4]}&lt;/button&gt;&lt;span className="progress-clock__time-colon"&gt;:&lt;/span&gt;&lt;button className="progress-clock__time-digit" data-unit="s" data-group="s" type="button"&gt;{formatDate[5]}&lt;/button&gt;
	&lt;span className="progress-clock__time-ampm" data-unit="ap"&gt;{formatDate[6]}&lt;/span&gt;
	&lt;svg className="progress-clock__rings" width="256" height="256" viewBox="0 0 256 256"&gt;
		&lt;defs&gt;
			&lt;linearGradient id="pc-red" x1="1" y1="0.5" x2="0" y2="0.5"&gt;
				&lt;stop offset="0%" stopColor="hsl(343,90%,55%)" /&gt;
				&lt;stop offset="100%" stopColor="hsl(323,90%,55%)" /&gt;
			&lt;/linearGradient&gt;
			&lt;linearGradient id="pc-yellow" x1="1" y1="0.5" x2="0" y2="0.5"&gt;
				&lt;stop offset="0%" stopColor="hsl(43,90%,55%)" /&gt;
				&lt;stop offset="100%" stopColor="hsl(23,90%,55%)" /&gt;
			&lt;/linearGradient&gt;
			&lt;linearGradient id="pc-blue" x1="1" y1="0.5" x2="0" y2="0.5"&gt;
				&lt;stop offset="0%" stopColor="hsl(223,90%,55%)" /&gt;
				&lt;stop offset="100%" stopColor="hsl(203,90%,55%)" /&gt;
			&lt;/linearGradient&gt;
			&lt;linearGradient id="pc-purple" x1="1" y1="0.5" x2="0" y2="0.5"&gt;
				&lt;stop offset="0%" stopColor="hsl(283,90%,55%)" /&gt;
				&lt;stop offset="100%" stopColor="hsl(263,90%,55%)" /&gt;
			&lt;/linearGradient&gt;
		&lt;/defs&gt;
		&lt;g data-units="d"&gt;
			&lt;circle className="progress-clock__ring" cx="128" cy="128" r="74" fill="none" opacity="0.1" stroke="#e13e78" strokeWidth="12" /&gt;
			&lt;circle className="progress-clock__ring-fill" data-ring="mo" cx="128" cy="128" r="74" fill="none" stroke="#e13e78" strokeWidth="12" strokeDasharray="465 465" strokeDashoffset={(1-dayProgress)*465} strokeLinecap="round" transform="rotate(-90,128,128)" /&gt;
		&lt;/g&gt;
		&lt;g data-units="h"&gt;
			&lt;circle className="progress-clock__ring" cx="128" cy="128" r="90" fill="none" opacity="0.1" stroke="#e79742" strokeWidth="12" /&gt;
			&lt;circle className="progress-clock__ring-fill" data-ring="d" cx="128" cy="128" r="90" fill="none" stroke="#e79742" strokeWidth="12" strokeDasharray="565.5 565.5" strokeDashoffset={(1-hourProgress)*565.5} strokeLinecap="round" transform="rotate(-90,128,128)" /&gt;
		&lt;/g&gt;
		&lt;g data-units="m"&gt;
			&lt;circle className="progress-clock__ring" cx="128" cy="128" r="106" fill="none" opacity="0.1" stroke="#4483ec" strokeWidth="12" /&gt;
			&lt;circle className="progress-clock__ring-fill" data-ring="h" cx="128" cy="128" r="106" fill="none" stroke="#4483ec" strokeWidth="12" strokeDasharray="666 666" strokeDashoffset={(1-minProgress)*666} strokeLinecap="round" transform="rotate(-90,128,128)" /&gt;
		&lt;/g&gt;
		&lt;g data-units="s"&gt;
			&lt;circle className="progress-clock__ring" cx="128" cy="128" r="122" fill="none" opacity="0.1" stroke="#8f30eb" strokeWidth="12" /&gt;
			&lt;circle className="progress-clock__ring-fill" data-ring="m" cx="128" cy="128" r="122" fill="none" stroke="#8f30eb" strokeWidth="12" strokeDasharray="766.5 766.5" strokeDashoffset={(1-secProgress)*766.5} strokeLinecap="round" transform="rotate(-90,128,128)" /&gt;
		&lt;/g&gt;
	&lt;/svg&gt;
&lt;/div&gt;

)
复制<br><br>]]></description><link>模板\ColorfulClock.html</link><guid isPermaLink="false">模板/ColorfulClock.md</guid><pubDate>Tue, 17 Oct 2023 13:49:09 GMT</pubDate></item><item><title><![CDATA[dataview模板]]></title><description><![CDATA[ 
 <br>(1.列出清单：从所有笔记中，列出某个标签为123的所有笔记）<br>Dataview: No results to show for list query.<br>(2.列出清单：从 示例文件 夹中，列出笔记名包含123(可复制一行，组合关键字筛选）的所有笔记）<br><br><a data-tooltip-position="top" aria-label="日记/2023-10-26.md" data-href="日记/2023-10-26.md" href="\日记\2023-10-26.html" class="internal-link" target="_self" rel="noopener">2023-10-26</a><br><a data-tooltip-position="top" aria-label="日记/2023-10-25.md" data-href="日记/2023-10-25.md" href="\日记\2023-10-25.html" class="internal-link" target="_self" rel="noopener">2023-10-25</a><br><a data-tooltip-position="top" aria-label="日记/2023-10-24.md" data-href="日记/2023-10-24.md" href="\日记\2023-10-24.html" class="internal-link" target="_self" rel="noopener">2023-10-24</a><br><a data-tooltip-position="top" aria-label="日记/2023-10-23.md" data-href="日记/2023-10-23.md" href="\日记\2023-10-23.html" class="internal-link" target="_self" rel="noopener">2023-10-23</a><br><a data-tooltip-position="top" aria-label="日记/2023-10-22.md" data-href="日记/2023-10-22.md" href="\日记\2023-10-22.html" class="internal-link" target="_self" rel="noopener">2023-10-22</a><br><a data-tooltip-position="top" aria-label="日记/2023-10-21.md" data-href="日记/2023-10-21.md" href="\日记\2023-10-21.html" class="internal-link" target="_self" rel="noopener">2023-10-21</a><br><a data-tooltip-position="top" aria-label="日记/2023-10-19.md" data-href="日记/2023-10-19.md" href="\日记\2023-10-19.html" class="internal-link" target="_self" rel="noopener">2023-10-19</a><br><a data-tooltip-position="top" aria-label="日记/2023-10-18.md" data-href="日记/2023-10-18.md" href="\日记\2023-10-18.html" class="internal-link" target="_self" rel="noopener">2023-10-18</a><br><a data-tooltip-position="top" aria-label="日记/2023-10-17.md" data-href="日记/2023-10-17.md" href="\日记\2023-10-17.html" class="internal-link" target="_self" rel="noopener">2023-10-17</a><br><a data-tooltip-position="top" aria-label="日记/2023-10-16.md" data-href="日记/2023-10-16.md" href="\日记\2023-10-16.html" class="internal-link" target="_self" rel="noopener">2023-10-16</a><br><a data-tooltip-position="top" aria-label="日记/2023-10-15.md" data-href="日记/2023-10-15.md" href="\日记\2023-10-15.html" class="internal-link" target="_self" rel="noopener">2023-10-15</a><br><a data-tooltip-position="top" aria-label="日记/2023-10-14.md" data-href="日记/2023-10-14.md" href="\日记\2023-10-14.html" class="internal-link" target="_self" rel="noopener">2023-10-14</a><br>(3.列出表格：从所有笔记中，列出某个标签为123的所有笔记）<br>Dataview: No results to show for table query.<br>&lt;&lt;<a data-tooltip-position="top" aria-label="Invalid date" data-href="Invalid date" href="\Invalid date" class="internal-link" target="_self" rel="noopener">回顾昨天</a>  <a data-tooltip-position="top" aria-label="Invalid date" data-href="Invalid date" href="\Invalid date" class="internal-link" target="_self" rel="noopener">|展望明天</a>&gt;&gt;<br>正在进行的任务<br>Dataview: No results to show for list query.<br>今天新建的笔记<br>Dataview: No results to show for list query.]]></description><link>模板\dataview模板.html</link><guid isPermaLink="false">模板/dataview模板.md</guid><pubDate>Wed, 18 Oct 2023 01:34:24 GMT</pubDate></item><item><title><![CDATA[MySQL数据库存储]]></title><description><![CDATA[<a class="tag" href="?query=tag:sql" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#sql</a> <a class="tag" href="?query=tag:python" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#python</a> 
 <br><br><a href=".?query=tag:sql" class="tag" target="_blank" rel="noopener">#sql</a> <a href=".?query=tag:python" class="tag" target="_blank" rel="noopener">#python</a><br><br>​		数据库, 是我们存储数据最常用的一种软件. 在之前的案例中, 我们都是把数据存储在文件里的. 这样存储的好处是比较方便. 简单. 但是这样存储数据的弊端也非常明显. 数据提取的时候相当繁琐. 尤其在存储了大量数据集后. 想提取一个人的名字可能都是非常困难的. 此时, 数据库就体现了它的用处. <br>​		数据库的本质就是一个能存储数据的软件. 我们把数据交给数据库后, 数据库会按照它自己的规则将数据存储在文件中. 区别于我们直接存储在文件里. 它的这种存储会非常的方便我们对数据进行提取和处理. 在今天的互联网如此发达的时代. 数据库已经是每一款软件作为数据持久化存储的必选方案. <br>​		数据库又分为关系型和非关系型数据库(NoSQL), 我们这节只聊关系型数据库. <br>​		常见的关系型数据库, <br>​				MySQL, MSSQL, Oracle,等等.....<br>​		总之, 目前市场上最认可的就是MySQL.所以我们学MySQL<br><br><br>先去mysql官网下载好安装包. (<a rel="noopener" class="external-link" href="https://dev.mysql.com/downloads/" target="_blank">https://dev.mysql.com/downloads/</a>)<br>“image-20210710114626664.png” could not be found.<br>“image-20210710114553371.png” could not be found.<br>“image-20210710114707734.png” could not be found.<br>“image-20210710114815804.png” could not be found.<br>“image-20210710114930711.png” could not be found.<br>“image-20210710115027365.png” could not be found.<br>“image-20210710115102924.png” could not be found.<br>“image-20210710115119749.png” could not be found.<br>接下来是MySQL的图形化界面工具.  推荐用navicat. 好多年了, 很好用. <br><br>“image-20210710144159210.png” could not be found.<br>“image-20210710144409473.png” could not be found.<br>“image-20210710144519309.png” could not be found.<br>“image-20210710144744253.png” could not be found.<br>哦了, 至此, Navicat可以操纵你的数据库了. <br><br><br>在所有关系型数据库中, 所有的数据都是以表格的形式进行存储的. 那表格应该如何进行设计呢? 其实这里蕴含了一个映射关系的. <br>比如, 我们想要存学生信息. 那我们先思考. 在你未来的规划中, 一个学生应该会有哪些数据存在? <br>学生: 学号(唯一标识), 姓名, 生日, 家庭住址, 电话号等信息. OK. 我们抛开数据库不谈. 想要存储这些数据, 表格创建起来的话应该是是这样的:<br>“image-20210712100401960.png” could not be found.<br>OK. 按照这个规则来看. 每一条数据对应的就是一个人的信息. <br><br>创建表有两种方案: <br>
<br>
用SQL语句创建表格
Sqlcreate table student(
	sno int(10) primary key auto_increment,
    sname varchar(50) not null, 
    sbirthday date not null,
    saddress varchar(255),
    sphone varchar(12),
    class_name varchar(50),
)
复制
数据类型:
​	int 整数
​	double小数
​	varchar  字符串
​	date  时间(年月日)
​	datetime 时间(年月日时分秒)
​	text 大文本
​	
约束条件:
​	primary key 主键, 全表唯一值. 就像学号. 身份证号. 能够唯一的确定一条数据
​	auto_increment 主键自增.
​	not null  不可以为空.
​	null  可以为空
​	default 设置默认值
​		

<br>
用Navicat图形化工具来创建
“image-20210712110029287.png” could not be found.
“image-20210712164027033.png” could not be found.
“image-20210712110221729.png” could not be found.

<br><br>
<br>
SQL方式(了解)
Python-- 添加一列
ALTER TABLE table_name
ADD COLUMN column_name datatype

-- 删除一列
ALTER TABLE table_name 
DROP COLUMN column_name

-- 修改一列的数据类型
ALTER TABLE table_name
MODIFY COLUMN column_name datatype

-- 表格重命名
ALTER TABLE table_name RENAME TO new_name;

复制

<br>
Navicat方式
“image-20210712143112984.png” could not be found.

<br><br><br>SqlINSERT INTO table_name(col1, col2, ccol3...) values (val1,val2,val3)
复制<br>Sql-- 添加学生信息
INTO STUDENT(sname, sbirthday, saddress, sage, class_name) values ('周杰伦', '2020-01-02', "北京市昌平区", 18, "二班");
复制<br>注意, 如果主键设置自增, 就不用处理主键了. mysql会自动的帮我们按照自然顺序进行逐一自增.<br><br>SqlDELETE FROM table_name where_clause
复制<br>Sql-- 删除学生信息
DELETE FROM STUDENT where sno = 1 ; 
复制<br><br>SqlUPDATE table_name SET col1 = val1, col2 = val2... where_clause
复制<br>Sql-- 修改学生信息
UPDATE STUDENT SET SNAME = '王力宏' where sno = 1;
复制<br>注意, 修改和删除数据的时候一定要带上where条件, 否则全表更新(删除). 小白一定要小心!!!!!<br><br><br>SqlSELECT *|col1, col2, col3 
FROM table_name 
where_clause
复制<br>Sql-- 全表查询
SELECT * FROM STUDENT;

-- 查询学生姓名, 年龄
SELECT sname, sage FROM STUDENT;

-- 查询学号是1的学生信息
select * from student where sno = 1;

-- 查询年龄大于20的学生信息
select * from student where sage &gt; 20;

-- 查询学生年龄大于20 小于40的信息(包含)
select * from student where sage &gt;= 20 and sage &lt;= 40;
select * from student where sage between 20 and 40 ;

-- 查询姓张的学生信息
-- 		_一位字符串
-- 		%多位字符串
select * from student where sname like '张%';
复制<br><br>如何查询每个班级学生的平均年龄? <br>我们先把数据扩充一下下<br>“image-20210712164529290.png” could not be found.<br>每个班级的平均年龄. 我们是不是需要先把班级与班级先分开.  每个班级自己内部进行计算.对吧. 此时, 我们需要的就是一个分组的操作. 此时需要用到group by语句<br>Sqlselect * from table_name group by col_name
复制<br>注意, 上方的sql是无法使用的.  sql要求分组后, 到底要做什么必须明确指出. 否则报错<br>那很容易呀, 我们分完组要计算每个班级的平均年龄. 平均数如何计算, 这就要用到聚合函数. sql中提供5种聚合函数, 分别是: avg(), sum(), min(), max(), count()<br>Sql-- 查询每一个班级的平均年龄
select avg(sage), class_name from STUDENT group by class_name;

-- 查询每个班级最小的年龄
select min(sage), class_name from STUDENT group by class_name;

-- 查询每个班的最大年龄
select max(sage), class_name from STUDENT group by class_name;

-- 查询每个班的学生数量
select count(*), class_name from STUDENT group by class_name;

-- 查询每个班级的年龄和
select sum(sage), class_name from STUDENT group by class_name;
复制<br>注意, 不要把没有放在group by的内容直接放在select中. 你想想. 按照班级来查询平均年龄, 你非要把某一个人的信息放在结果里. 是不合适的. <br><br>如果我们需要对聚合函数计算的结果进一步的筛选. 可以用having语句<br>Sql-- 查询平均年龄在15岁以上的班级信息
select avg(sage), class_name from student group by class_name having avg(sage) &gt; 15;
复制<br>having和where的区别:<br>
<br>
where, 在原始数据上进行的数据筛选. 

<br>
having, 在聚合函数计算后的结果进行筛选. 

<br><br>sql中使用order by语句对查询结果进行排序. <br>Sql-- 按照年龄从小到大查询学生信息
select * from student order by sage asc

-- 按照年龄从大到小查询学生信息
select * from student order by sage desc
复制<br><br>(从这里开始后面内容想了解就了解, 不想了解直接过. )<br>在实际使用中, 一个表格肯定是无法满足我们数据存储的. 比如, 在学生选课系统中. 我们就可以设计成以下表结构:<br>
<br>学生表: 学号, 姓名, 性别, 住址等...
<br>课程表: 课程编号, 课程名称, 授课教师等...
<br>学生课程-成绩表:  成绩表编号, 学号, 课程编号, 成绩
<br>在这样的表结构中:<br>​	优势: 每个表的结构相对明确. 不存在歧义.  数据保存完整, 没有冗余.
​	劣势: 新手不太好想. 想不通为什么要这样设计. 这里涉及到数据库表结构设计范式, 该模型属于第三范式(听过就行).<br>在该模型表结构中. 成绩表是非常重要的.  在成绩表中, 明确的说明了哪个学生的哪一门课程得了多少分. 它将两个原来毫不相关的表关联了起来. 建立了主外键关系.  <br>何为主外键关系: <br>​		把A表中的主键放在另一张表里作为普通字段使用,  但数据要求必须来自于A. 这个很好理解. 比如, 学生成绩表中的学生编号数据就必须来自于学生表. 否则该数据是无意义的. <br>注意, 以上结构只是为了讲解多表关系. 并非完整的学生选课系统表结构. <br>建表语句:<br>Sql-- 创建学生表, 课程表, 成绩表
-- 1. 学生表: 学号, 姓名, 性别, 住址等...
-- 2. 课程表: 课程编号, 课程名称, 授课教师等...
-- 3. 学生课程-成绩表:  成绩表编号, 学号, 课程编号, 成绩
create table stu(
	sid int primary key auto_increment,
	sname varchar(50) not null, 
	gender int(1),
	address varchar(255)
);

create table course(
	cid int primary key auto_increment,
	cname varchar(50) not null, 
	teacher varchar(50)
);

create table sc(
	sc_id int primary key auto_increment,
	s_id int, 
	c_id int,
	score int,
	CONSTRAINT FK_SC_STU_S_ID FOREIGN key(s_id) REFERENCES stu(sid),
	CONSTRAINT FK_SC_COURSE_C_ID FOREIGN key(c_id) REFERENCES course(cid)
);

复制<br><br>​	在where语句中可以进行另外的一个查询. <br>​	例如, 查询选择了"编程"这门课的学生<br>Sql-- 查询选择了"编程"这门课的学生
-- 先查询编程课程的编号
select cid from course where cname = '编程';
-- 根据cid可以去sc表查询出学生的id
select s_id from sc where c_id = 2;
-- 根据学生ID查询学生信息
select * from stu where sid in (1,2,3,4,5,6);

-- 把上面的sql穿起来 
select * from stu where sid in (
    select s_id from sc where c_id in (
        select cid from course where cname = '编程'
    )
);

-- 查询课程名称为“编程”，且分数低于60的学生姓名和分数
select stu.sname, sc.score from stu, sc where stu.sid = sc.s_id and sc.score &lt; 60 and sc.c_id in (
	select cid from course where cname = '编程'
)

复制<br><br>关联查询就是把多个表格通过join的方式合并在一起. 然后进行条件检索. <br>语法规则:<br>Sqlselect ... from A xxx join B on A.字段1 = b.字段2

表示:  A表和B表连接. 通过A表的字段1和b表的字段2进行连接. 通常on后面的都是主外键关系
复制<br><br>Sql-- 查询每门课程被选修的学生数
-- count(*)
-- group by cid

select c.cid,c.cname, count(*) from sc inner join course c on sc.c_id = c.cid group by c.cid, c.cname
复制<br><br>Sql-- 查询所有学生的选课情况
select s.sname, c.cname from stu s left join sc on s.sid= sc.s_id left join course c on sc.c_id = c.cid

-- 查询任何一门课程成绩在70分以上的姓名、课程名称和分数
-- score &gt; 70 sc
-- sname student
-- cname course
select s.sname, c.cname, sc.score from stu s inner join sc on s.sid = sc.s_id inner join course c on sc.c_id = c.cid
where sc.score &gt; 70
复制<br><br>python连接mysql可以用pymysql模块<br>Pythonpip install pymysql
复制<br>pymysql基本使用:<br>Pythonimport pymysql

# 链接数据库
conn = pymysql.connect(
    host='localhost',
    port=3306,
    user='root',
    password='test123456',
    database='spider_back'
)
# 创建游标
cursor = conn.cursor()
# 接下来就可以用游标去执行各种操作了
复制<br><br><br>Pythontry:
    cursor = conn.cursor()
    result = cursor.execute("insert into stu(sname, address, gender) values ('李嘉诚', '八宝山', 1)")
    print(cursor.lastrowid)  # 获取自增的ID值
    print(result)  # result是该sql会影响多少条数据
    conn.commit()  # 提交
except:
    conn.rollback()  # 回滚
复制<br><br>Pythontry:
    cursor = conn.cursor()
    result = cursor.execute("update stu set gender = 2 where sid = 12")
    print(result)  # result是该sql会影响多少条数据
    conn.commit()  # 提交
except:
    conn.rollback()  # 回滚
复制<br><br>Pythontry:
    cursor = conn.cursor()
    result = cursor.execute("delete from stu where sid = 12")
    print(result)  # result是该sql会影响多少条数据
    conn.commit()  # 提交
except:
    conn.rollback()  # 回滚
复制<br><br>Python# 查询
from pymysql.cursors import DictCursor
# cursor = conn.cursor(DictCursor)  # 使用字典游标. 查询出的结果自动保存在字典中
cursor = conn.cursor()  # 默认游标. 查询出的结果自动保存在元组中

sql = """
    select * from stu
"""
ret_num = cursor.execute(sql)
# result = cursor.fetchall()  # 获取全部结果
# result = cursor.fetchmany(5)  # 获取部分结果
result = cursor.fetchone()  # 获取单个结果
print(result)
result = cursor.fetchone()  # 获取单个结果, 可以连续获取
print(result)
复制<br>注意, 一个游标如果被拿空了. 则不能再次获取内容. <br><br>Python
import pymysql
from pymysql.cursors import DictCursor


class NoDataBaseException(Exception):
    pass


class DBHelper:
    def __init__(self, database=None, host="localhost", port=3306, username="root", password="test123456"):
        if database:
            self.conn = pymysql.connect(
                host=host,
                port=port,
                user=username,
                password=password,
                database=database
            )
        else:
            raise NoDataBaseException("没有提供正确的数据库")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        return self.conn.close()

    def _change(self, sql,  *args, isInsert=False):
        cursor = self.conn.cursor()
        try:
            rownum = cursor.execute(sql, args)
            self.conn.commit()
            if isInsert:
                return cursor.lastrowid
            else:
                return rownum
        except Exception as e:
            print("报错了", e)
            self.conn.rollback()
        finally:
            cursor.close()

    def insert(self, sql, *args):
        return self._change(sql, *args, isInsert=True)

    def update(self, sql, *args):
        return self._change(sql, *args)

    def delete(self, sql, *args):
        return self._change(sql, *args)

    def query_list(self, sql, *args):
        cursor = self.conn.cursor()
        try:
            cursor.execute(sql, args)
            result = cursor.fetchall()
            return result
        finally:
            cursor.close()

    def query_one(self, sql, *args):
        cursor = self.conn.cursor()
        try:
            cursor.execute(sql, args)
            result = cursor.fetchone()
            return result
        finally:
            cursor.close()


if __name__ == '__main__':
    with DBHelper("spider_back") as db:
        # result = db.query_list("select * from stu where gender=%s and address like %s", 1, "%北京%")
        # print(result)
        result = db.delete("delete from stu where sid = %s", 10)
        print(result)

复制<br>附: MySQL远程连接时, 必须对数据库做一个简短的配置<br>1, 打开mysql命令行. <br>2, 输入以下内容<br>MysqlGRANT ALL PRIVILEGES ON *.* TO '用户名'@'%' IDENTIFIED BY '密码' WITH GRANT OPTION;
FLUSH PRIVILEGES;
复制<br>“image-20210727172811954.png” could not be found.]]></description><link>爬虫\爬虫文档\06_MySQL数据存储.html</link><guid isPermaLink="false">爬虫/爬虫文档/06_MySQL数据存储.md</guid><pubDate>Tue, 17 Oct 2023 12:44:04 GMT</pubDate></item><item><title><![CDATA[面向对象]]></title><description><![CDATA[<a class="tag" href="?query=tag:python" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#python</a> <a class="tag" href="?query=tag:面向对象" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#面向对象</a> 
 <br><br><a href=".?query=tag:python" class="tag" target="_blank" rel="noopener">#python</a> <a href=".?query=tag:面向对象" class="tag" target="_blank" rel="noopener">#面向对象</a><br><br>
<br>
软件编程就是将我们的思维转变成计算机能够识别语言的一个过程
“面向对象.assets/image-20200110221611335.png” could not be found.

<br>
什么是面向过程？
自上而下顺序执行，逐步求精
其程序结构是按功能划分为若干个基本模块，这些模块形成一个树状结构；
各模块之间的关系尽可能简单，在功能上相对独立
每一模块内部均是由顺序、选择和循环三种基本结构组成
其模块化实现的具体方法是使用子程序
程序流程在写程序时就已决定

<br>
什么是面向对象？
把数据及对数据的操作方法放在一起，作为一个相互依存的整体——对象
对同类对象抽象出其共性，形成类
类中的大多数数据，只能用本类的方法进行处理
类通过一个简单的外部接口与外界发生关系，对象与对象之间通过消息进行通信
程序流程由用户在使用中决定

<br>
理解面向对象

<br>
面向对象是相对面向过程而言

<br>
面向对象和面向过程都是一种思想

<br>
面向过程
强调的是功能行为
关注的是解决问题需要哪些步骤

<br>
面向对象
将功能封装进对象，强调具备了功能的对象
关注的是解决问题需要哪些对象

<br>
面向对象是基于面向过程的



<br>
把🐘关冰箱
面向过程
“面向对象.assets/截屏2020-01-1022.19.20.png” could not be found.
面向对象：更加符合人民思考习惯的思想，从执行者变成了指挥者
“面向对象.assets/截屏2020-01-1022.19.24.png” could not be found.

<br>
面向对象的特点
是一种符合人们思考习惯的思想
可以将复杂的事情简单化
将程序员从执行者转换成了指挥者

<br>
完成需求时
a、先要去找具有所需的功能的对象来用
b、如果该对象不存在，那么创建一个具有所需功能的对象

<br>
类与对象的关系
使用计算机语言就是不断的在描述现实生活中的事物
Python中描述事物通过类的形式体现，类是具体事物的抽象，概念上的定义
对象即是该类事物实实在在存在的个体
“面向对象.assets/截屏2020-01-1022.23.25.png” could not be found.
“面向对象.assets/截屏2020-01-1022.23.32.png” could not be found.

<br><br>
<br>
设计类只关心3样东西

<br>
说明
类名：首字母大写，遵守标识符规则
属性：遵守标识符规则
行为(功能、方法、函数)：遵守标识符规则

<br>
设计类
人类
类名：Personl
属性：name age sex height weight
行为：搬砖、盖房子
复制
汽车类
类名：Car
属性：color kuanshi(款式) weight pailiang(排量)
行为：跑、拉东西
复制



<br>
创建类

<br>
格式
class 类名(父类名):
    属性&amp;行为
复制

<br>
说明
class：表明这是一个类
类名：类的名字
()：父类集合的开始和结束
父类名：父类的名字，定义的类继承自父类，可以不写，默认是object。object是所有类的直接或间接父类

<br>
代码
class Person(object):
    pass
class Person:
    pass
复制



<br>
<br>
创建存在属性与方法的类
"""
wife
属性: sex age  height  weight  name faceValue
行为：洗衣服  做饭    生小孩
"""
"""
husband
属性：sex age  height  weight  name faceValue
行为：吃饭   帮忙生小孩   买菜  赚钱
"""
class Wife:
    sex = "女" #类属性
    age = "20"
    height = 160
    name = "苍苍"
    faceValue = "很 高"
    #self 代表当前 的对象  不一定名字叫做self  标准来说 都叫self
    def washClothes(self):
        print("洗衣服")
    def cook(self):
        print("做饭")
    def papapa(self):
        print("生小孩")
zhangSan = Wife() #类的实例化  称为 对象
复制

<br><br>
<br>
格式
对象名 = 类名(参数列表)

<br>
说明
参数列表：为了后面给属性进行赋值的

<br>
实例化代码
per1 = Person()
print(per1)
per2 = Person()
print(per2)
复制

<br><br>
<br>
概述

<br>
在类地内部，使用 def 关键字来定义一个方法，与一般函数定义不同，类方法必须包含参数 self，且为第一个参数，self 代表的是类的实例，而非类

<br>
self 的名字并不是规定死的，也可以使用 this，但是最好还是按照约定使用 self。

<br>
这个self的参数 不需要传参的  它是代表 当前实例化后的对象



<br>
self 不是 python 关键字，我们把他换成arg也是可以正常执行
class Test:
    def prt(arg):
        print(arg)
        print(arg.__class__)
 
t = Test()
t.prt()
复制
下面代码可以验证
class A:
    name = "ss"
    def aa(self):
        print(self.__dict__)
        print(id(self)) #打印slef对象的存储地址
a = A()
a.name = 'a'
print(a.__dict__)
print(id(a)) #打印当前类实例的变量a的存储地址
复制

<br>
注意：

<br>self 就是当前类实例化的对象  哪个对象调用它  它就是哪个对象
<br>self不是必须的 但是作为标准规范 是必须叫self
<br>self 在类中创建对象  调用类中的属性与方法


<br><br>
<br>
格式
对象.属性名
对象.方法名(参数...)
复制

<br>
方法的返回值 return
方法里 如果 没有返回值 打印方法的时候  返回 None
如果有返回值得 方法  需要使用 print() 将其输出
class 类名：
	def 方法名():
        return "值"
复制

<br>
注意：
return 下方的代码不在执行  遇到return 就认为是结束了！

<br><br><br>
<br>
定义
class Person(object):
    # 定义类属性
    # 当多个类的对象的属性值需要相同时可以使用类属性
    name = "lilei"
    age = 18
    sex = "男"
复制

<br>
调用
可以通过类调用，也可以通过对象调用

<br>
给类绑定属性
​	类名.属性名 = 属性值

<br>
使用
# 实例化对象时，每一个对象的属性值都相同
per1 = Person()
per2 = Person()

#通过对象方法类属性   对象名.类属性名
print(per1.name, per1.age, per1.sex)
print(per2.name, per2.age, per2.sex)

# 修改对象属性值，如果对象属性不存在则变为增加对象属性
per1.name = "liudh"
per2.name = "daol"
print(per1.name, per2.name, Person.name)

print(hasattr(per1, 'name'))
print(setattr(per1, 'name', 'lucky'))
print(getattr(per1, 'name'))
复制

<br>
问题
使用类创建的所有对象的属性初始值都是一样的

<br><br>
<br>
概述：

<br>通过self以及实例化后对象所创建的属性为对象属性


<br>
通过类中定义以及类名称创建的属性为对象属性 

<br>对象属性只存在于当前对象中


<br>
对象属性只能由对象调用

<br>
实例对象绑定属性

<br>
通过类的self变量

<br>
直接给实例对象赋值


class Student(object):
    def __init__(self, name):
        self.name = name
s = Student('lucky')#方法一 通过类的self变量绑定属性
s.score = 90#方法二 直接赋值
复制

<br>
实例
class Person(object):
     # 这里的属性实际上属于类属性(用类名来调用)
    name = "person"
    def __init__(self, name):
        pass
        #对象属性
        self.name = name


print(Person.name)
per = Person("lucky")
#对象属性的优先级高于类属性
print(per.name)
#动态的给对象添加对象属性
per.age = 18  # 只针对于当前对象生效，对于类创建的其他对象没有作用
print(Person.name)
per2 = Person("lilei")
#print(per2.age)  #没有age属性


#删除对象中的name属性，在调用会使用到同名的类属性
del per.name
print(per.name)

#注意：以后千万不要将对象属性与类属性重名，因为对象属性会屏蔽掉类属性。但是当删除对象属性后，在使用又能使用类属性了。
复制

<br>
注意：

<br>使用对象创建的属性 只有当前对象存在该属性  称为对象属性
<br>使用类创建的属性 在每个对象里都会存在 称为类属性
<br>当在对象里去修改 类属性的值的时候 在当前的对象里面新创建了一个对象属性  当调用的时候 会优先调用 对象属性  而类属性本身的值 不会改变
<br>使用 类里面的方法创建的属性 也是对象属性  不能够使用类去调用
<br>当对象的属性删掉以后 还会调用 类属性
<br>不能再对象里去删除类的属性  AttributeError: 属性名
<br>对象属性的优先级高于类属性


<br><br>
<br>注意：

<br>通过self以及实例化后对象所创建的属性为对象属性
<br>通过类中定义以及类名称创建的属性为对象属性 
<br>类属性会存在每个实例化的对象中
<br>对象属性只存在于当前对象中
<br>类属性可以使用对象进行调用
<br>对象属性只能由对象调用


<br><br>
<br>
需求
类创建的多个对象的属性的初始值是不同的

<br>
_init__ 方法
定义
class Person(object):
    # 类属性
    country = "中国"

    # 构造方法：在使用类创建实例对象时自动调用，目的是初始化对象的内容
    def __init__(self, name, age, sex):
        # 定义对象属性
        # 只能对象调用
        self.name = name
        self.age = age
        self.sex = sex
复制

<br>
使用
# 创建对象时给每个对象的属性进行赋值
per1 = Person("daol", 45, "男")
per2 = Person("liudh", 55, "男")

print(per1.name)
print(per2.name)

# print(Person.name) # 类中没有name属性，且类无法调用对象属性
复制

<br><br><br>“面向对象.assets/11、继承.png” could not be found.<br>面向对象的编程带来的主要好处之一是代码的重用，实现这种重用的方法之一是通过继承机制。继承完全可以理解成类之间的类型和子类型关系。<br>在OOP程序设计中，当我们定义一个class的时候，可以从某个现有的class继承，新的class称为子类（Subclass），而被继承的class称为基类、父类或超类（Base class、Super class）。<br><br>class后面紧接着是类名，即Student，类名通常是大写开头的单词，紧接着是(object)，表示该类是从哪个类继承下来的，继承的概念我们后面再讲，通常，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类<br>单继承的实现<br>
<br>
格式：
class Student(object):
    pass
复制
Python 同样支持类的继承，如果一种语言不支持继承，类就没有什么意义。派生类的定义如下所示:
class DerivedClassName(BaseClassName1):
    &lt;statement-1&gt;
    .
    .
    .
    &lt;statement-N&gt;
复制

#类定义
class people:
    #定义基本属性
    name = ''
    age = 0
    #定义私有属性,私有属性在类外部无法直接进行访问
    __weight = 0
    #定义构造方法
    def __init__(self,n,a,w):
        self.name = n
        self.age = a
        self.__weight = w
    def speak(self):
        print("%s 说: 我 %d 岁。" %(self.name,self.age))
  #单继承示例
class student(people):
    grade = ''
    def __init__(self,n,a,w,g):
        #调用父类的构函
        people.__init__(self,n,a,w)
        self.grade = g
    #覆写父类的方法
    def speak(self):
        print("%s 说: 我 %d 岁了，我在读 %d 年级"%(self.name,self.age,self.grade))
复制
​

<br><br>
<br>
方式

<br>
方式1
super(Dog, self).saySomething()

<br>
方式2 [推荐]
super().saySomething()  

<br>
方式3
Animal.saySomething(self)



<br>


<br>
如果是单一继承  super() 会自动调用 父类的被覆盖的方法

<br>
使用super(类名,self) 会根据当前的类名 往右进行检索 父类相同的方法
这样做有一些缺点，比如说如果修改了父类名称，那么在子类中会涉及多处修改，另外，Python是允许多继承的语言，如上所示的方法在多继承时就需要重复写多次，显得累赘。为了解决这些问题，Python引入了super()机制



<br>
实例
#  方式1
super(类名, self).saySomething()
#  方式2 [推荐]
super().saySomething()
# 方式3
Animal.saySomething(self)
class Animal():
    def __init__(self, name):
        self.name = name
    def saySomething(self):
        print("I am " + self.name)

class Dog(Animal):
    def __init__(self, name):
        super().__init__(name)

    def saySomething(self):
        print("I am " + self.name + ", and I can bark")

    def animal_say_1(self):
        # 子类调用父类的方法
        #  方式1
        super(Dog, self).saySomething()

    def animal_say_2(self):
        #  方式2 [推荐]
        super().saySomething()

    def animal_say_3(self):
        # 方式3
        Animal.saySomething(self)

dog = Dog("Blake")
dog.saySomething()
dog.animal_say_1()
dog.animal_say_2()
dog.animal_say_3()
复制

<br><br>
<br>
目的：如果你的父类方法的功能不能满足你的需求，你可以在子类重写你父类的方法
class Parent:        # 定义父类
   def myMethod(self):
      print ('调用父类方法')
 
class Child(Parent): # 定义子类
   def myMethod(self):
      print ('调用子类方法')
 
c = Child()          # 子类实例
c.myMethod()         # 子类调用重写方法
复制

]]></description><link>爬虫\爬虫文档\面向对象.html</link><guid isPermaLink="false">爬虫/爬虫文档/面向对象.md</guid><pubDate>Mon, 16 Oct 2023 15:06:21 GMT</pubDate></item><item><title><![CDATA[beautifulsoup]]></title><description><![CDATA[<a class="tag" href="?query=tag:bs4" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#bs4</a> 
 <br><br><a href=".?query=tag:bs4" class="tag" target="_blank" rel="noopener">#bs4</a><br><br>简单来说，Beautiful Soup是python的一个库，最主要的功能是从网页抓取数据。官方解释如下：<br>Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。
它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。<br><br>pip install beautifulsoup4
复制<br>pip install beautifulsoup4 -i <a rel="noopener" class="external-link" href="http://pypi.douban.com/simple/" target="_blank">http://pypi.douban.com/simple/</a><br><br>Beautiful Soup支持Python标准库中的HTML解析器,还支持一些第三方的解析器，如果我们不安装它，则 Python 会使用 Python默认的解析器，lxml 解析器更加强大，速度更快，推荐安装。<br>Pythonpip install lxml
复制<br><br><a data-tooltip-position="top" aria-label="http://beautifulsoup.readthedocs.io/zh_CN/latest/" rel="noopener" class="external-link" href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank">官网文档</a><br><br>下面的一段HTML代码将作为例子被多次用到.这是 爱丽丝梦游仙境的 的一段内容(以后内容中简称为 爱丽丝 的文档):<br>Pythonhtml_doc = """
&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;

&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were
&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,
&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and
&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;
and they lived at the bottom of a well.&lt;/p&gt;

&lt;p class="story"&gt;...&lt;/p&gt;
"""
复制<br>使用BeautifulSoup解析这段代码,能够得到一个 BeautifulSoup 的对象,并能按照标准的缩进格式的结构输出:<br>Pythonfrom bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc, 'lxml')
# html进行美化
print(soup.prettify())
复制<br>匹配代码<br>Python&lt;html&gt;
 &lt;head&gt;
  &lt;title&gt;
   The Dormouse's story
  &lt;/title&gt;
 &lt;/head&gt;
 &lt;body&gt;
  &lt;p class="title"&gt;
   &lt;b&gt;
    The Dormouse's story
   &lt;/b&gt;
  &lt;/p&gt;
  &lt;p class="story"&gt;
   Once upon a time there were three little sisters; and their names were
   &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;
    Elsie
   &lt;/a&gt;
   ,
   &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;
    Lacie
   &lt;/a&gt;
   and
   &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;
    Tillie
   &lt;/a&gt;
   ;
and they lived at the bottom of a well.
  &lt;/p&gt;
  &lt;p class="story"&gt;
   ...
  &lt;/p&gt;
 &lt;/body&gt;
&lt;/html&gt;
复制<br>几个简单的浏览结构化数据的方法:<br>Pythonsoup.title  # 获取标签title
# &lt;title&gt;The Dormouse's story&lt;/title&gt;

soup.title.name   # 获取标签名称
# 'title'

soup.title.string   # 获取标签title内的内容
# 'The Dormouse's story'

soup.title.parent  # 获取父级标签

soup.title.parent.name  # 获取父级标签名称
# 'head'

soup.p
# &lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;

soup.p['class']  # 获取p的class属性值
# 'title'

soup.a
# &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;

soup.find_all('a')
# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,
#  &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,
#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]

soup.find(id="link3")  # 获取id为link3的标签
# &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;
复制<br>从文档中找到所有&lt;a&gt;标签的链接:<br>Pythonfor link in soup.find_all('a'):
    print(link.get('href'))
    # http://example.com/elsie
    # http://example.com/lacie
    # http://example.com/tillie
复制<br>从文档中获取所有文字内容:<br>Pythonprint(soup.get_text())
复制<br><br>将一段文档传入BeautifulSoup 的构造方法,就能得到一个文档的对象, 可以传入一段字符串或一个文件句柄.<br>Pythonfrom bs4 import BeautifulSoup

soup = BeautifulSoup(open("index.html"))

soup = BeautifulSoup("&lt;html&gt;data&lt;/html&gt;", 'lxml')
复制<br>然后,Beautiful Soup选择最合适的解析器来解析这段文档,如果手动指定解析器那么Beautiful Soup会选择指定的解析器来解析文档。<br><br>还拿”爱丽丝梦游仙境”的文档来做例子:<br>Pythonhtml_doc = """
&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;
    &lt;body&gt;
&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;

&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were
&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,
&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and
&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;
and they lived at the bottom of a well.&lt;/p&gt;

&lt;p class="story"&gt;...&lt;/p&gt;
"""

from bs4 import BeautifulSoup
# lxml和html.parser解析的有时候会根据html是否完整而有解析不同的问题，需要注意
soup = BeautifulSoup(html_doc, 'html.parser')
复制<br>通过这段例子来演示怎样从文档的一段内容找到另一段内容<br><br>一个Tag可能包含多个字符串或其它的Tag,这些都是这个Tag的子节点.Beautiful Soup提供了许多操作和遍历子节点的属性.<br>注意: Beautiful Soup中字符串节点不支持这些属性,因为字符串没有子节点。<br><br>tag的 .contents 属性可以将tag的子节点以列表的方式输出:<br>Pythonhead_tag = soup.head
head_tag
# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;

head_tag.contents
[&lt;title&gt;The Dormouse's story&lt;/title&gt;]

title_tag = head_tag.contents[0]
title_tag
# &lt;title&gt;The Dormouse's story&lt;/title&gt;
title_tag.contents
# [u'The Dormouse's story']
复制<br>字符串没有 .contents 属性,因为字符串没有子节点:<br>Pythontext = title_tag.contents[0]
text.contents
# AttributeError: 'NavigableString' object has no attribute 'contents'
复制<br><br><br>如果tag只有一个 NavigableString 类型子节点,那么这个tag可以使用 .string 得到子节点。如果一个tag仅有一个子节点,那么这个tag也可以使用 .string 方法,输出结果与当前唯一子节点的 .string 结果相同。<br>通俗点说就是：如果一个标签里面没有标签了，那么 .string 就会返回标签里面的内容。如果标签里面只有唯一的一个标签了，那么 .string 也会返回最里面的内容。例如：<br>Pythonprint (soup.head.string)
#The Dormouse's story
# &lt;title&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/title&gt;
print (soup.title.string)
#The Dormouse's story
复制<br>如果tag包含了多个子节点,tag就无法确定，string 方法应该调用哪个子节点的内容, .string 的输出结果是 None<br>Pythonprint (soup.html.string)
#None
复制<br><br>如果tag包含了多个子节点, text则会返回内部所有文本内容<br>print (soup.html.text)
复制<br>注意：<br>strings和text都可以返回所有文本内容  <br>区别：text返回内容为字符串类型  strings为生成器generator<br><br>.strings .stripped_strings 属性
复制<br><br>获取多个内容，不过需要遍历获取，比如下面的例子：<br>Pythonfor string in soup.strings:
    print(repr(string))
    
    
'''
  '\n'
"The Dormouse's story"
'\n'
'\n'
"The Dormouse's story"
'\n'
'Once upon a time there were three little sisters; and their names were\n'
'Elsie'
',\n'
'Lacie'
' and\n'
'Tillie'
';\nand they lived at the bottom of a well.'
'\n'
'...'
'\n'  
    
'''    
复制<br><br>输出的字符串中可能包含了很多空格或空行,使用 .stripped_strings 可以去除多余空白内容<br>Pythonfor string in soup.stripped_strings:
    print(repr(string))


'''

"The Dormouse's story"
"The Dormouse's story"
'Once upon a time there were three little sisters; and their names were'
'Elsie'
','
'Lacie'
'and'
'Tillie'
';\nand they lived at the bottom of a well.'
'...'

'''
复制<br><br>继续分析文档树,每个tag或字符串都有父节点:被包含在某个tag中<br><br>通过 .parent 属性来获取某个元素的父节点.在例子“爱丽丝”的文档中,&lt;head&gt;标签是&lt;title&gt;标签的父节点:<br>Pythontitle_tag = soup.title
title_tag
# &lt;title&gt;The Dormouse's story&lt;/title&gt;
title_tag.parent
# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;
复制<br>文档的顶层节点比如&lt;html&gt;的父节点是 BeautifulSoup 对象:<br>Pythonhtml_tag = soup.html
type(html_tag.parent)
# &lt;class 'bs4.BeautifulSoup'&gt;
复制<br><br><br>Pythonfind_all( name , attrs , recursive , string , **kwargs )
复制<br>find_all() 方法搜索当前tag的所有tag子节点,并判断是否符合过滤器的条件:<br>Pythonsoup.find_all("title")
# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]

soup.find_all("a")
# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,
#  &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,
#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]

soup.find_all(id="link2")
# [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]

import re
# 模糊查询 包含sisters的就可以
soup.find(string=re.compile("sisters"))
# 'Once upon a time there were three little sisters; and their names were\n'
复制<br>有几个方法很相似,还有几个方法是新的,参数中的 string 和 id 是什么含义? 为什么 find_all("p", "title") 返回的是CSS Class为”title”的&lt;p&gt;标签? 我们来仔细看一下 find_all() 的参数.<br><br>name 参数可以查找所有名字为 name 的tag,字符串对象会被自动忽略掉.<br>简单的用法如下:<br>Pythonsoup.find_all("title")
# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]
复制<br>搜索 name 参数的值可以使任一类型的 过滤器 ,字符串,正则表达式,列表,方法或是 True .<br>&lt;1&gt; 传字符串<br>最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的标签<br>Pythonsoup.find_all('b')
# [&lt;b&gt;The Dormouse's story&lt;/b&gt;]
复制<br>&lt;2&gt; 传正则表达式<br>如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.下面例子中找出所有以b开头的标签,这表示&lt;body&gt;和&lt;b&gt;标签都应该被找到<br>Pythonimport re
for tag in soup.find_all(re.compile("^b")):
    print(tag.name)
# body
# b
复制<br>&lt;3&gt; 传列表<br>如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有&lt;a&gt;标签和&lt;b&gt;标签<br>Pythonsoup.find_all(["a", "b"])
# [&lt;b&gt;The Dormouse's story&lt;/b&gt;,
#  &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,
#  &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,
#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]
复制<br><br>如果一个指定名字的参数不是搜索内置的参数名,搜索时会把该参数当作指定名字tag的属性来搜索,如果包含一个名字为 id 的参数,Beautiful Soup会搜索每个tag的”id”属性.<br>Pythonsoup.find_all(id='link2')
# [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]

import re
# 超链接包含elsie标签
print(soup.find_all(href=re.compile("elsie")))
# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]
# 以The作为开头的字符串
print(soup.find_all(text=re.compile("^The"))) 
# ["The Dormouse's story", "The Dormouse's story"]
# class选择器包含st的节点
print(soup.find_all(class_=re.compile("st")))
复制<br>搜索指定名字的属性时可以使用的参数值包括 字符串 , 正则表达式 , 列表, True .<br>下面的例子在文档树中查找所有包含 id 属性的tag,无论 id 的值是什么:<br>Pythonsoup.find_all(id=True)
# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,
#  &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,
#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]
复制<br>使用多个指定名字的参数可以同时过滤tag的多个属性:<br>Pythonsoup.find_all(href=re.compile("elsie"), id='link1')
# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;three&lt;/a&gt;]
复制<br>在这里我们想用 class 过滤，不过 class 是 python 的关键词，这怎么办？加个下划线就可以<br>Pythonprint(soup.find_all("a", class_="sister"))

'''
[&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,
&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,
&lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;
]

'''
复制<br>通过 find_all() 方法的 attrs 参数定义一个字典参数来搜索包含特殊属性的tag:<br>Pythondata_soup.find_all(attrs={"data-foo": "value"})
# [&lt;div data-foo="value"&gt;foo!&lt;/div&gt;]
复制<br>注意：如何查看条件id和class同时存在时的写法<br>Pythonprint(soup.find_all('b', class_="story", id="x"))
print(soup.find_all('b', attrs={"class":"story", "id":"x"}))
复制<br><br>通过 text 参数可以搜搜文档中的字符串内容.与 name 参数的可选值一样, text 参数接受 字符串 , 正则表达式 , 列表, True<br>Pythonimport re

print(soup.find_all(text="Elsie"))
# ['Elsie']

print(soup.find_all(text=["Tillie", "Elsie", "Lacie"]))
# ['Elsie', 'Lacie', 'Tillie']

# 只要包含Dormouse就可以
print(soup.find_all(text=re.compile("Dormouse")))
# ["The Dormouse's story", "The Dormouse's story"]
复制<br><br>find_all() 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢.如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量.效果与SQL中的limit关键字类似,当搜索到的结果数量达到 limit 的限制时,就停止搜索返回结果.<br>Pythonprint(soup.find_all("a",limit=2))
print(soup.find_all("a")[0:2])

'''
[&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;, 
&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]
'''
复制<br><br>Pythonfind( name , attrs , recursive , string , **kwargs )
复制<br>find_all() 方法将返回文档中符合条件的所有tag,尽管有时候我们只想得到一个结果.比如文档中只有一个&lt;body&gt;标签,那么使用 find_all() 方法来查找&lt;body&gt;标签就不太合适, 使用 find_all 方法并设置 limit=1 参数不如直接使用 find() 方法.下面两行代码是等价的:<br>Pythonsoup.find_all('title', limit=1)
# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]

soup.find('title')
# &lt;title&gt;The Dormouse's story&lt;/title&gt;
复制<br>唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果.<br>find_all() 方法没有找到目标是返回空列表, find() 方法找不到目标时,返回 None .<br>Pythonprint(soup.find("nosuchtag"))
# None
复制<br>soup.head.title 是 tag的名字 方法的简写.这个简写的原理就是多次调用当前tag的 find() 方法:<br>Pythonsoup.head.title
# &lt;title&gt;The Dormouse's story&lt;/title&gt;

soup.find("head").find("title")
# &lt;title&gt;The Dormouse's story&lt;/title&gt;
复制<br><br>我们在写 CSS 时，标签名不加任何修饰，类名前加点，id名前加 在这里我们也可以利用类似的方法来筛选元素，用到的方法是 soup.select()，返回类型是 list<br><br>Pythonprint(soup.select("title"))  #[&lt;title&gt;The Dormouse's story&lt;/title&gt;]
print(soup.select("b"))      #[&lt;b&gt;The Dormouse's story&lt;/b&gt;]
复制<br><br>Pythonprint(soup.select(".sister")) 

'''
[&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;, 
&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;, 
&lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]

'''
复制<br><br>Pythonprint(soup.select("#link1"))
# [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]
复制<br><br>组合查找即和写 class 文件时，标签名与类名、id名进行的组合原理是一样的，例如查找 p 标签中，id 等于 link1的内容，二者需要用空格分开<br>Pythonprint(soup.select("p #link2"))

#[&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]
复制<br>直接子标签查找<br>Pythonprint(soup.select("p &gt; #link2"))
# [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]
复制<br>查找既有class也有id选择器的标签<br>a_string = soup.select(".story#test")
复制<br>查找有多个class选择器的标签<br>a_string = soup.select(".story.test")
复制<br>查找有多个class选择器和一个id选择器的标签<br>a_string = soup.select(".story.test#book")
复制<br><br>查找时还可以加入属性元素，属性需要用中括号括起来，注意属性和标签属于同一节点，所以中间不能加空格，否则会无法匹配到。<br>Pythonprint(soup.select("a[href='http://example.com/tillie']"))
#[&lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]
复制<br>select 方法返回的结果都是列表形式，可以遍历形式输出，然后用 get_text() 方法来获取它的内容：<br>Pythonfor title in soup.select('a'):
    print (title.get_text())

'''
Elsie
Lacie
Tillie
'''
复制<br><br><br>“./BS4解析完整.assets/image-20220610123612886.png” could not be found.<br><br>只要城市与气温<br>“./BS4解析完整.assets/image-20220610123825110.png” could not be found.<br>匹配后结果为<br>“./BS4解析完整.assets/image-20220610123845530.png” could not be found.<br><br>匹配红色框内的内容<br>“./BS4解析完整.assets/image-20220610124307970.png” could not be found.]]></description><link>爬虫\爬虫文档\BS4笔记.html</link><guid isPermaLink="false">爬虫/爬虫文档/BS4笔记.md</guid><pubDate>Mon, 16 Oct 2023 15:07:14 GMT</pubDate></item><item><title><![CDATA[xpath]]></title><description><![CDATA[<a class="tag" href="?query=tag:xpath" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#xpath</a> 
 <br><br><a href=".?query=tag:xpath" class="tag" target="_blank" rel="noopener">#xpath</a><br><br>安装<br>
<br>
安装lxml库
pip install lxml  -i pip源

<br><br>解析流程<br>
<br>实例化一个etree的对象，把即将被解析的页面源码加载到该对象
<br>调用该对象的xpath方法结合着不同形式的xpath表达进行标签定位和数据提取
<br>使用<br>
<br>
导入lxml.etree
from lxml import etree

<br>
etree.parse()
解析本地html文件
html_tree = etree.parse('XX.html')

<br>
etree.HTML()(建议)
解析网络的html字符串
html_tree = etree.HTML(html字符串)

<br>
html_tree.xpath()
使用xpath路径查询信息，返回一个列表

<br>注意：如果lxml解析本地HTML文件报错可以安装如下添加参数<br>Pythonparser = etree.HTMLParser(encoding="utf-8")
selector = etree.parse('./lol_1.html',parser=parser)
result=etree.tostring(selector)
复制<br><br>XPath 是一门在 XML 文档中查找信息的语言。XPath 用于在 XML 文档中通过元素和属性进行导航。<br>
<br>
路径表达式

实例
在下面的表格中，我们已列出了一些路径表达式以及表达式的结果：


<br>
谓语（Predicates）
谓语用来查找某个特定的节点或者包含某个指定的值的节点。
谓语被嵌在方括号中。
实例
在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果：


<br>
选取未知节点
XPath 通配符可用来选取未知的 XML 元素。

实例
在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：

选取若干路径
通过在路径表达式中使用“|”运算符，您可以选取若干个路径。
实例
在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：


<br>
逻辑运算

<br>
查找所有id属性等于head并且class属性等于s_down的div标签
Python//div[@id="head" and @class="s_down"]
复制

<br>
选取文档中的所有 title 和 price 元素。
//title | //price
复制
注意: “|”两边必须是完整的xpath路径



<br>
属性查询

<br>
查找所有包含id属性的div节点
//div[@id]
复制

<br>
查找所有id属性等于maincontent的div标签
//div[@id="maincontent"]
复制

<br>
查找所有的class属性
//@class
复制

<br>
//@attrName
//li[@name="xx"]//text()  # 获取li标签name为xx的里面的文本内容
复制



<br>
获取第几个标签  索引从1开始
tree.xpath('//li[1]/a/text()')  # 获取第一个
tree.xpath('//li[last()]/a/text()')  # 获取最后一个
tree.xpath('//li[last()-1]/a/text()')  # 获取倒数第二个
复制

<br>
模糊查询

<br>
查询所有id属性中包含he的div标签
//div[contains(@id, "he")]
复制

<br>
查询所有id属性中包以he开头的div标签
//div[starts-with(@id, "he")]
复制



<br>
内容查询
查找所有div标签下的直接子节点h1的内容
//div/h1/text()
复制

<br>
属性值获取
//div/a/@href   获取a里面的href属性值
复制

<br>
获取所有
//*  #获取所有
//*[@class="xx"]  #获取所有class为xx的标签
复制

<br>
获取节点内容转换成字符串
c = tree.xpath('//li/a')[0]
result=etree.tostring(c, encoding='utf-8')
print(result.decode('UTF-8'))
复制

<br>练习：<br>
<br>
豆瓣
匹配出虚构类和非虚构类中的图片，标题和简介

<br>
股票数据
抓到每一行股票数据

]]></description><link>爬虫\爬虫文档\day06 xpath.html</link><guid isPermaLink="false">爬虫/爬虫文档/day06 xpath.md</guid><pubDate>Mon, 16 Oct 2023 15:07:32 GMT</pubDate></item><item><title><![CDATA[IP代理池]]></title><description><![CDATA[ 
 <br><br><br>目录结构<br>
<br>get_ip.py    抓取免费ip的文件
<br>test_ip.py   测试ip是否可用的文件
<br>proxy_redis.py   存储和处理当前的ip
<br>app.py    请求这个文件的接口    返回可用的ip
<br>settings.py  全局的配置文件
<br>main.py   整个ip代理池运行的入口
<br><br><br>
<br>
zadd  添加集合
zadd proxy_redis 1 a 2 b  添加有序集合中a 成员 权重1  b成员 权重2

<br>
zscore  获取成员的权重
zscore proxy_redis a  获取成员a的权重

<br>
zincrby  增加/减少权重
zincrby proxy_redis 10 a       给成员a 增加10权重
 zincrby proxy_redis -10 a    给成员a 减少10权重

<br>
zrange  获取范围区间的ip
 zrange proxy_redis 0 -1  获取所有的成员

<br>
zrangebyscore   获取权重区间范围的ip
 zrangebyscore proxy_redis 0 20     返回权重在0-20之间的成员

<br>
zrem  删除
zrem proxy_redis a  删除成员a

]]></description><link>爬虫\爬虫文档\ip代理词思路规划.html</link><guid isPermaLink="false">爬虫/爬虫文档/ip代理词思路规划.md</guid><pubDate>Tue, 19 Sep 2023 09:55:12 GMT</pubDate></item><item><title><![CDATA[NoSQL Mongodb]]></title><description><![CDATA[<a class="tag" href="?query=tag:数据库" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#数据库</a> <a class="tag" href="?query=tag:MGdb" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#MGdb</a> 
 <br><br><a href=".?query=tag:数据库" class="tag" target="_blank" rel="noopener">#数据库</a> <a href=".?query=tag:MGdb" class="tag" target="_blank" rel="noopener">#MGdb</a><br>
<br>下载mongodb的版本，两点注意

<br>根据业界规则，偶数为稳定版，如1.6.X，奇数为开发版，如1.7.X
<br>32bit的mongodb最大只能存放2G的数据，64bit就没有限制


<br>性能<br>BSON格式的编码和解码都是非常快速的。它使用了C风格的数据表现形式，这样在各种语言中都可以高效地使用。<br>NoSQL(NoSQL = Not Only SQL )，意即"不仅仅是SQL"。<br>MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。<br><br>下载mongodb的版本，两点注意<br>
<br>根据业界规则，偶数为稳定版，如1.6.X，奇数为开发版，如1.7.X
<br>32bit的mongodb最大只能存放2G的数据，64bit就没有限制
<br>首先去官网下载MongoDB的安装包, <a rel="noopener" class="external-link" href="https://www.mongodb.com/try/download/community" target="_blank">https://www.mongodb.com/try/download/community</a><br>“Mongodb.assets/image-20210728093724660.png” could not be found.<br>“Mongodb.assets/image-20210728093838499.png” could not be found.<br>“Mongodb.assets/image-20210728093906551.png” could not be found.<br>“Mongodb.assets/image-20210728094404958.png” could not be found.<br>“Mongodb.assets/image-20210728094441168.png” could not be found.<br>“Mongodb.assets/image-20210728094521878.png” could not be found.<br>“Mongodb.assets/image-20210728094547434.png” could not be found.<br>“Mongodb.assets/image-20210728094627838.png” could not be found.<br>“Mongodb.assets/image-20210728094653296.png” could not be found.<br>“Mongodb.assets/image-20210728094727302.png” could not be found.<br>“Mongodb.assets/image-20210728094750500.png” could not be found.<br>将mongodb目录下的bin文件夹添加到环境变量<br>“Mongodb.assets/image-20210728095340874.png” could not be found.<br>“Mongodb.assets/image-20210728095433900.png” could not be found.<br>“Mongodb.assets/image-20210728095507330.png” could not be found.<br>“Mongodb.assets/image-20210728095631337.png” could not be found.<br>“Mongodb.assets/image-20210728100113133.png” could not be found.<br>“Mongodb.assets/image-20210729195132883.png” could not be found.<br>“Mongodb.assets/image-20210729195203348.png” could not be found.<br>对于mac的安装可以使用homebrew安装.  或参考这里<a rel="noopener" class="external-link" href="https://www.runoob.com/mongodb/mongodb-osx-install.html" target="_blank">https://www.runoob.com/mongodb/mongodb-osx-install.html</a><br><br><br><br><br>
<br>文档中的键/值对是有序的。
<br>文档中的值不仅可以是在双引号里面的字符串,还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。
<br>MongoDB区分类型和大小写。
<br>MongoDB的文档不能有重复的键。
<br>文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。
<br><br>
<br>.和$有特别的意义，只有在特定环境下才能使用。
<br>以下划线"_"开头的键是保留的(不是严格要求的)。
<br><br><br>输入 mongod.exe     --dbpath=路径<br><br>​	cd   mongo安装的目录/bin<br>​	mongo.exe   此刻 进入到Mongodb数据库了<br><br><br>​	show dbs<br><br>​	use 库名 <br>​	注意:<br>​		新创建的数据库 默认你是看不到的 可以使用db/db.getName() 去查看当前所在的库   往新的库里创建集合<br><br>Python1. db
2. db.getName()
复制<br><br>
<br>
db.createCollection("集合名")
remo例如：db.createCollection("user") 创建一个user的集合 在当前的库里

<br>
db.集合名.insert(文档)  如果 当前的集合名不存在 那么就创建该集合 并插入文档(数据) 

<br>注意：<br>Python1. 在库里对于文档 集合的操作 统一使用db. (db代表当前的库)
2. 严格区分大小写
复制<br><br>​	show collections<br><br>​	db.集合名.drop()<br><br>使用insert <br>​	db.集合名.insert(文档) 如果是添加数据 建议使用 insert<br>插入多条数据:<br>​	db.集合名.insert([文档])注意 一定要加[] 否则可能只会把 第一条文档插入进去<br>db.user.insert({'name':'lisi', 'age': 20})

db.user.insert([{'name':'lisi', 'age': 20},{'name': 'wangwu', 'age': 30}])
复制<br>3.2 版本后还有以下几种语法可用于插入文档:（建议使用）<br>
<br>db.collection.insertOne():向指定集合中插入一条文档数z据
<br>db.collection.insertMany():向指定集合中插入多条文档数据
<br>db.user.insertOne({'name':'lisi', 'age': 20})

db.user.insertMany([{'name':'lisi', 'age': 20},{'name': 'wangwu', 'age': 30}])
复制<br><br><br>​	db.集合名.find([条件],{key1:1[,[key2:1]]}) 查询所有的数据   代表 显示哪些字段名<br>​	db.collection.find(query, {title: 1, by: 1}) // inclusion模式 指定返回的键，不返回其他键<br>​	db.collection.find(query, {title: 0, by: 0}) // exclusion模式 指定不返回的键,返回其他键<br>注意：<br>两种模式不可混用（因为这样的话无法推断其他键是否应返回）<br>Pythondb.collection.find(query, {title: 1, by: 0}) // 错误
复制<br>_id 键默认返回，需要主动指定 _id:0 才会隐藏<br>只能全1或全0，除了在inclusion模式时可以指定_id为0<br>Pythondb.collection.find(query, {_id:0, title: 1, by: 1}) // 正确
复制<br>实例<br>db.user.find({}, {'name': 0})
db.user.find({'age': 18}, {'name': 1})  # 只返回那么字段
db.user.find({'age': 18}, {'name': 0})  # 不返回name字段
复制<br><br>​	db.集合名.findOne([条件],{key1:1[,[key2:1]]}) 查询一条数据  代表 显示哪些字段名<br>db.user.findOne({}, {name:1})
复制<br><br>​	db.集合名.find([条件]).count()<br>db.user.find({}).count()
复制<br><br>​	db.集合名.find([条件]).pretty()<br>db.user.find({}).pretty()
复制<br><br><br><br>Pythondb.col.find({key1:value1, key2:value2}).pretty()
复制<br>db.集合名.find({条件一，条件二,,,})<br>例如：	<br>db.user.find({name:"张三",age:{$gt:10}}) #查询name为张三的 且 年龄 大于10岁的

db.user.find({name:"张三",age:10}) 		#查询name为张三的 且 年龄为10岁的
复制<br><br>db.集合名.find({$or:[{条件一},{条件二},,,]})<br>例如：<br>db.user.find({$or:[{name:"张三"},{name:"赵六"}]})   #查询name为张三 或者为赵六的所有数据
复制<br><br>db.集合名.find({条件一,,,$or:[{条件1},{条件2}]})<br>例如:<br>db.user.find({name:"张三",$or:[{age:10},{age:28}]})  #name为张三   年龄为10岁或者28岁的所有数据
复制<br><br>db.集合名.find().limit(num)   从第0个开始取几个<br>例如：<br>db.user.find().limit(5)   #从0开始取5条数据
复制<br><br>db.集合名.find().skip(num) 跳过几条数据<br>例如:<br>db.user.find().skip(2) #从第三条数据 取到最后
复制<br><br>db.集合名.find().skip().limit(num) <br>例如:<br>db.user.find().skip(2).limit(2)  #从第三个开始 取2个
复制<br><br>在MongoDB中使用使用sort()方法对数据进行排序，sort()方法可以通过参数指定排序的字段，并使用 1 和 -1 来指定排序的方式，其中 1 为升序排列，而-1是用于降序排列。<br>db.集合名.find().sort({key:1|-1})  升序或者降序<br>例如:	<br>db.user.find().sort({age:1})  #查询所有数据 按照年龄 升序

db.user.find().sort({age:-1})  #查询所有数据 按照年龄 降序
复制<br><br>结构：<br>Pythondb.collection.update(
   &lt;query&gt;,
   &lt;update&gt;,
   {
     upsert: &lt;boolean&gt;,
     multi: &lt;boolean&gt;,
     writeConcern: &lt;document&gt;
   }
)u
复制<br>
<br>query : update的查询条件，类似sql update查询内where后面的。
<br>update : update的对象和一些更新的操作符（如inc...）等，也可以理解为sql update查询内set后面的
<br><br>
<br>
db.集合名.update(条件,数据,{multi:true}) 更改
Sqldb.user.update({name:"张三"},{$inc:{age:5}}) #修改name为张三的文档  将age在原有的基础上 加5

db.user.update({name:"张三"},{$set:{age:5}}) #修改name为张三的文档  将age的值 修改为5

db.user.update({name:"张三"},{$set:{age:5}}) #将name为张三的文档 的年龄 修改为 5
复制

<br>
只更新第一条记录：
Sqldb.col.update({"count" : {$gt:1}} , { $set : {"test2" : "OK"}});
复制

<br>
全部更新：
Sqldb.col.update({"count" : {$gt: 3 }} , {$set : {"test2" : "OK"} }); 
复制

<br>
只添加第一条：
Sqldb.col.update({ "count" : { $gt : 4 }} , {$set : {"test5" : "OK"} });
复制
  ​
3.2版本以后（建议使用）

<br>updateOne()  更新一条<br>db.user.updateOne({'name':'lisi'}, {$inc:{'age':5}})
复制<br>updateMany(query,update) 更新多条 <br>db.user.updateMany({'name':'lisi'}, {$inc:{'age':5}})
复制<br><br>主体结构<br>Pythondb.collection.remove(
   &lt;query&gt;,
   {
     justOne: &lt;boolean&gt;,
     writeConcern: &lt;document&gt;
   }
)
复制<br><br>
<br>query :（可选）删除的文档的条件。
<br>justOne : （可选）如果设为 true 或 1，则只删除一个文档。
<br>writeConcern :（可选）抛出异常的级别。
<br><br>db.集合名.remove(条件) 默认将所有都匹配到的数据进行删除<br>db.集合名.remove(条件,1) 只删除 第一个匹配到的数据<br>db.集合名.remove(条件,,{justOne:true}) 只删除 第一个匹配到的数据<br>示例<br>Sqlb.user.remove({'age':{$gt: 30}})  # 删除年龄大于30的所有数据
b.user.remove({'age':{$gt: 30}}, 1)  # 删除年龄大于30的一条数据
db.col.remove({})  清空集合 "col" 的数据
复制<br><br>remove() 方法已经过时了，现在官方推荐使用 <br>deleteOne() 删除一条<br>Sqldb.user.deleteOne({'age':{$gt: 0}})
复制<br>deleteMany() 删除多条<br>Sqldb.user.deleteMany({'age':{$gt: 0}})
复制<br><br><br>
<br>删除之前 最好use一下
<br>db.dropDatabase()
<br><br>exit<br><br><br>from pymongo import MongoClient<br><br><br>连接MongoDB我们需要使用PyMongo库里面的MongoClient，一般来说传入MongoDB的IP及端口即可，第一个参数为地址host，第二个参数为端口port，端口如果不传默认是27017。<br><br>Pythonconn = MongoClient("localhost")

MongoClient(host='127.0.0.1',port=27017)
复制<br><br><br><br><br>###4、插入数据<br><br>insert_one 插入一条数据 <br>insert_many() 插入多条数据<br><br>data.inserted_id<br>data.inserted_ids<br><br><br>db.user.find_one()<br><br>db.user.find({"name":"张三"})<br><br>from bson.objectid import ObjectId*#用于ID查询<br>data = db.user.find({"_id":ObjectId("59a2d304b961661b209f8da1")})<br>Pythondata = db.user.find({'_id': ObjectId('59f290b01683f9339214746d')}) #_id': ObjectId('59f290b01683f9339214746d')
复制<br><br>
<br>
{"name":{'$regex':"张"}}

<br>
{'xxx':re.compile('xxx')}  

<br><br><br>年龄 大于10<br>Pythondata = db.user.find({"age":{"$gt":10}}).sort("age",1) #年龄 升序 查询  pymongo.ASCENDING   --升序

data = db.user.find({"age":{"$gt":10}}).sort("age",-1) #年龄 降序 查询	pymongo.DESCENDING --降序
复制<br><br><br>Pythondb.user.find().limit(3)
m= db.user.find({"age":{"$gt":10}}).sort("age",-1).limit(3)
复制<br><br>db.user.find().skip(2)<br><br>update()方法其实也是官方不推荐使用的方法，在这里也分了update_one()方法和update_many()方法，用法更加严格，<br><br>db.user.update_one({"name":"张三"},{"$set":{"age":99}})<br><br>db.user.update_many({"name":"张三"},{"$set":{"age":91}})<br>(3) 其返回结果是UpdateResult类型，然后调用matched_count和modified_count属性分别可以获得匹配的数据条数和影响的数据条数。<br>
<br>result.matched_count
<br>result.modified_count
<br><br>删除操作比较简单，直接调用remove()方法指定删除的条件即可，符合条件的所有数据均会被删除，<br><br>delete_one()即删除第一条符合条件的数据<br>collection.delete_one({“name”:“ Kevin”})<br><br>delete_many()即删除所有符合条件的数据，返回结果是DeleteResult类型<br>collection.delete_many({“age”: {$lt:25}})<br><br>result.deleted_count<br><br>conn.close()]]></description><link>爬虫\爬虫文档\Mongodb.html</link><guid isPermaLink="false">爬虫/爬虫文档/Mongodb.md</guid><pubDate>Mon, 16 Oct 2023 15:08:32 GMT</pubDate></item><item><title><![CDATA[Redis数据库]]></title><description><![CDATA[ 
 <br><br><br>Redis是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。<br>Redis与其他 key- value 缓存产品有以下三个特点：<br>Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。<br>Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。<br>redis： 半持久化，存储于内存和硬盘<br><br>Redis是完全在内存中保存数据的数据库，使用磁盘只是为了持久性目的,Redis数据全部存在内存，定期写入磁盘，当内存不够时，可以选择指定的LRU算法删除数据,持久化是使用RDB方式或者aof方式。<br>mongodb是文档型的非关系型数据库，MongoDB更类似MySQL，支持字段索引、游标操作，其优势在于查询功能比较强大，擅长查询JSON数据，能存储海量数据，但是不支持事务。<br><br><br><a rel="noopener" class="external-link" href="https://github.com/ServiceStack/redis-windows" target="_blank">https://github.com/ServiceStack/redis-windows</a><br><a rel="noopener" class="external-link" href="https://github.com/MSOpenTech/redis/releases" target="_blank">https://github.com/MSOpenTech/redis/releases</a><br><br>455行 maxheap 1024000000   设置最大的数据堆的大小<br>387行 requirepass 123456		设置数据库的密码<br><br>cd C:\redis64-2.8.2101<br>C:\redis64-2.8.2101&gt;redis-server.exe redis.windows.conf  执行 redis-server.exe 并加载Windows的配置文件<br>C:\redis64-2.8.2101—&gt;dump.rdb  为数据文件<br><br>1、执行 brew install redis<br>2、启动 redis，可以使用后台服务启动 brew services start redis。或者直接启动：redis-server /usr/local/etc/redis.conf<br><br><br><br>C:\redis64-2.8.2101&gt;redis-cli.exe<br>127.0.0.1:6379&gt;auth '123456'    <br><br>密码 为 字符串类型<br><br>
<br>


<br>


<br>


<br>


<br>


<br><br><a rel="noopener" class="external-link" href="http://redis.cn/commands.html" target="_blank">http://redis.cn/commands.html</a><br>config get databases 查看所有的数据库 数据库以0开始 一共16个<br><br>概述：String是redis最基本的类型，最大能存储512MB的数据，String类型是二进制安全的，即可以存储任何数据、比如数字、图片、序列化对象等<br>一个key对应一个value<br>string类型是Redis最基本的数据类型，一个键最大能存储512MB。<br><br><br>
set key value 
<br>Pythonset name "zhangsan"
复制<br><br>
setex key seconds value
<br>Pythonsetex name 10 'zhangsan'
复制<br><br>
ttl key
<br>Pythonttl name
复制<br><br>
persist key
<br>persist name
复制<br><br>
setnx key value
<br> setnx name 'a'
复制<br><br>
mset key value [key value ……]
<br>Pythonmset name 'zs' age 18 	
复制<br><br><br>
get key
<br> get name<br><br>
mget key [key ……]
<br>  mget name age<br><br>
getrange key start end
<br>Pythongetrange name 0 4
复制<br><br>
getset key value
<br>Pythongetset name 'x'
复制<br><br><br><br>
incr key   
<br> incr age<br><br>
decr key
<br>decr age<br><br>
incrby key intnum
<br> incrby age 10		<br><br>
decrby key intnum
<br>decrby age 10<br><br>
strlen key
<br>  strlen age<br><br><br>
keys *
<br><br>
exists key
<br>exists name<br><br>
type key
<br> type name<br><br>
del key [key ……]
<br><br>
expire key seconds
<br>   expire age 10<br><br>
ttl key
<br><br>
pttl key 
<br><br>
persist key
<br><br>
flushdb	删除当前数据库中的所有
<br>
flushall		删除所有数据库中的key
<br><br>
rename key newkey
<br><br>
Move key db
<br>Pythonmove name 1	# 将name 移动到数据库1
复制<br><br>
randomkey
<br><br><br>{<br>​	name:"tom",
​	age:18<br>}<br>Redis hash 是一个键值(key=&gt;value)对集合。<br><br><br>
hset key field value
<br>Pythonredis&gt; hset myhash name lucky
(integer) 1
redis&gt; HGET myhash name
"Hello"
复制<br><br>
hmset key field value [field value ……]
<br>Pythonhmset myhash a 1 b 2 c 3
复制<br><br>
hincrby key field incrment
<br>Pythonhincrby hh age 10
复制<br><br>
hsetnx key field value
<br>Python hget hh name
复制<br><br><br>
hget key field
<br>hget name field1<br><br>
hmget key filed [filed ……]
<br><br>
hgetall key
<br><br>
hkeys key
<br><br>
hvals key
<br><br>
hlen key
<br><br><br>
hexists key field
<br>hexists a x<br><br>
hdel key field [field ……]
<br> hdel a x y z<br><br>
hstrlen key field
<br><br>概述：Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）<br><br><br>
lpush key value [vlaue ……] 
<br>Pythonlpush demo 2 3`
复制<br><br>
Lpushx  key val
<br>lpushx list 'a'
复制<br><br>
rpush key value [vlaue ……]
<br>Pythonrpush demo 2 1
复制<br><br>
rpushx  key val
<br>Pythonrpushx mm 'a'	
复制<br><br><br>
lpop key
<br>Pythonlpop demo
复制<br><br>
rpop key
<br>Pythonrpop demo
复制<br><br>
lrange key  start end
<br>Pythonlrange demo 0 -1	#查看列表中的所有元素
复制<br>注意：start end都是从0开始
注意：偏移量可以是负数<br><br><br>
ltrim key start end
<br>Pythonltrim demo 1 -1	#将索引为1 到 -1的元素裁剪出来
复制<br>注意：start end都是从0开始
注意：偏移量可以是负数<br><br>
llen key
<br><br>
lindex key index
<br>PythonLINDEX mylist 0
复制<br><br>概述：无序集合，元素类型为String类型，元素具有唯一性，不重复<br>{ 'a','b'}<br><br><br>
sadd key member [member ……]
<br>Pythonsadd set 'a' 'b' 'c'
复制<br><br><br>
smembers key
<br>Pythonsmembers set
复制<br><br>
scard key
<br>Pythonscard set
复制<br><br>
spop  key
<br>spop set
复制<br><br>
srandmember  key  count
<br>Pythons set		#返回一个随机元素
srandmember set 2	#返回2个随机元素
复制<br><br>
srem  key   member1 [memkber2]
<br>Pythonsrem set 'd' 'b'ss
复制<br><br><br>
sinter key [key ……]
<br>Python sinter m l	#求集合l和集合m的交集
复制<br><br>
sdiff key [key ……]
<br>Pythonsdiff m l	#求差集 注意比较顺序
复制<br><br>
sismember key member
<br>Pythonsissmember m 'a'   #集合m中是否存在元素'a'
复制<br><br><br><br><br><br><br>
zadd key score member [score member ……]
<br>Pythonzadd zset 1 a 5 b 3 c 2 d 4 e
复制<br><br>
Zincrby	key increment  mcfaember
<br>zincrby zset 10 'a'   #给a的权重上加10
复制<br><br><br>
zrange key start end
<br>Pythonzrange z1 0 -1
复制<br><br>
zcard key
<br>Python zcard z1
复制<br><br>
zcount key min max
<br><br>
zscore key member
<br>Pythonzscore l 'c'	#s返回c的权重
复制<br><br>
ZRANGE key 0 -1 WITHSCORES
<br><br>
ZRANGEBYSCORE key min max [WITHSCORES][LIMIT offset count]
<br>区间及无限<br>min和max可以是-inf和+inf，这样一来，你就可以在不知道有序集的最低和最高score值的情况下，使用ZRANGEBYSCORE这类命令。<br>Sqlredis&gt; ZADD myzset 1 "one"
(integer) 1
redis&gt; ZADD myzset 2 "two"
(integer) 1
redis&gt; ZADD myzset 3 "three"
(integer) 1
redis&gt; ZRANGEBYSCORE myzset -inf +inf
1) "one"
2) "two"
3) "three"
redis&gt; ZRANGEBYSCORE myzset 1 2
1) "one"
2) "two"
复制<br><br><br>当key存在，但是其不是有序集合类型，就返回一个错误。<br>
ZREM key member [member ...]
<br>Sqlredis&gt; ZADD myzset 1 "one"
(integer) 1
redis&gt; ZADD myzset 2 "two"
(integer) 1
redis&gt; ZADD myzset 3 "three"
(integer) 1
redis&gt; ZREM myzset "two"
(integer) 1
redis&gt; ZRANGE myzset 0 -1 WITHSCORES
1) "one"
2) "1"
3) "three"
4) "3"
redis&gt; 
复制<br><br>默认在 数据库 0<br>select num  进行数据库的切换<br>select  1  进入到数据库1<br><br>注意：当前密码修改后如果服务重启则需要重新设定<br>我们可以通过 redis 的配置文件设置密码参数，这样客户端连接到 redis 服务就需要密码验证，这样可以让你的 redis 服务更安全。<br><br>我们可以通过以下命令查看是否设置了密码验证：<br>127.0.0.1:6379&gt; CONFIG get requirepass
1) "requirepass"
2) ""
复制<br>默认情况下 requirepass 参数是空的，这就意味着你无需通过密码验证就可以连接到 redis 服务。<br>你可以通过以下命令来修改该参数：<br>127.0.0.1:6379&gt; CONFIG set requirepass "lucky"
OK
127.0.0.1:6379&gt; CONFIG get requirepass
1) "requirepass"
2) "lucky"
复制<br>设置密码后，客户端连接 redis 服务就需要密码验证，否则无法执行命令。<br><br>AUTH 命令基本语法格式如下：<br>127.0.0.1:6379&gt; AUTH password
复制<br><br>127.0.0.1:6379&gt; AUTH "lucky"
OK
127.0.0.1:6379&gt; SET mykey "Test value"
OK
127.0.0.1:6379&gt; GET mykey
"Test value"
复制<br><br>Redis SAVE 命令用于创建当前数据库的备份。<br><br>redis Save 命令基本语法如下：<br>redis 127.0.0.1:6379&gt; SAVE 
复制<br><br>redis 127.0.0.1:6379&gt; SAVE 
OK
复制<br>该命令将在 redis 安装目录中创建dump.rdb文件。<br><br><br>如果需要恢复数据，只需将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可。获取 redis 目录可以使用 CONFIG 命令，如下所示：<br>redis 127.0.0.1:6379&gt; CONFIG GET dir
1) "dir"
2) "/usr/local/redis/bin"
复制<br>以上命令 CONFIG GET dir 输出的 redis 安装目录为 /usr/local/redis/bin。<br><br><br>创建 redis 备份文件也可以使用命令 BGSAVE，该命令在后台执行。<br><br>127.0.0.1:6379&gt; BGSAVE

Background saving started
复制<br><br><br>pip install redis<br>导入<br>import  redis<br><br>redis提供了2个方法<br>
<br>StrictRedis：实现大部分官方的命令
<br>Redis：是StrictRedis的子类，用于向后兼容旧版的redis。
<br><br><br>Pythonimport redis

# decode_responses=True  自动解码
r = redis.Redis(host='127.0.0.1',port=6379,password='123c456',db=0,decode_responses=True) #默认数据库为0 

r = redis.StrictRedis(host='10.10.2.14',port=6379,password='123456',decode_responses=True)
复制<br><br> 管理对一个redis server的所有连接，避免每次建立，释放连接的开销。默认，每个redis实例都会维护一个自己的连接池，可以直接建立一个连接池，作为参数传给redis，这样可以实现多个redis实例共享一个连接池。<br>举例（连接池）：<br>Pythonpool = redis.ConnectionPool(host='127.0.0.1',port=6379,db=0,password='123456',decode_responses=True)

r = redis.Redis(connection_pool=pool)
复制]]></description><link>爬虫\爬虫文档\redis.html</link><guid isPermaLink="false">爬虫/爬虫文档/redis.md</guid><pubDate>Mon, 16 Oct 2023 15:03:24 GMT</pubDate></item><item><title><![CDATA[Scrapy框架]]></title><description><![CDATA[<a class="tag" href="?query=tag:scrapy" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scrapy</a> 
 <br><br><a href=".?query=tag:scrapy" class="tag" target="_blank" rel="noopener">#scrapy</a> <br><br><br>前面我们学习了基础的爬虫实现方法和selenium以及mongodb数据库，那么接下来会我们学习一个上场率非常高的爬虫框架：scrapy<br><br>
<br>scrapy的基础概念和工作流程
<br>scrapy入门使用
<br><br><br>
<br>了解 scrapy的概念
<br>掌握 scrapy框架的运行流程
<br>掌握 scrapy框架的作用
<br><br>
<br>能够让开发过程方便、快速
<br>scrapy框架能够让我们的爬虫效率更高
<br><br>文档地址：<a rel="noopener" class="external-link" href="https://docs.scrapy.org/en/latest/" target="_blank">https://docs.scrapy.org/en/latest/</a><br>Scrapy 使用了Twisted['twɪstɪd]异步网络框架，可以加快我们的下载速度。<br>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，我们只需要实现少量的代码，就能够快速的抓取。<br><br>前面我们说Twisted是一个异步的网络框架，经常我们也听到一个词语叫做非阻塞，那么他们有什么区别呢？<br>“Scrapy框架01.assets/同步和异步-2520822.png” could not be found.<br>异步：调用在发出之后，这个调用就直接返回，不管有无结果；异步是过程。 非阻塞：关注的是程序在等待调用结果（消息，返回值）时的状态，指在不能立刻得到结果之前，该调用不会阻塞当前线程。<br><br><br>“Scrapy框架01.assets/爬虫流程-1-2520822.png” could not be found.<br><br>“Scrapy框架01.assets/爬虫流程-2-2520822.png” could not be found.<br><br>“Scrapy框架01.assets/爬虫流程-3-1047805-2520822.png” could not be found.<br><br>
<br>调度器把requests--&gt;引擎--&gt;下载中间件---&gt;下载器
<br>下载器发送请求，获取响应----&gt;下载中间件----&gt;引擎---&gt;爬虫中间件---&gt;爬虫
<br>爬虫提取url地址，组装成request对象----&gt;爬虫中间件---&gt;引擎---&gt;调度器
<br>爬虫提取数据---&gt;引擎---&gt;管道
<br>管道进行数据的处理和保存
<br><br>
<br>图中绿色线条的表示数据的传递
<br>注意图中中间件的位置，决定了其作用
<br>注意其中引擎的位置，所有的模块之前相互独立，只和引擎进行交互
<br><br>“Scrapy框架01.assets/scrapy组件-2520822.png” could not be found.<br><br>
<br>
scrapy的概念：Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架

<br>
scrapy框架的运行流程以及数据传递过程：

<br>调度器把requests--&gt;引擎--&gt;下载中间件---&gt;下载器
<br>下载器发送请求，获取响应----&gt;下载中间件----&gt;引擎---&gt;爬虫中间件---&gt;爬虫
<br>爬虫提取url地址，组装成request对象----&gt;爬虫中间件---&gt;引擎---&gt;调度器
<br>爬虫提取数据---&gt;引擎---&gt;管道
<br>管道进行数据的处理和保存


<br>
scrapy框架的作用：通过少量代码实现快速抓取

<br>
掌握scrapy中每个模块的作用： 
引擎(engine)：负责数据和信号在不同模块间的传递 调度器(scheduler)：实现一个队列，存放引擎发过来的request请求对象 
下载器(downloader)：发送引擎发过来的request请求，获取响应，并将响应交给引擎 
爬虫(spider)：处理引擎发过来的response，提取数据，提取url，并交给引擎 
管道(pipeline)：处理引擎传递过来的数据，比如存储 
下载中间件(downloader middleware)：可以自定义的下载扩展，比如设置代理ip 
爬虫中间件(spider middleware)：可以自定义request请求和进行response过滤

<br>
理解异步和非阻塞的区别：异步是过程，非阻塞是状态

<br><br><br>
<br>掌握 scrapy的安装
<br>应用 创建scrapy的项目
<br>应用 创建scrapy爬虫
<br>应用 运行scrapy爬虫
<br>应用 解析并获取scrapy爬虫中的数据
<br><br>
<br>创建一个scrapy项目:scrapy startproject mySpider
<br>生成一个爬虫:scrapy genspider myspider <a class="internal-link" data-href="www.xxx.cn" href="\www.xxx.cn" target="_self" rel="noopener">www.xxx.cn</a>
<br>提取数据:完善spider，使用xpath等方法
<br>保存数据:pipeline中保存数据
<br><br>安装scrapy命令：<br> pip install scrapy==2.5.1
 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scrapy==2.5.1 
 pip install scrapy-redis==0.7.2
复制<br>如果安装失败. 请先升级一下pip.  然后重新安装scrapy即可. <br>最新版本的pip升级完成后. 安装依然失败, 可以根据报错信息进行一点点的调整, 多试几次pip. 直至success. <br>注意：<br>如果上述过程还是无法正常安装scrapy, 可以考虑用下面的方案来安装:<br>如果上述过程还是无法正常安装scrapy, 可以考虑用下面的方案来安装:<br>
<br>
安装wheel
pip install wheel
复制

<br>
下载twisted安装包, <a rel="noopener" class="external-link" href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted" target="_blank">https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a>
“Scrapy框架01.assets/image-20210803144429440-2520822.png” could not be found.

<br>
用wheel安装twisted. 
pip install Twisted‑21.7.0‑py3‑none‑any.whl
复制

<br>
安装pywin32
pip install pywin32
复制

<br>
安装scrapy
pip install scrapy
复制

<br>总之, 最终你的控制台输入scrapy version能显示版本号. 就算成功了<br><br>创建scrapy项目的命令：scrapy startproject +&lt;项目名字&gt;<br>示例：scrapy startproject myspider<br><br>“Scrapy框架01.assets/scrapy入门使用-1-2520822.png” could not be found.<br><br>
<br>引擎(Scrapy)
用来处理整个系统的数据流处理, 触发事务(框架核心)
<br>调度器(Scheduler)
用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址
<br>下载器(Downloader)
用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)
<br>爬虫(Spiders)
爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面
<br>项目管道(Pipeline)
负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。
<br><br>命令：在项目路径下执行:scrapy genspider +&lt;爬虫名字&gt; + &lt;允许爬取的域名&gt;<br>示例：<br>
<br>scrapy startproject duanzi01
<br>cd duanzi01/
<br>scrapy genspider duanzi duanzixing.com
<br>生成的目录和文件结果如下：<br>“Scrapy框架01.assets/scrapy入门使用-2-2520822.png” could not be found.<br><br>完善spider即通过方法进行数据的提取等操作<br>在/duanzi01/duanzi01/spiders/duanzi.py中修改内容如下:<br>Python import scrapy
 
 # 自定义spider类，继承scrapy.spider
 class DuanziSpider(scrapy.Spider):
     # 爬虫名字
     name = 'duanzi'
     # 允许爬取的范围，防止爬虫爬到别的网站
     allowed_domains = ['duanzixing.com']
     # 开始爬取的url地址
     start_urls = ['http://duanzixing.com/']
 
     # 数据提取的方法，接受下载中间件传过来的response 是重写父类中的parse方法
     def parse(self, response, **kwargs):
         # 打印抓取到的页面源码
         # print(response.text)
         # xpath匹配每条段子的article列表
         article_list = response.xpath('//article[@class="excerpt"]')
         # print(article_list)
         # 循环获取每一个article
         for article in article_list:
             # 匹配标题
             # title = article.xpath('./header/h2/a/text()')
             # [&lt;Selector xpath='./header/h2/a/text()' data='一个不小心就把2000块钱的包包设置成了50包邮'&gt;]
             # title = article.xpath('./header/h2/a/text()')[0].extract()
             # 等同于
             title = article.xpath('./header/h2/a/text()').extract_first()
 
             # 获取段子内容
             con = article.xpath('./p[@class="note"]/text()').extract_first()
             print('title', title)
             print('con', con)
复制<br>启动爬虫命令： scrapy crawl  duanzi<br>response响应对象的常用属性<br>
<br>response.url：当前响应的url地址
<br>response.request.url：当前响应对应的请求的url地址
<br>response.headers：响应头
<br>response.request.headers：当前响应的请求头
<br>response.body：响应体，也就是html代码，byte类型
<br>response.text  返回响应的内容 字符串
<br>response.status：响应状态码
<br><br>
<br>
response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法

<br>
extract() 返回一个包含有字符串的列表  
如果使用列表调用extract()则表示，extract会将列表中每一个列表元素进行extract操作，返回列表

<br>
extract_first() 返回列表中的第一个字符串，列表为空没有返回None

<br>
spider中的parse方法必须有

<br>
需要抓取的url地址必须属于allowed_domains,但是start_urls中的url地址没有这个限制

<br>
启动爬虫的时候注意启动的位置，是在项目路径下启动

<br><br>
<br>
ROBOTSTXT_OBEY = False
robots是一种反爬协议。在协议中规定了哪些身份的爬虫无法爬取的资源有哪些。
在配置文件中setting，取消robots的监测：

<br>
在配置文件中配置全局的UA：USER_AGENT='xxxx'

<br>
在配置文件中加入日志等级：LOG_LEVEL = 'ERROR'  只输出错误信息
其它日志级别

<br>CRITICAL  严重错误
<br>ERROR  错误
<br>WARNING  警告
<br>INFO  消息
<br>DEBUG   调试


<br>代码实例<br>Python # Scrapy settings for mySpider project
 USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'
 
 ROBOTSTXT_OBEY = False
 
 LOG_LEVEL = 'ERROR'
复制<br><br><br>
<br>
代码配置
/myspider/myspider/spiders/ITSpider.py
Python class ITSpider(scrapy.Spider):
     name = 'ITSpider'
     # allowed_domains = ['www.xxx.com']
     start_urls = ['https://duanzixing.com/page/2/']
 
     # 通过终端写入文件的方式
     def parse(self, response):
         article_list = response.xpath('/html/body/section/div/div/article')
         # 创建列表， 存储数据
         all_data = []
         for article in article_list:
             title = article.xpath('./header/h2/a/text()').extract_first()
             con = article.xpath('./p[2]/text()').extract_first()
             dic = {
                 'title': title,
                 'con': con
             }
             all_data.append(dic)
         return all_data
复制

<br>
终端命令
scrapy crawl ITSpider -o ITSpider.csv  
将文件存储到ITSpider.csv  文件中

<br><br>先跟着配置 后面会单讲<br>代码配置<br>
<br>
打开items.py文件 添加如下代码
myspider/myspider/items.py
Python import scrapy
 
 
 class MyspiderItem(scrapy.Item):
     # define the fields for your item here like:
     # name = scrapy.Field()
     title = scrapy.Field()
     con = scrapy.Field()
复制

<br>
/myspider/myspider/spiders/ITSpider.py
Python import scrapy
 from myspider.items import MyspiderItem
 
 class ITSpiderSpider(scrapy.Spider):
     name = 'ITSpider'
     # allowed_domains = ['www.xxx.com']
     start_urls = ['https://duanzixing.com/page/2/']
 
     # 写入管道 持久化存储
     def parse(self, response):
         article_list = response.xpath('/html/body/section/div/div/article')
         for article in article_list:
             title = article.xpath('./header/h2/a/text()').extract_first()
             con = article.xpath('./p[2]/text()').extract_first()
             item = DuanziproItem()
             item['title'] = title
             item['con'] = con
             yield item
复制
在爬虫文件ITSpider.py中parse()函数中最后添加
Python yield item
复制
思考：为什么要使用yield？

<br>让整个函数变成一个生成器，有什么好处呢？
<br>遍历这个函数的返回值的时候，挨个把数据读到内存，不会造成内存的瞬间占用过高
<br>python3中的range和python2中的xrange同理

注意：yield能够传递的对象只能是：BaseItem,Request,dict,None

<br>
打开管道文件 pipelines.py  添加如下代码 
myspider/myspider/pipelines.py
 class ITSpiderPipeline:
     f = None
     def open_spider(self, spider):
         print('爬虫开始时被调用一次')
         self.f = open('./duanzi.text', 'w')
         
     # 爬虫文件中提取数据的方法每yield一次item，就会运行一次
     # 该方法为固定名称函数
     def process_item(self, item, spider):
         print(item)
         self.f.write(item['title']+item['con']+'\n')
         return item
 
     def close_spider(self, spider):
         print('爬虫结束时被调用')
         self.f.close()
复制

<br>
open_spider方法
重写父类中open_spider方法  只有爬虫开始十被调用一次

<br>
close_spider 方法
重写父类中lose_spider方法  爬虫结束时被调用一次



<br>
在settings.py设置开启pipeline
将默认被注释的管道打开
 ITEM_PIPELINES = {
    'myspider.pipelines.MyspiderPipeline': 300,
 }
复制
其中数值代表优先级  数值越小优先级越高

<br><br>命令：在项目目录下执行scrapy crawl +&lt;爬虫名字&gt;<br>示例：scrapy crawl ITSpider<br><br>
<br>srapy的安装：pip install scrapy
<br>创建scrapy的项目: scrapy startproject myspider
<br>创建scrapy爬虫：在项目目录下执行 scrapy genspider ITSpider <a class="internal-link" data-href="www.xxx.cn" href="\www.xxx.cn" target="_self" rel="noopener">www.xxx.cn</a>
<br>运行scrapy爬虫：在项目目录下执行 scrapy crawl ITSpider
<br>解析并获取scrapy爬虫中的数据：

<br>response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法
<br>extract() 返回一个包含有字符串的列表
<br>extract_first() 返回列表中的第一个字符串，列表为空没有返回None


<br>scrapy管道的基本使用:

<br>完善pipelines.py中的process_item函数
<br>在settings.py中设置开启pipeline


]]></description><link>爬虫\爬虫文档\Scrapy01-框架初认识.html</link><guid isPermaLink="false">爬虫/爬虫文档/Scrapy01-框架初认识.md</guid><pubDate>Mon, 16 Oct 2023 15:10:16 GMT</pubDate></item><item><title><![CDATA[Scrapy深入使用-存储]]></title><description><![CDATA[<a class="tag" href="?query=tag:scrapy" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scrapy</a> <a class="tag" href="?query=tag:框架" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#框架</a> <a class="tag" href="?query=tag:sql" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#sql</a> <a class="tag" href="?query=tag:MGdb" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#MGdb</a> 
 <br><br><a href=".?query=tag:scrapy" class="tag" target="_blank" rel="noopener">#scrapy</a> <a href=".?query=tag:框架" class="tag" target="_blank" rel="noopener">#框架</a><br><br><br>
<br>了解 scrapy debug信息
<br>了解 scrapy shell的使用
<br>掌握 scrapy的settings.py设置
<br>掌握 scrapy管道(pipelines.py)的使用
<br>掌握scrapy下载图片
<br><br><br>“Scrapy02-存储.assets/scrapy_debug.png” could not be found.<br><br>scrapy shell是scrapy提供的一个终端工具，能够通过它查看scrapy中对象的属性和方法，以及测试xpath<br>使用方法：<br>Linuxscrapy shell http://www.baidu.com
复制<br>在终端输入上述命令后，能够进入python的交互式终端，此时可以使用：<br>
<br>response.xpath()：直接测试xpath规则是否正确
<br>response.url：当前响应的url地址
<br>response.request.url：当前响应对应的请求的url地址
<br>response.headers：响应头
<br>response.body：响应体，也就是html代码，默认是byte类型
<br>response.request.headers：当前响应的请求头
<br><br><br>在配置文件中存放一些公共变量，在后续的项目中方便修改，如：本地测试数据库和部署服务器的数据库不一致<br><br>
<br>变量名一般全部大写
<br>导入即可使用
<br><br>
<br>
USER_AGENT 设置ua

<br>
ROBOTSTXT_OBEY 是否遵守robots协议，默认是遵守

<br>
CONCURRENT_REQUESTS 设置并发请求的数量，默认是16个

<br>
DOWNLOAD_DELAY 下载延迟，默认无延迟 （下载器在从同一网站下载连续页面之前应等待的时间（以秒为单位）。这可以用来限制爬行速度，以避免对服务器造成太大影响）

<br>
COOKIES_ENABLED 是否开启cookie，即每次请求带上前一次的cookie，默认是开启的

<br>
DEFAULT_REQUEST_HEADERS 设置默认请求头，这里加入了USER_AGENT将不起作用

<br>
SPIDER_MIDDLEWARES 爬虫中间件，设置过程和管道相同

<br>
DOWNLOADER_MIDDLEWARES 下载中间件

<br>
LOG_LEVEL 控制终端输出信息的log级别，终端默认显示的是debug级别的log信息

<br>LOG_LEVEL = "WARNING"

<br>CRITICAL  严重
<br>ERROR  错误
<br>WARNING  警告
<br>INFO  消息
<br>DEBUG   调试




<br>
LOG_FILE 设置log日志文件的保存路径，如果设置该参数，终端将不再显示信息
LOG_FILE = "./test.log"

<br>
其他设置参考：<a rel="noopener" class="external-link" href="https://www.jianshu.com/p/df9c0d1e9087" target="_blank">https://www.jianshu.com/p/df9c0d1e9087</a>

<br><br>
之前我们在scrapy入门使用一节中学习了管道的基本使用，接下来我们深入的学习scrapy管道的使用
<br><br>
<br>
代码配置
/myspider/myspider/spiders/ITSpider.py
Pythonclass ITSpider(scrapy.Spider):
    name = 'ITSpider'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://duanzixing.com/page/1/']

    # 通过终端写入文件的方式
    def parse(self, response):
        article_list = response.xpath('/html/body/section/div/div/article')
        # 创建列表， 存储数据
        all_data = []
        for article in article_list:
            title = article.xpath('./header/h2/a/text()').extract_first()
            con = article.xpath('./p[2]/text()').extract_first()
            dic = {
                'title': title,
                'con': con
            }
            all_data.append(dic)
        return all_data
复制

<br>
终端命令
scrapy crawl 爬虫名称-o 文件名.csv  
scrapy crawl ITSpider -o ITSpider.csv  
将文件存储到ITSpider.csv  文件中

<br><br><br><br>
<br>
scrapy startproject doubanfile

<br>
cd doubanfile

<br>
scrapy genspider db movie.douban.com/chart

<br><br>抓取：<br>
<br>封面
<br>电影名称
<br>主演
<br><br>LOG_LEVEL = 'ERROR'
ROBOTSTXT_OBEY = False
复制<br><br>先抓取每一行数据的tr列表<br>Pythonimport scrapy


class DbSpider(scrapy.Spider):
    name = 'db'
    allowed_domains = ['https://movie.douban.com/chart']
    start_urls = ['https://movie.douban.com/chart']

    def parse(self, resp, **kwargs):
        print(resp.text)
        # 先获取到每一行数据的tr
        tr_list = resp.xpath('//div[@class="indent"]/div/table/tr[@class="item"]')
        print(tr_list)
复制<br>此刻运行打印<br>srapy crawl db<br>发现无任何打印，将LOG_LEVEL = 'ERROR' 更改为 INFO  ，发现此刻请求没有权限<br>“Scrapy02-存储.assets/image-20220907143413102.png” could not be found.<br>发现问题所在后修改settings.py<br>在默认请求头DEFAULT_REQUEST_HEADERS中添加User-Agent<br>PythonDEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en',
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'
}
复制<br>再次请求，成功<br><br>Pythonimport scrapy
import re

class DbSpider(scrapy.Spider):
    name = 'db'
    allowed_domains = ['https://movie.douban.com/chart']
    start_urls = ['https://movie.douban.com/chart']

    def parse(self, resp, **kwargs):
        # print(resp.text)
        # 先获取到每一行数据的tr
        tr_list = resp.xpath('//div[@class="indent"]/div/table/tr[@class="item"]')
        for tr in tr_list:
            # 获取封面
            img_src = tr.xpath('./td[1]/a/img/@src').extract_first()
            # 电影名称
            name = tr.xpath('./td[2]/div[@class="pl2"]/a//text()').extract_first()
            # 去除空白字符使用replace替换
            # name = name.replace('\n', '').replace('\r', '').replace('/', '').replace(' ', '')
            # 去除空白字符使用正则替换
            name = re.sub('(/)|(\s)', '', name)
            # 主演
            to_star = tr.xpath('./td[2]/div[@class="pl2"]/p[@class="pl"]/text()').extract_first()
复制<br><br>添加如下代码<br>属性名称和当前爬虫db.py中抓到要存储数据的变量一致 否则报错<br>Pythonimport scrapy


class DoubanfileItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    img_src = scrapy.Field()
    name = scrapy.Field()
    to_star = scrapy.Field()
复制<br><br>Pythonimport scrapy
import re
from doubanfile.items import DoubanfileItem

class DbSpider(scrapy.Spider):
    name = 'db'
    allowed_domains = ['https://movie.douban.com/chart']
    start_urls = ['https://movie.douban.com/chart']

    def parse(self, resp, **kwargs):
        item = DoubanfileItem()  # 实例化item类
        # print(resp.text)
        # 先获取到每一行数据的tr
        tr_list = resp.xpath('//div[@class="indent"]/div/table/tr[@class="item"]')
        for tr in tr_list:
            # 获取封面
            item['img_src'] = tr.xpath('./td[1]/a/img/@src').extract_first()
            # 电影名称
            name = tr.xpath('./td[2]/div[@class="pl2"]/a//text()').extract_first()
            # 去除空白字符使用replace替换
            # name = name.replace('\n', '').replace('\r', '').replace('/', '').replace(' ', '')
            # 去除空白字符使用正则替换
            item['name'] = re.sub('(/)|(\s)', '', name)
            # 主演
            item['to_star'] = tr.xpath('./td[2]/div[@class="pl2"]/p[@class="pl"]/text()').extract_first()
            yield item
复制<br>思考：为什么要使用yield？<br>
<br>让整个函数变成一个生成器，有什么好处呢？
<br>遍历这个函数的返回值的时候，挨个把数据读到内存，不会造成内存的瞬间占用过高
<br>python3中的range和python2中的xrange同理
<br>注意：yield能够传递的对象只能是：BaseItem,Request,dict,None<br><br>pipeline中常用的方法：<br>
<br>process_item(self,item,spider):实现对item数据的处理
<br>open_spider(self, spider): 在爬虫开启的时候仅执行一次
<br>close_spider(self, spider): 在爬虫关闭的时候仅执行一次
<br>settings.py 打开当前注释<br>PythonITEM_PIPELINES = {
   'doubanfile.pipelines.DoubanfilePipeline': 300,
}
复制<br><br>Pythonclass DoubanfilePipeline:
    f = None
    def open_spider(self, item):
        self.f = open('./db.text', 'w')
    def process_item(self, item, spider):
        print(item)
        self.f.write(item['img_src']+'\n')
        self.f.write(item['name']+'\n')
        self.f.write(item['to_star']+'\n')
        return item

    def close_spider(self, item):
        self.f.close()
复制<br>注意：<br>当前process_item中的return item必须存在，如果当前爬虫存在于多个管道的时候，如果没有return item 则下一个管道不能获取到当前的item数据<br><br><a href=".?query=tag:sql" class="tag" target="_blank" rel="noopener">#sql</a> <br><br><br>
<br>
scrapy startproject doubanmysql

<br>
cd doubanmysql

<br>
scrapy genspider db movie.douban.com/chart

<br><br>抓取：<br>
<br>封面
<br>电影名称
<br>主演
<br><br>Python# 设置日志级别
LOG_LEVEL = 'ERROR'
ROBOTSTXT_OBEY = False
# 设置请求头
DEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en',
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'
}
# 开启管道
ITEM_PIPELINES = {
   'doubanmysql.pipelines.DoubanmysqlPipeline': 300,
}
复制<br><br>Sqlcreate database douban character set utf8;
use douban
CREATE TABLE `douban` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `img_src` varchar(200) NOT NULL COMMENT '封面地址',
  `name` varchar(50) NOT NULL COMMENT '电影名称',
  `to_star` varchar(250) NOT NULL COMMENT '主演',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8
复制<br>示例SQL语句<br>Pythoninsert into info(img_src, name, to_star) values("https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2673202034.webp", "尼罗河上的惨案 Death on the Nile", "肯尼思·布拉纳", "迈克尔·格林 / 阿加莎·克里斯蒂", "肯尼思·布拉纳艾玛·麦基/艾米·汉莫/珍妮弗·桑德斯/苏菲·奥康内多/安妮特·贝宁/妮基塔·查达哈/汤姆·巴特曼/亚当·加西亚/汉/爱德华·刘易斯·弗伦奇/拉普洛斯·卡伦福佐斯")
复制<br><br>Pythonimport scrapy


class DoubanmysqlItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    img_src = scrapy.Field()
    name = scrapy.Field()
    to_star = scrapy.Field()
复制<br><br>Pythonimport scrapy
from doubanmysql.items import DoubanmysqlItem
import re

class DbSpider(scrapy.Spider):
    name = 'db'
    allowed_domains = ['movie.douban.com/chart']
    start_urls = ['http://movie.douban.com/chart/']

    def parse(self, resp, **kwargs):
        item = DoubanmysqlItem()  # 实例化item类
        # print(resp.text)
        # 先获取到每一行数据的tr
        tr_list = resp.xpath('//div[@class="indent"]/div/table/tr[@class="item"]')
        for tr in tr_list:
            # 获取封面
            item['img_src'] = tr.xpath('./td[1]/a/img/@src').extract_first()
            # 电影名称
            name = tr.xpath('./td[2]/div[@class="pl2"]/a//text()').extract_first()
            # 去除空白字符使用replace替换
            # name = name.replace('\n', '').replace('\r', '').replace('/', '').replace(' ', '')
            # 去除空白字符使用正则替换
            item['name'] = re.sub('(/)|(\s)', '', name)
            # 主演
            item['to_star'] = tr.xpath('./td[2]/div[@class="pl2"]/p[@class="pl"]/text()').extract_first()
            yield item
复制<br><br>Pythonfrom itemadapter import ItemAdapter
import pymysql

class DoubanmysqlPipeline:
    db = None
    cursor = None
    def open_spider(self, spider):
        # 判断当前运行的是否为db爬虫，不是db爬虫则下面代码不执行
        # 当前仅限于一个scrapy下有多个爬虫工程
        if spider.name == 'db':
            self.db = pymysql.connect(host='127.0.0.1', port=3306, db='douban', user='root', passwd='123456', charset='utf8')
            self.cursor = self.db.cursor()

    def process_item(self, item, spider):
        # 判断当前运行的是否为db爬虫
        if spider.name == 'db':
            try:
                sql = f'insert into douban(img_src, name, to_star) values("{item["img_src"]}", "{item["name"]}", "{item["to_star"]}")'
                self.cursor.execute(sql)
                self.db.commit()
            except Exception as e:
                print(e)
                print(sql)
                self.db.rollback()
				return item
    def close_spider(self, item):
        # 关闭数据库连接
        self.db.close()
复制<br><br>scrapy crawl db<br>查看终端数据是否有报错，如果没有报错查看数据库数据是否存储成功<br>“Scrapy02-存储.assets/image-20220907153347418.png” could not be found.<br><br><a href=".?query=tag:MGdb" class="tag" target="_blank" rel="noopener">#MGdb</a> <br><br><br>
<br>
scrapy startproject doubanmongodb

<br>
cd doubanmongodb

<br>
scrapy genspider db movie.douban.com/chart

<br><br>抓取：<br>
<br>封面
<br>电影名称
<br>主演
<br><br>Python# 设置日志级别
LOG_LEVEL = 'ERROR'
ROBOTSTXT_OBEY = False
# 设置请求头
DEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en',
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'
}
# 开启管道
ITEM_PIPELINES = {
   'doubanmongodb.pipelines.DoubanmongodbPipeline': 300,
}
复制<br><br>Pythonimport scrapy


class DoubanmongodbItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    img_src = scrapy.Field()
    name = scrapy.Field()
    to_star = scrapy.Field()
复制<br><br>Pythonimport scrapy
from doubanmongodb.items import DoubanmongodbItem
import re


class DbSpider(scrapy.Spider):
    name = 'db'
    allowed_domains = ['movie.douban.com/chart']
    start_urls = ['http://movie.douban.com/chart/']

    def parse(self, resp, **kwargs):
        item = DoubanmongodbItem()  # 实例化item类
        # print(resp.text)
        # 先获取到每一行数据的tr
        tr_list = resp.xpath('//div[@class="indent"]/div/table/tr[@class="item"]')
        for tr in tr_list:
            # 获取封面
            item['img_src'] = tr.xpath('./td[1]/a/img/@src').extract_first()
            # 电影名称
            name = tr.xpath('./td[2]/div[@class="pl2"]/a//text()').extract_first()
            # 去除空白字符使用replace替换
            # name = name.replace('\n', '').replace('\r', '').replace('/', '').replace(' ', '')
            # 去除空白字符使用正则替换
            item['name'] = re.sub('(/)|(\s)', '', name)
            # 主演
            item['to_star'] = tr.xpath('./td[2]/div[@class="pl2"]/p[@class="pl"]/text()').extract_first()
            yield item
复制<br><br>Pythonfrom itemadapter import ItemAdapter
from pymongo import MongoClient

class DoubanmongodbPipeline:
    con = None
    collection = None
    def open_spider(self, spider):  # 在爬虫开启的时候仅执行一次
        if spider.name == 'db':
            self.con = MongoClient(host='127.0.0.1', port=27017) # 实例化mongoclient
            self.collection = self.con.spider.douban  # 创建数据库名为spider,集合名为douban的集合操作对象

    def process_item(self, item, spider):
        if spider.name == 'db':
            # print(spider.name)
            self.collection.insert_one(dict(item)) # 此时item对象需要先转换为字典,再插入
        # 不return的情况下，另一个权重较低的pipeline将不会获得item
        return item

    def close_spider(self, item):
        # 关闭数据库连接
        self.con.close()
复制<br>注意：<br>需要开启mongo服务<br>mongod.exe  --dbpath=C:/User/xxx/db<br>新开终端<br>mongo.exe<br><br>scrapy crawl db<br>查看终端数据是否有报错，如果没有报错查看数据库数据是否存储成功<br>“Scrapy02-存储.assets/image-20220907154832959.png” could not be found.<br><br><br><br>
<br>
scrapy startproject douban

<br>
cd douban

<br>
scrapy genspider db movie.douban.com/chart

<br><br>抓取：<br>
<br>封面
<br>电影名称
<br>主演
<br><br>Python# 设置日志级别
LOG_LEVEL = 'ERROR'
ROBOTSTXT_OBEY = False
# 设置请求头
DEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en',
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'
}
# 开启管道
ITEM_PIPELINES = {
   'douban.pipelines.DoubanPipeline': 300,
}
复制<br><br>Pythonimport scrapy


class DoubanItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    img_src = scrapy.Field()
    name = scrapy.Field()
    to_star = scrapy.Field()
复制<br><br>Pythonimport scrapy
from douban.items import DoubanItem
import re


class DbSpider(scrapy.Spider):
    name = 'db'
    allowed_domains = ['movie.douban.com/chart']
    start_urls = ['http://movie.douban.com/chart/']

    def parse(self, resp, **kwargs):
        item = DoubanItem()  # 实例化item类
        # print(resp.text)
        # 先获取到每一行数据的tr
        tr_list = resp.xpath('//div[@class="indent"]/div/table/tr[@class="item"]')
        for tr in tr_list:
            # 获取封面
            item['img_src'] = tr.xpath('./td[1]/a/img/@src').extract_first()
            # 电影名称
            name = tr.xpath('./td[2]/div[@class="pl2"]/a//text()').extract_first()
            # 去除空白字符使用replace替换
            # name = name.replace('\n', '').replace('\r', '').replace('/', '').replace(' ', '')
            # 去除空白字符使用正则替换
            item['name'] = re.sub('(/)|(\s)', '', name)
            # 主演
            item['to_star'] = tr.xpath('./td[2]/div[@class="pl2"]/p[@class="pl"]/text()').extract_first()
            yield item
复制<br><br>Python# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter
import pymysql
from pymongo import MongoClient


class DoubanFilePipeline:
    '''
    设置文件存储
    '''
    f = None
    def open_spider(self, item):
        self.f = open('./db.text', 'w')

    def process_item(self, item, spider):
        print(item)
        self.f.write(item['img_src'] + '\n')
        self.f.write(item['name'] + '\n')
        self.f.write(item['to_star'] + '\n')
        return item

    def close_spider(self, item):
        self.f.close()

class DoubanmysqlPipeline:
    '''
    存储到MySQL数据库中
    '''
    db = None
    cursor = None
    def open_spider(self, spider):
        # 判断当前运行的是否为db爬虫，不是db爬虫则下面代码不执行
        # 当前仅限于一个scrapy下有多个爬虫工程
        if spider.name == 'db':
            self.db = pymysql.connect(host='127.0.0.1', port=3306, db='douban', user='root', passwd='123456', charset='utf8')
            self.cursor = self.db.cursor()

    def process_item(self, item, spider):
        # 判断当前运行的是否为db爬虫
        if spider.name == 'db':
            try:
                sql = f'insert into douban(img_src, name, to_star) values("{item["img_src"]}", "{item["name"]}", "{item["to_star"]}")'
                self.cursor.execute(sql)
                self.db.commit()
            except Exception as e:
                print(e)
                print(sql)
                self.db.rollback()
        return item
    def close_spider(self, item):
        # 关闭数据库连接
        self.db.close()


class DoubanmongodbPipeline:
    '''
    存储到MongoDB数据库中
    '''
    con = None
    collection = None
    def open_spider(self, spider):  # 在爬虫开启的时候仅执行一次
        if spider.name == 'db':
            self.con = MongoClient(host='127.0.0.1', port=27017) # 实例化mongoclient
            self.collection = self.con.spider.douban  # 创建数据库名为spider,集合名为douban的集合操作对象

    def process_item(self, item, spider):
        if spider.name == 'db':
            # print(spider.name)
            self.collection.insert_one(dict(item)) # 此时item对象需要先转换为字典,再插入
        # 不return的情况下，另一个权重较低的pipeline将不会获得item
        return item

    def close_spider(self, item):
        # 关闭数据库连接
        self.con.close()
复制<br><br>PythonITEM_PIPELINES = {
   'douban.pipelines.DoubanFilePipeline': 300,  # 300表示权重
   'douban.pipelines.DoubanmysqlPipeline': 400,
   'douban.pipelines.DoubanmongodbPipeline': 500,
}
复制<br>您在此设置中分配给类的整数值决定了它们运行的顺序：项目从低值到高值的类。通常将这些数字定义在 0-1000 范围内。<br>思考：pipeline在settings中能够开启多个，为什么需要开启多个？<br>
<br>不同的pipeline可以处理不同爬虫的数据，通过spider.name属性来区分
<br>不同的pipeline能够对一个或多个爬虫进行不同的数据处理的操作，比如一个进行数据清洗，一个进行数据的保存
<br>同一个管道类也可以处理不同爬虫的数据，通过spider.name属性来区分
<br><br>
<br>使用之前需要在settings中开启
<br>pipeline在setting中键表示位置(即pipeline在项目中的位置可以自定义)，值表示距离引擎的远近，越近数据会越先经过
<br>有多个pipeline的时候，process_item的方法必须return item,否则后一个pipeline取到的数据为None值
<br>pipeline中process_item的方法必须有，否则item没有办法接受和处理
<br>process_item方法接受item和spider，其中spider表示当前传递item过来的spider
<br>open_spider(spider) :能够在爬虫开启的时候执行一次
<br>close_spider(spider) :能够在爬虫关闭的时候执行一次
<br>上述俩个方法经常用于爬虫和数据库的交互，在爬虫开启的时候建立和数据库的连接，在爬虫关闭的时候断开和数据库的连接
<br><br>
<br>debug能够展示当前程序的运行状态 
<br>scrapy shell能够实现xpath的测试和对象属性和方法的尝试
<br>scrapy的settings.py能够实现各种自定义的配置，比如下载延迟和请求头等
<br>管道能够实现数据的清洗和保存，能够定义多个管道实现不同的功能，其中有个三个方法

<br>process_item(self,item,spider):实现对item数据的处理
<br>open_spider(self, spider): 在爬虫开启的时候仅执行一次
<br>close_spider(self, spider): 在爬虫关闭的时候仅执行一次


<br><br><br>在之前的抓取中我们都是抓取当前再生成spider时候的网址中的数据，那如果我们想要访问当前数据中的子页面的数据，那又该如何操作呢，回忆下我们在前面requests课程中是如何抓取子页面数据的<br>思路：<br>
<br>先对第一层url进行请求
<br>请求返回数据进行解析循环  找到每一条子页面的url
<br>找到子页面的url以后进行再次请求
<br>请求解析子页面请求返回的数据
<br>结束
<br>那么在我们scrapy中的思路也是一样的，通过抓取当前第一层页面中解析出来的子页面的数据，在通过scrapy.Rquests进行子页面的请求，那我们了解了当前抓取的思路，就开始我们scrapy子页面的请求与抓取吧<br><br><br>
<br>
scrapy startproject doubandetail

<br>
cd doubandetail

<br>
scrapy genspider db movie.douban.com/chart

<br><br>抓取：<br>
<br>电影名称
<br>导演
<br>编剧
<br>主演
<br>类型
<br>通过第一层匹配到的子页面的url进行请求详情页的url数据<br>“Scrapy02-存储.assets/image-20220907165307805.png” could not be found.<br><br><br>Python# 设置日志级别
LOG_LEVEL = 'ERROR'
ROBOTSTXT_OBEY = False
COOKIES_ENABLED = False  # cookies的中间件将不起作用，下面的cookie起作用

# 设置请求头
DEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en',
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36',
  'Cookie': '设定cookie 防止反扒'
}
# 开启管道
ITEM_PIPELINES = {
   'doubandetail.pipelines.DoubandetailPipeline': 300,
}
复制<br><br>
<br>
概述
因为当前需要对详情页面再次请求以获取详情页数据，请求思路和之前requests一样，只是在这里我们使用	yield scrapy.Request() 进行请求 默认请求方式为get

<br>
yield scrapy.Request(url, callback=self.parse_deatil)

<br>
url：为再次请求的URL地址

<br>
callback：请求后进行处理的回调方法

<br>
method: 请求方式

<br>
callback: 回调函数

<br>
errback: 报错回调

<br>
dont_filter: 默认False, 表示"不过滤", 该请求会重新进行发送

<br>
headers: 请求头

<br>
cookies: cookie信息



<br>
代码实现
Pythonimport scrapy
from doubandetail.items import DoubandetailItem
import re

class DbSpider(scrapy.Spider):
    name = 'db'
    # 需要注释掉
    # allowed_domains = ['movie.douban.com/chart']
    start_urls = ['http://movie.douban.com/chart/']

    def parse(self, resp, **kwargs):
        print(resp.text)
        # 先获取到每一行数据的tr
        tr_list = resp.xpath('//div[@class="indent"]/div/table/tr[@class="item"]')
        for tr in tr_list:
            # 获取每个详情页的url
            detail_url = tr.xpath('./td[1]/a/@href').extract_first()
            # 请求子页面
            print(detail_url)
            yield scrapy.Request(detail_url, callback=self.parse_deatil)

    # 解析子页面数据
    def parse_deatil(self, response):
        # 默认携带我们settings.py中所配置的请求头进行请求
        # print(response.request.headers)
        item = DoubandetailItem()
        item['name'] = response.xpath('//*[@id="content"]/h1/span[1]/text()').extract_first()  # 电影名称
        item['director'] = response.xpath('//*[@id="info"]/span[1]/span[2]/a/text()').extract_first()  # 导演
        item['screenwriter'] = ''.join(response.xpath('//*[@id="info"]/span[2]/span[2]//text()').extract())  # 编剧
        item['to_star'] = ''.join(response.xpath('//*[@id="info"]/span[3]/span[2]//text()').extract())  # 主演
        item['type'] = '/'.join(response.xpath('//span[@property="v:genre"]//text()').extract())  # 类型
        item['link_report'] = re.sub('(/)|(\s)|(\u3000)|(\'\n\')', '', link_report)
        print(item)
        return item
复制

<br>
注意：
需要将allowed_domains注释掉，否则详情页url不符合当前允许，所以会出现不请求的问题

<br><br>Pythonimport scrapy


class DoubandetailItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    name = scrapy.Field()  # 电影名称
    director = scrapy.Field()  # 导演
    screenwriter = scrapy.Field()  # 编剧
    to_star = scrapy.Field()  # 主演
    type = scrapy.Field()  # 类型
复制<br><br>Python# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter
from pymongo import MongoClient

class DoubandetailPipeline:
    con = None
    collection = None
    def open_spider(self, spider):  # 在爬虫开启的时候仅执行一次
        self.con = MongoClient(host='127.0.0.1', port=27017)  # 实例化mongoclient
        self.collection = self.con.spider.douban  # 创建数据库名为spider,集合名为douban的集合操作对象

    def process_item(self, item, spider):
        print(item)
        self.collection.insert_one(dict(item))  # 此时item对象需要先转换为字典,再插入
        return item

    def close_spider(self, item):
        # 关闭数据库连接
        self.con.close()
复制<br>注意：<br>如果访问频率过高被禁止访问，可以携带登录后的cooki进行访问<br><br><br>pip3 install pillow
复制<br><br><a rel="noopener" class="external-link" href="https://desk.zol.com.cn/dongman/" target="_blank">https://desk.zol.com.cn/dongman/</a><br><br>
<br>
scrapy startproject desk

<br>
cd desk

<br>
scrapy genspider img desk.zol.com.cn/dongman

<br><br>Python# 设置日志级别
LOG_LEVEL = 'ERROR'
ROBOTSTXT_OBEY = False
# 设置请求头
DEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en',
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'
}
复制<br><br>思路：<br>抓取到详情页中图片的url地址，交给图片管道进行下载<br>Pythonimport scrapy
from urllib.parse import urljoin


class ImgSpider(scrapy.Spider):
    name = 'img'
    # allowed_domains = ['desk.zol.com.cn/dongman']
    start_urls = ['http://desk.zol.com.cn/dongman/']

    def parse(self, resp, **kwargs):
        # 先抓取到每个图片详情的url
        url_list = resp.xpath('//ul[@class="pic-list2  clearfix"]/li/a/@href').extract()
        # 获取到url列表后 进行循环进行每一个url详情页的请求
        for url in url_list:
            # 因为抓取到的url并不完整，需要进行手动拼接
            # urljoin('https://desk.zol.com.cn/dongman/', '/bizhi/8301_103027_2.html')
            url = urljoin('https://desk.zol.com.cn/dongman/', url)
            # 拼凑完发现当前url中有下载exe的url，将其去除
            if url.find('exe') != -1:
                continue
            yield scrapy.Request(url, callback=self.parse_detail)

    # 对详情页进行解析
    def parse_detail(self, resp):
        # 获取当前详情页中最大尺寸图片的url
        max_img_url = resp.xpath('//dd[@id="tagfbl"]/a/@href').extract()
        # 判断当前最大图片的url地址，为倒数第二个，如果当前图片列表url长度小于2 则当前证明不是图片的url
        if len(max_img_url) &gt; 2:
            max_img_url = urljoin('https://desk.zol.com.cn/', max_img_url[0])
            # 对url页面进行请求 获取最终大图的页面
            yield scrapy.Request(max_img_url, callback=self.parse_img_detail)

    def parse_img_detail(self, resp):
        # 解析出大图的url
        img_src = resp.xpath("//img[1]/@src").extract_first()
        return {'img_src': img_src}
复制<br>注意：<br>如果抓取过程中遇到如下报错，可能是cryptography 版本问题<br>twisted.web._newclient.ResponseNeverReceived: [&lt;twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'unsafe legacy renegotiation disabled')]&gt;]
复制<br>解决：<br>pip uninstall cryptography
pip install cryptography==36.0.2<br><br>打开Pipelines文件夹<br>因为我们不能再像之前存储文本一样，使用之前的管道类（Pipeline），我们需要用到新的存储图片的管道类ImagesPipeline，因此我们需要先导入该类<br>pipelines.py<br>
<br>
导入
Pythonfrom scrapy.pipelines.images import ImagesPipeline
复制

<br>
定义一个Images类
Pythonfrom itemadapter import ItemAdapter
from scrapy.pipelines.images import ImagesPipeline
import scrapy

class DeskPipeline:
    def process_item(self, item, spider):
        return item

class Imgspipline(ImagesPipeline):
    # 1. 发送请求(下载图片, 文件, 视频,xxx)
    def get_media_requests(self, item, info):
      	# 获取到图片的url
        url = item['img_src']
        # 进行请求
        yield scrapy.Request(url=url, meta={"url": url})  # 直接返回一个请求对象即可

    # 2. 图片存储路径
    def file_path(self, request, response=None, info=None, *, item=None):
        # 当前获取请求的url的方式有2种
        # 获取到当前的url 用于处理下载图片的名称
        file_name = item['img_src'].split("/")[-1]  # 用item拿到url
        # file_name = request.meta['url'].split("/")[-1]  # 用meta传参获取
        return file_name

    # 3. 可能需要对item进行更新
    def item_completed(self, results, item, info):
        # print('results', results)
        for r in results:
            # 获取每个图片的路径
            print(r[1]['path'])
        return item  # 一定要return item 把数据传递给下一个管道
复制

<br><br>接着我们再定义一个保存数据的函数，并设置好存储的文件名，然后存储的路径需要在设置中（setting）文件中，添加IMAGE_STORE设置好存储的路径<br>开启图片管道<br>settings.py<br>PythonITEM_PIPELINES = {
   'desk.pipelines.DeskPipeline': 300,
   'desk.pipelines.Imgspipline': 400,  # 开启图片管道
}
# 配置存储图片的路径
IMG_STORE = './imgs'
复制]]></description><link>爬虫\爬虫文档\Scrapy02深入使用-存储.html</link><guid isPermaLink="false">爬虫/爬虫文档/Scrapy02深入使用-存储.md</guid><pubDate>Tue, 17 Oct 2023 12:44:04 GMT</pubDate></item><item><title><![CDATA[scrapy模拟登陆&amp;分页]]></title><description><![CDATA[<a class="tag" href="?query=tag:scrapy" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scrapy</a> <a class="tag" href="?query=tag:框架" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#框架</a> 
 <br><a href=".?query=tag:scrapy" class="tag" target="_blank" rel="noopener">#scrapy</a>  <a href=".?query=tag:框架" class="tag" target="_blank" rel="noopener">#框架</a><br><br><br><br>
<br>应用 scrapy直接携带cookie模拟登陆的方法
<br>应用 scrapy.FormRequest()发送post请求进行登陆
<br>应用 scrapy.FormRequest.from_response()发送表单请求
<br><br><br><br>
<br>直接携带cookies请求页面
<br>找url地址，发送post请求存储cookie
<br><br>
<br>找到对应的input标签，输入文本点击登陆
<br><br>
<br>直接携带cookies
<br>找url地址，发送post请求存储cookie
<br>找到对应的form表单，自动解析input标签，自动解析post请求的url地址，自动带上数据，自动发送请求
<br><br>17k小说网<br>https://user.17k.com/
复制<br><br>
<br>cookie过期时间很长，常见于一些不规范的网站
<br>能在cookie过期之前把搜有的数据拿到
<br>配合其他程序使用，比如其使用selenium把登陆之后的cookie获取到保存到本地，scrapy发送请求之前先读取本地cookie
<br><br>settings.py<br>PythonDEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en',
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36',
  'Cookie': 'ASP.NET_SessionId=n4lwamv5eohaqcorfi3dvzcv; xiaohua_visitfunny=103174; xiaohua_web_userid=120326; xiaohua_web_userkey=r2sYazfFMt/rxUn8LJDmUYetwR2qsFCHIaNt7+Zpsscpp1p6zicW4w=='
}
复制<br>注意：需要打开COOKIES_ENABLED，否则上面设定的cookie将不起作用<br>Python# Disable cookies (enabled by default)
COOKIES_ENABLED = False
复制<br>xiaoshuo.py<br>Pythonimport scrapy


class DengluSpider(scrapy.Spider):
    name = 'denglu'
    allowed_domains = ['17k.com']
    start_urls = ['https://user.17k.com/ck/user/mine/readList?page=1&amp;appKey=2406394919']

    def parse(self, res):
        print(res.text)
复制<br>局限性：<br>当前设定方式虽然可以实现携带cookie保持登录，但是无法获取到新cookie，也就是当前cookie一直是固定的<br>如果cookie是经常性变化，那么当前不适用<br><br>scrapy中start_url是通过start_requests来进行处理的，其实现代码如下<br>Pythondef start_requests(self):
    cls = self.__class__
    if method_is_overridden(cls, Spider, 'make_requests_from_url'):
        warnings.warn(
            "Spider.make_requests_from_url method is deprecated; it "
            "won't be called in future Scrapy releases. Please "
            "override Spider.start_requests method instead (see %s.%s)." % (
                cls.__module__, cls.__name__
            ),
        )
        for url in self.start_urls:
            yield self.make_requests_from_url(url)
    else:
        for url in self.start_urls:
            yield Request(url, dont_filter=True)
复制<br>所以对应的，如果start_url地址中的url是需要登录后才能访问的url地址，则需要重写start_request方法并在其中手动添加上cookie<br>settings.py<br>Pythonimport scrapy


class DengluSpider(scrapy.Spider):
    name = 'denglu'
    # allowed_domains = ['https://user.17k.com/ck/user/mine/readList?page=1']
    start_urls = ['https://user.17k.com/ck/user/mine/readList?page=1&amp;appKey=2406394919']

    def start_requests(self):
        cookies = 'GUID=796e4a09-ba11-4ecb-9cf6-aad19169267d; Hm_lvt_9793f42b498361373512340937deb2a0=1660545196; c_channel=0; c_csc=web; accessToken=avatarUrl%3Dhttps%253A%252F%252Fcdn.static.17k.com%252Fuser%252Favatar%252F18%252F98%252F90%252F96139098.jpg-88x88%253Fv%253D1650527904000%26id%3D96139098%26nickname%3D%25E4%25B9%25A6%25E5%258F%258BqYx51ZhI1%26e%3D1677033668%26s%3D8e116a403df502ab; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2296139098%22%2C%22%24device_id%22%3A%22181d13acb2c3bd-011f19b55b75a8-1c525635-1296000-181d13acb2d5fb%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_referrer_host%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%7D%2C%22first_id%22%3A%22796e4a09-ba11-4ecb-9cf6-aad19169267d%22%7D; Hm_lpvt_9793f42b498361373512340937deb2a0=1661483362'
        cookie_dic = {}
        for i in cookies.split(';'):
            v = i.split('=')
            cookie_dic[v[0]] = v[1]
        # {i.split('=')[0]:i.split('=')[1] for i in cookies_str.split('; ')}   # 简写
        for url in self.start_urls:
            yield scrapy.Request(url, cookies=cookie_dic)


    def parse(self, response):
        print(response.text)
复制<br><br>
<br>scrapy中cookie不能够放在headers中，在构造请求的时候有专门的cookies参数，能够接受字典形式的coookie
<br>在setting中设置ROBOTS协议、USER_AGENT
<br><br>
我们知道可以通过scrapy.Request()指定method、body参数来发送post请求；那么也可以使用scrapy.FormRequest()来发送post请求
<br><br>通过scrapy.FormRequest能够发送post请求，同时需要添加fromdata参数作为请求体，以及callback<br>Pythonlogin_url = 'https://passport.17k.com/ck/user/login'      
yield scrapy.FormRequest(
            url=login_url, 
            formdata={'loginName': '17346570232', 'password': 'xlg17346570232'}, 
            callback=self.do_login
)
复制<br><br><br>
<br>找到post的url地址：点击登录按钮进行抓包，然后定位url地址为<a rel="noopener" class="external-link" href="https://www.xiaohua.com/Handler/Login.ashx" target="_blank">https://www.xiaohua.com/Handler/Login.ashx</a>
<br>找到请求体的规律：分析post请求的请求体，其中包含的参数均在前一次的响应中
<br>否登录成功：通过请求个人主页，观察是否包含用户名
<br><br>Pythonimport scrapy


class DengluSpider(scrapy.Spider):
    name = 'denglu'
    # allowed_domains = ['17k.com']
    start_urls = ['https://user.17k.com/ck/user/mine/readList?page=1&amp;appKey=2406394919']

    def start_requests(self):
        '''
        请求登陆的账号和密码
        '''
        login_url = 'https://passport.17k.com/ck/user/login'
        # 使用request进行请求
        # yield scrapy.Request(url=login_url, body='loginName=17346570232&amp;password=xlg17346570232', callback=self.do_login, method='POST')
        # 使用Request子类FormRequest进行请求  自动为post请求
        yield scrapy.FormRequest(
            url=login_url,
            formdata={'loginName': '17346570232', 'password': 'xlg17346570232'},
            callback=self.do_login
        )

    def do_login(self, response):
        '''
        登陆成功后调用parse进行处理
        cookie中间件会帮我们自动处理携带cookie
        '''
        for url in self.start_urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response, **kwargs):
        print(response.text)
复制<br><br>在settings.py中通过设置COOKIES_DEBUG=TRUE 能够在终端看到cookie的传递传递过程<br>注意关闭LOG_LEVEL <br>“Scrapy03-模拟登陆以及分页.assets/scrapy-login-1.png” could not be found.<br><br>
<br>start_urls中的url地址是交给start_request处理的，如有必要，可以重写start_request函数
<br>直接携带cookie登陆：cookie只能传递给cookies参数接收
<br>scrapy.FormRequest()发送post请求
<br><br>笑话网：<a rel="noopener" class="external-link" href="https://www.xiaohua.com/" target="_blank">https://www.xiaohua.com/</a><br><br><br>
<br>应用 完善并使用Item数据类
<br>应用 构造Request对象，并发送请求
<br>应用 利用meta参数在不同的解析函数中传递数据
<br><br><br>对于要提取如下图中所有页面上的数据该怎么办？<br>“./Scrapy03-模拟登陆以及分页.assets/scrapy翻页.png” could not be found.<br>回顾requests模块是如何实现翻页请求的：<br>
<br>找到下一页的URL地址
<br>调用requests.get(url)
<br>scrapy实现翻页的思路：<br>
<br>找到下一页的url地址
<br>构造url地址的请求，传递给引擎
<br><br><br>
<br>确定url地址
<br>构造请求，scrapy.Request(url,callback)

<br>callback：指定解析函数名称，表示该请求返回的响应使用哪一个函数进行解析


<br>把请求交给引擎：yield scrapy.Request(url,callback)
<br><br>
通过爬取段子页面的信息,学习如何实现翻页请求
地址：<a rel="noopener" class="external-link" href="https://duanzixing.com/" target="_blank">https://duanzixing.com/</a>
<br><br>
<br>获取首页的数据
<br>寻找下一页的地址，进行翻页，获取数据
<br><br>
<br>
可以在settings中设置ROBOTS协议
Python# False表示忽略网站的robots.txt协议，默认为True
ROBOTSTXT_OBEY = False
复制

<br>
可以在settings中设置User-Agent：
Python# scrapy发送的每一个请求的默认UA都是设置的这个User-Agent
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'
复制

<br><br>
<br>
段子实现多页抓取
Pythonurl = 'https://duanzixing.com/page/%d/'
    num = 1
    def parse(self, response):
        article_list = response.xpath('/html/body/section/div/div/article')
        for article in article_list:
            title = article.xpath('./header/h2/a/text()').extract_first()
            con = article.xpath('./p[2]/text()').extract_first()
            # print(title)
            item = PageproItem()
            item['title'] = title
            item['con'] = con
            yield item
            
        if self.num &lt; 11:
          	# 构造url
            new_url = format(self.url%self.num)
            self.num += 1
            print(new_url)
            # 构造scrapy.Request对象，并yield给引擎
        		# 利用callback参数指定该Request对象之后获取的响应用哪个函数进行解析
        		# 利用meta参数将本函数中提取的数据传递给callback指定的函数
        		# 注意这里是yield
            yield scrapy.Request(new_url, callback=self.parse)
复制

<br>
豆瓣电影抓取子页面数据(手动调用请求练习)
新建豆瓣项目
scrapy startproject douban
cd douban
scrapy genspider db
scrapy crawl db
Pythonimport scrapy


class DbSpider(scrapy.Spider):
    name = 'db'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://movie.douban.com/chart']

    def parse(self, response):
        table_list = response.xpath('//*[@id="content"]/div/div[1]/div/div/table')
        for table in table_list:
            # 抓取子页面url地址
            href = table.xpath('./tr/td[2]/div/a/@href').extract_first()
            print(href)
            # 手动请求子页面
            yield scrapy.Request(href, callback=self.parse_deatil)

    # 解析子页面数据
    def parse_deatil(self, response):
        # 获取所有的电影信息
        print(response.xpath('//*[@id="info"]//text()').extract())
复制

<br><br>Pythonscrapy.Request(url[,callback,method="GET",headers,body,cookies, 
meta,dont_filter=False])
复制<br><br>
<br>中括号中的参数为可选参数
<br>callback：表示当前的url的响应交给哪个函数去处理
<br>meta：实现数据在不同的解析函数中传递，meta默认带有部分数据，比如下载延迟，请求深度等
<br>dont_filter:默认为False，会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以把它设置为Ture，比如贴吧的翻页请求，页面的数据总是在变化;start_urls中的地址会被反复请求，否则程序不会启动
<br>method：指定POST或GET请求
<br>headers：接收一个字典，其中不包括cookies
<br>cookies：接收一个字典，专门放置cookies
<br>body：接收一个字典，为POST的数据
<br><br><br><br>在爬虫文件的parse方法中，提取详情页增加之前callback指定的parse_detail函数：<br>Pythondef parse(self,response):
    ...
    yield scrapy.Request(detail_url, callback=self.parse_detail,meta={"item":item})
...

def parse_detail(self,response):
    #获取之前传入的item
    item = resposne.meta["item"]
复制<br><br>
<br>meta参数是一个字典
<br>meta字典中有一个固定的键proxy，表示代理ip，关于代理ip的使用我们将在scrapy的下载中间件的学习中进行介绍
<br><br><br>
<br>
定义item即提前规划好哪些字段需要抓取，scrapy.Field()仅仅是提前占坑，通过item.py能够让别人清楚自己的爬虫是在抓取什么，同时定义好哪些字段是需要抓取的，没有定义的字段不能使用，防止手误

<br>
在python大多数框架中，大多数框架都会自定义自己的数据类型(在python自带的数据结构基础上进行封装)，目的是增加功能，增加自定义异常

<br><br>在items.py文件中定义要提取的字段：<br>Pythonimport scrapy

class DoubanItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    name = scrapy.Field()     # 电影名称
    director = scrapy.Field()     # 导演
    screenwriter = scrapy.Field()    # 编剧
    to_star = scrapy.Field()    # 主演
复制<br><br>Item使用之前需要先导入并且实例化，之后的使用方法和使用字典相同<br>修改爬虫文件db.py：<br>Pythonimport scrapy
from douban.items import DoubanItem

class DbSpider(scrapy.Spider):
    name = 'db'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://movie.douban.com/chart']
 
    def parse(self, response):
        table_list = response.xpath('//*[@id="content"]/div/div[1]/div/div/table')
        for table in table_list:
            # 抓取子页面url地址
            href = table.xpath('./tr/td[2]/div/a/@href').extract_first()
            print(href)
            # 手动请求子页面
            yield scrapy.Request(href, callback=self.parse_deatil)

    # 解析子页面数据
    def parse_deatil(self, response):
        # 提取电影详情
        item = DoubanItem()
        item['name'] = response.xpath('//*[@id="content"]/h1/span[1]/text()').extract_first()  # 电影名称
        item['director'] = response.xpath('//*[@id="info"]/span[1]/span[2]/a/text()').extract_first()  # 导演
        item['screenwriter'] = ''.join(response.xpath('//*[@id="info"]/span[2]/span[2]//text()').extract())  # 编剧
        item['to_star'] = ''.join(response.xpath('//*[@id="info"]/span[3]/span[2]//text()').extract())  # 主演
        print(item)
        return item
复制<br><br>
<br>from myspider.items import ITSpiderItem这一行代码中 注意item的正确导入路径，忽略pycharm标记的错误
<br>python中的导入路径要诀：从哪里开始运行，就从哪里开始导入
<br>在parse_deatil中item[key] 必须要和items.py中的key对应，否则抛出异常
<br><br>
<br>完善并使用Item数据类：

<br>在items.py中完善要爬取的字段
<br>在爬虫文件中先导入Item
<br>实力化Item对象后，像字典一样直接使用


<br>构造Request对象，并发送请求：

<br>导入scrapy.Request类
<br>在解析函数中提取url
<br>yield scrapy.Request(url, callback=self.parse_detail, meta={})


<br>利用meta参数在不同的解析函数中传递数据:

<br>通过前一个解析函数 yield scrapy.Request(url, callback=self.xxx, meta={}) 来传递meta
<br>在self.xxx函数中 response.meta.get('key', '') 或 response.meta['key'] 的方式取出传递的数据


]]></description><link>爬虫\爬虫文档\Scrapy03-模拟登陆以及分页.html</link><guid isPermaLink="false">爬虫/爬虫文档/Scrapy03-模拟登陆以及分页.md</guid><pubDate>Mon, 16 Oct 2023 15:12:12 GMT</pubDate></item><item><title><![CDATA[Scrapy中间件]]></title><description><![CDATA[<a class="tag" href="?query=tag:scrapy" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scrapy</a> <a class="tag" href="?query=tag:框架" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#框架</a> 
 <br><br><a href=".?query=tag:scrapy" class="tag" target="_blank" rel="noopener">#scrapy</a> <a href=".?query=tag:框架" class="tag" target="_blank" rel="noopener">#框架</a><br><br>
<br>应用 scrapy中使用中间件使用随机UA的方法
<br>了解 scrapy中使用代理ip的的方法
<br><br><br><br>根据scrapy运行流程中所在位置不同分为：<br>
<br>下载中间件
<br>爬虫中间件
<br><br>
<br>主要功能是在爬虫运行过程中进行一些处理，如对非200响应的重试（重新构造Request对象yield给引擎）
<br>也可以对header以及cookie进行更换和处理
<br>其他根据业务需求实现响应的功能
<br>但在scrapy默认的情况下 两种中间件都在middlewares.py一个文件中<br>爬虫中间件使用方法和下载中间件相同，常用下载中间件<br><br>
接下来我们对爬虫进行修改完善，通过下载中间件来学习如何使用中间件 编写一个Downloader Middlewares和我们编写一个pipeline一样，定义一个类，然后在setting中开启
<br>Downloader Middlewares默认的方法：在中间件类中，有时需要重写处理请求或者响应的方法】<br>
<br>
process_request(self, request, spider)：【此方法是用的最多的】

<br>当每个request通过下载中间件时，该方法被调用。
<br>返回None值：继续请求  没有return也是返回None，该request对象传递给下载器，或通过引擎传递给其他权重低的process_request方法 【如果所有的下载器中间件都返回为None，则请求最终被交给下载器处理】
<br>返回Response对象：不再请求，把response返回给引擎【如果返回为请求，则将请求交给调度器】
<br>返回Request对象：把request对象交给调度器进行后续的请求  


<br>
process_response(self, request, response, spider)：

<br>当下载器完成http请求，传递响应给引擎的时候调用
<br>返回Resposne：通过引擎交给爬虫处理或交给权重更低的其他下载中间件的process_response方法 【如果返回为请求，则将请求交给调度器】
<br>返回Request对象：交给调取器继续请求，此时将不通过其他权重低的process_request方法  【将响应对象交给spider进行解析】


<br>
process_exception(self, request, exception, spider):

<br>
请求出现异常的时候进行调用

<br>
比如当前请求被识别为爬虫 可以使用代理
Pythondef process_exception(self, request, exception, spider):
	request.meta['proxy'] = 'http://ip地址'
  	request.dont_filter = True  # 因为默认请求是去除重复的，因为当前已经请求过，所以需要设置当前为不去重
	return request  # 将修正后的对象重新进行请求
复制



<br>
在settings.py中配置开启中间件，权重值越小越优先执行  【同管道的注册使用】

<br>
spider参数：为爬虫中类的实例化可以在这里进行调用爬虫中的属性
如：spider.name

<br><br>spiders.py（爬虫代码）news.163.com/domestic/<br>Pythonclass WySpider(scrapy.Spider):
    name = 'wy'
    start_urls = ['http://www.baidu.com/']  # 给一个正确的网址
    # start_urls = ['http://www.baidu123.com/']  # 给一个错误的网址
    def parse(self, response, **kwargs):
        pass
复制<br>settings.py(开启中间件)<br>PythonDOWNLOADER_MIDDLEWARES = {
   'wangyi.middlewares.WangyiDownloaderMiddleware': 543,
}
复制<br>middlewares.py<br>Pythonclass WangyiDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        print('process_request')
        return None

    def process_response(self, request, response, spider):
        print('process_response')
        return response

    def process_exception(self, request, exception, spider):
        print('process_exception')
        return request
复制<br>运行查看<br>“Scrapy04-中间件.assets/image-20220926140805300.png” could not be found.<br>修改spiders.py  给一个错误的网址在进行查看，会发现当前会循环执行<br>“Scrapy04-中间件.assets/image-20220926141048451.png” could not be found.<br><br><br>PythonUSER_AGENTS_LIST = [ 
"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)", 
"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)",
"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)",
"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)",
"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0", 
"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5" 
]
复制<br><br>Pythonimport random
from Tencent.settings import USER_AGENTS_LIST # 注意导入路径,请忽视pycharm的错误提示

class UserAgentMiddleware(object):
    def process_request(self, request, spider):
        user_agent = random.choice(USER_AGENTS_LIST)
        request.headers['User-Agent'] = user_agent
复制<br><br>Pythonclass CheckUA:
    def process_response(self,request,response,spider):
        print(request.headers['User-Agent'])
复制<br><br>PythonDOWNLOADER_MIDDLEWARES = {
   'Tencent.middlewares.UserAgentMiddleware': 543,
}
复制<br><br><br><br>
<br>
代理添加的位置：request.meta中增加proxy字段

<br>
获取一个代理ip，赋值给
Pythonrequest.meta['proxy']
复制

<br>代理池中随机选择代理ip
<br>代理ip的webapi发送请求获取一个代理ip


<br><br>Pythonclass ProxyMiddleware(object):
    def process_request(self,request,spider):
        proxy = random.choice(proxies) # proxies可以在settings.py中，也可以来源于代理ip的webapi
        # proxy = 'http://192.168.1.1:8118'
        request.meta['proxy'] = proxy
        return None # 可以不写return
复制<br><br>在使用了代理ip的情况下可以在下载中间件的process_response()方法中处理代理ip的使用情况，如果该代理ip不能使用可以替换其他代理ip<br>Pythonclass ProxyMiddleware(object):
    def process_response(self, request, response, spider):
        if response.status != '200' and response.status != '302':
            #此时对代理ip进行操作，比如删除
            return request
复制<br><br>网址：<a rel="noopener" class="external-link" href="https://www.kuaidaili.com/" target="_blank">https://www.kuaidaili.com/</a><br>
<br>
输入网址   点击购买代理
<img style="zoom:50%;" alt="image-20221222111151687" src="\Scrapy04-中间件.assets\image-20221222111151687.png" referrerpolicy="no-referrer">

<br>
选择你想购买代理的类型
“Scrapy04-中间件.assets/image-20221222111226189.png” could not be found.

<br>
以隧道代理为例  点击购买
“Scrapy04-中间件.assets/image-20221222111311561.png” could not be found.

<br>
购买后点击 文档中心
<img style="zoom:50%;" alt="image-20221222111528183" src="\Scrapy04-中间件.assets\image-20221222111528183.png" referrerpolicy="no-referrer">

<br>
点击
<img style="zoom:50%;" alt="image-20221222111550153" src="\Scrapy04-中间件.assets\image-20221222111550153.png" referrerpolicy="no-referrer">

<br>
选择隧道代理
<img style="zoom:50%;" alt="image-20221222111626751" src="\Scrapy04-中间件.assets\image-20221222111626751.png" referrerpolicy="no-referrer">

<br>
向下拉 选择你当前要使用代理的模块
我们是scrapy使用隧道 所以选择   Python-Scrapy
“Scrapy04-中间件.assets/image-20221222111731483.png” could not be found.

<br>
找到middlewares.py
<img style="zoom:50%;" alt="image-20221222111824669" src="\Scrapy04-中间件.assets\image-20221222111824669.png" referrerpolicy="no-referrer">

<br>
将中间件类代码复制到当前自己scrapy的中间件得文件中即可
“Scrapy04-中间件.assets/image-20221222112013039.png” could not be found.

<br>
按照步骤开启中间件与填写自己的用户名与密码等信息即可

<br><br>中间件的使用：<br>
<br>完善中间件代码：

<br>process_request(self, request, spider)：

<br>当每个request通过下载中间件时，该方法被调用。
<br>返回None值：继续请求
<br>返回Response对象：不再请求，把response返回给引擎
<br>返回Request对象：把request对象交给调度器进行后续的请求


<br>process_response(self, request, response, spider)：

<br>当下载器完成http请求，传递响应给引擎的时候调用
<br>返回Resposne：交给process_response来处理
<br>返回Request对象：交给调取器继续请求




<br>需要在settings.py中开启中间件 DOWNLOADER_MIDDLEWARES = { 'myspider.middlewares.UserAgentMiddleware': 543, }
<br><br>url：<a rel="noopener" class="external-link" href="https://news.163.com/" target="_blank">https://news.163.com/</a><br><br>
<br>scrapy startproject wangyi
<br>cd wangyi
<br>scrapy genspider wy  <a rel="noopener" class="external-link" href="https://news.163.com/" target="_blank">https://news.163.com/</a>
<br><br>抓取 国内 国际 军事 航空<br>
<br>
分析 
国内等数据是由动态加载的 并不是跟着当前的请求一起返回的 
“Scrapy04-中间件.assets/image-20220509095648595.png” could not be found.
解决方式2种

<br>
通过selenium配合爬虫抓取页面进行数据

<br>
找到加载动态数据的url地址  通过爬虫进行抓取
“Scrapy04-中间件.assets/image-20220509100159399.png” could not be found.
“Scrapy04-中间件.assets/image-20220509100235422.png” could not be found.


“Scrapy04-中间件.assets/image-20220509100359070.png” could not be found.
将找到的URL放到浏览器中进行请求  效果如下
“Scrapy04-中间件.assets/image-20220509100447858.png” could not be found.

<br><br>
<br>
配置文件处理settings.py
Python# Scrapy settings for wangyi project
BOT_NAME = 'wangyi'

SPIDER_MODULES = ['wangyi.spiders']
NEWSPIDER_MODULE = 'wangyi.spiders'

# 默认请求头
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'

# 用于更换随机请求头
USER_AGENTS_LIST = [
"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)",
"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)",
"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)",
"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0",
"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5" ]

LOG_LEVEL = 'ERROR'
ROBOTSTXT_OBEY = False

DOWNLOADER_MIDDLEWARES = {
   'wangyi.middlewares.WangyiDownloaderMiddleware': 543,
}
ITEM_PIPELINES = {
   'wangyi.pipelines.WangyiPipeline': 300,
}
复制

<br>
爬虫代码wy.py
Pythonimport scrapy
from selenium import webdriver
from selenium.webdriver import ChromeOptions
from selenium.webdriver.chrome.options import Options
from wangyi.items import WangyiItem
class WySpider(scrapy.Spider):
      name = 'wy'
      # allowed_domains = ['news.163.com']
      start_urls = ['https://news.163.com/domestic/']
      li_index = [1, 2]  # 当前要爬取菜单的索引位置
      page_url = []
      # 隐藏浏览器界面
      chrome_option = Options()
      chrome_option.add_argument('--headless')
      chrome_option.add_argument('--disable-gpu')
      # 防止检测
      option = ChromeOptions()
      option.add_experimental_option('excludeSwitches', ['enable-automation'])
      # 导入配置
      driver = webdriver.Chrome(chrome_options=chrome_option, options=option)
      def parse(self, response, **kwargs):
          # 抓取 国内 国际
          menu = response.xpath('/html/body/div/div[3]/div[2]/div[2]/div/ul/li/a/@href').extract()
          # 循环获取当前我们要抓取栏目的url
          for i in range(len(menu)):
              if i in self.li_index:
                  url = menu[i]
                  self.page_url.append(url)
                  # 向详情页发起请求
                  yield scrapy.Request(url, callback=self.parse_detail)
      # 对栏目页进行请求
      def parse_detail(self, response):
          # 获取每个新闻的url
          detail_href_list = response.xpath('/html/body/div/div[3]/div[3]/div[1]/div[1]/div/ul/li/div/div/a/@href').extract()
          # print(detail_href_list)
          for url in detail_href_list:
              print(url)
              yield scrapy.Request(url, callback=self.parse_detail_con)
  # 对于新闻详情页进行解析
  def parse_detail_con(self, response):
      # 实例化item
      item = WangyiItem()
      # 匹配标题
      title = response.xpath('//*[@id="container"]/div[1]/h1/text()').extract_first()
      # 匹配内容
      con = ''.join(response.xpath('//*[@id="content"]/div[2]//text()').extract())
      item['title'] = title
      item['con'] = con
      print(item)
      yield item

+ Items.py

​```python
import scrapy
class WangyiItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title = scrapy.Field()
    content = scrapy.Field()
复制

<br>
Middlewares.py
Pythonfrom wangyi.settings import USER_AGENTS_LIST
import random
from scrapy.http import HtmlResponse

class WangyiDownloaderMiddleware:

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        request.headers['User-Agent'] = random.choice(USER_AGENTS_LIST)
        # print(request.headers)
        return None

    def process_response(self, request, response, spider):
        # Called with the response returned from the downloader.
        driver = spider.driver  # 获取到selenium
        if request.url in spider.page_url:
            driver.get(request.url)
            # 滚动条滚动到最下方
            driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')
            time.sleep(1)
            # 拖动两次
            driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')
            time.sleep(1)
            text = driver.page_source
            # 篡改响应对象并返回
            return HtmlResponse(url=request.url, body=text, encoding='UTF-8', request=request)
        return response

    def process_exception(self, request, exception, spider):
        # Called when a download handler or a process_request()
        pass
    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)
复制

]]></description><link>爬虫\爬虫文档\Scrapy04-中间件.html</link><guid isPermaLink="false">爬虫/爬虫文档/Scrapy04-中间件.md</guid><pubDate>Mon, 16 Oct 2023 15:12:15 GMT</pubDate><enclosure url="Scrapy04-中间件.assets\image-20221222111151687.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;Scrapy04-中间件.assets\image-20221222111151687.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[scrapy的crawlspider爬虫]]></title><description><![CDATA[<a class="tag" href="?query=tag:scrapy" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scrapy</a> <a class="tag" href="?query=tag:框架" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#框架</a> 
 <br><br><a href=".?query=tag:scrapy" class="tag" target="_blank" rel="noopener">#scrapy</a> <a href=".?query=tag:框架" class="tag" target="_blank" rel="noopener">#框架</a> <br><br>
<br>了解 crawlspider的作用
<br>应用 crawlspider爬虫创建的方法
<br>应用 crawlspider中rules的使用 
<br><br><br>
回顾之前的代码中，我们有很大一部分时间在寻找下一页的url地址或者是内容的url地址上面，这个过程能更简单一些么？
<br><br>
<br>从response中提取所有的满足规则的url地址
<br>自动的构造自己requests请求，发送给引擎
<br>对应的crawlspider就可以实现上述需求，能够匹配满足条件的url地址，组装成Reuqest对象后自动发送给引擎，同时能够指定callback函数<br>即：crawlspider爬虫可以按照规则自动获取连接<br><br>
通过crawlspider爬取豆瓣TOP250详情页的信息
url：<a rel="noopener" class="external-link" href="https://movie.douban.com/top250" target="_blank">https://movie.douban.com/top250</a>
<br><br>
<br>定义一个规则，来进行列表页翻页，follow需要设置为True
<br>定义一个规则，实现从列表页进入详情页，并且指定回调函数
<br>在详情页提取数据
<br><br><br><br>scrapy startproject project<br>cd project<br>scrapy genspider -t crawl douban book.douban.com/latest?subcat=%E5%85%A8%E9%83%A8&amp;p=1<br>url: 豆瓣图书 <a rel="noopener" class="external-link" href="https://book.douban.com/latest?subcat=%E5%85%A8%E9%83%A8&amp;p=1" target="_blank">https://book.douban.com/latest?subcat=%E5%85%A8%E9%83%A8&amp;p=1</a><br><br>Pythonimport scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class BookSpider(CrawlSpider):
    name = 'douban'
    # allowed_domains = ['https://movie.douban.com/top250']

    start_urls = ['https://movie.douban.com/top250']
    # 匹配页码地址
    link = LinkExtractor(allow=r'start=\d+&amp;filter=')
    # 匹配详情页地址
    link_detail = LinkExtractor(allow=r'https://movie.douban.com/subject/\d+/')
    # allow值什么都不写 则为提取所有的url
    link_all = LinkExtractor(allow=r'')
    rules = (
        # Rule(link, callback='parse_item', follow=True),
        # Rule(link_detail, callback='parse_detail_item', follow=False),
        Rule(link_all, callback='parse_all_item', follow=True),
    )

    def parse_item(self, response):
        item = {}
        print(response)
        return item

    def parse_detail_item(self, response):
        item = {}
        print(response)
        return item

    def parse_all_item(self, response):
        item = {}
        print(response)
        return item
复制<br><br>在crawlspider爬虫中，没有parse函数<br><br>
<br>rules是一个元组或者是列表，包含的是Rule对象
<br>Rule表示规则，其中包含LinkExtractor,callback和follow等参数
<br>LinkExtractor:连接提取器，可以通过正则或者是xpath来进行url地址的匹配
<br>callback :表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调函数的处理
<br>follow：连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，True表示会，Flase表示不会
<br><br>
<br>除了用命令scrapy genspider -t crawl &lt;爬虫名&gt; &lt;allowed_domail&gt;创建一个crawlspider的模板，页可以手动创建
<br>crawlspider中不能再有以parse为名的数据提取方法，该方法被crawlspider用来实现基础url提取等功能
<br>Rule对象中LinkExtractor为固定参数，其他callback、follow为可选参数
<br>不指定callback且follow为True的情况下，满足rules中规则的url还会被继续提取和请求
<br>如果一个被提取的url满足多个Rule，那么会从rules中选择一个满足匹配条件的Rule执行
<br><br>
<br>
链接提取器LinkExtractor的更多常见参数

<br>
allow: 满足括号中的're'表达式的url会被提取，如果为空，则全部匹配

<br>
deny: 满足括号中的're'表达式的url不会被提取，优先级高于allow

<br>
allow_domains: 会被提取的链接的domains(url范围)，如：['https://movie.douban.com/top250']

<br>
deny_domains: 不会被提取的链接的domains(url范围)

<br>
restrict_xpaths: 使用xpath规则进行匹配，和allow共同过滤url，即xpath满足的范围内的url地址会被提取
如：restrict_xpaths='//div[@class="pagenav"]'

<br>
restrict_css: 接收一堆css选择器, 可以提取符合要求的css选择器的链接

<br>
attrs: 接收一堆属性名, 从某个属性中提取链接, 默认href

<br>
tags: 接收一堆标签名, 从某个标签中提取链接, 默认a, area


值得注意的, 在提取到的url中, 是有重复的内容的. 但是我们不用管. scrapy会自动帮我们过滤掉重复的url请求

<br>
模拟使用

<br>　　正则用法：　links1 = LinkExtractor(allow=r'list23\d+.html')<br>　　xpath用法： links2 = LinkExtractor(restrict_xpaths=r'//div[@class="x"]')<br>　	css用法：  links3 = LinkExtractor(restrict_css='.x')<br>5.提取连接<br>
<br>
Rule常见参数

<br>LinkExtractor: 链接提取器，可以通过正则或者是xpath来进行url地址的匹配
<br>callback: 表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调函数的处理
<br>follow: 连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，默认True表示会，Flase表示不会
<br>process_links: 当链接提取器LinkExtractor获取到链接列表的时候调用该参数指定的方法，这个自定义方法可以用来过滤url，且这个方法执行后才会执行callback指定的方法


<br><br>
<br>crawlspider的作用：crawlspider可以按照规则自动获取连接
<br>crawlspider爬虫的创建：scrapy genspider -t crawl xxx  <a data-tooltip-position="top" aria-label="http://www.xxx.com" rel="noopener" class="external-link" href="http://www.xxx.com" target="_blank">www.xxx.com</a>
<br>crawlspider中rules的使用：

<br>rules是一个元组或者是列表，包含的是Rule对象
<br>Rule表示规则，其中包含LinkExtractor,callback和follow等参数
<br>LinkExtractor:连接提取器，可以通过正则或者是xpath来进行url地址的匹配
<br>callback :表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调函数的处理
<br>follow：连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，True表示会，Flase表示不会


<br>]]></description><link>爬虫\爬虫文档\Scrapy05-分页抓取_20230315_220513.html</link><guid isPermaLink="false">爬虫/爬虫文档/Scrapy05-分页抓取_20230315_220513.md</guid><pubDate>Mon, 16 Oct 2023 15:12:20 GMT</pubDate></item><item><title><![CDATA[scrapy_redis]]></title><description><![CDATA[<a class="tag" href="?query=tag:scrapy" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scrapy</a> <a class="tag" href="?query=tag:框架" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#框架</a> 
 <br><br><a href=".?query=tag:scrapy" class="tag" target="_blank" rel="noopener">#scrapy</a> <a href=".?query=tag:框架" class="tag" target="_blank" rel="noopener">#框架</a><br><br><br>
<br>了解 scarpy_redis的概念和功能
<br>了解 scrapy_redis的原理
<br>了解 redis数据库操作命令
<br><br>
在前面scrapy框架中我们已经能够使用框架实现爬虫爬取网站数据,如果当前网站的数据比较庞大, 我们就需要使用分布式来更快的爬取数据
<br><br><br>pip install scrapy_redis == 0.7.2<br><br>Scrapy_redis在scrapy的基础上实现了更多，更强大的功能，具体体现在：<br>
<br>请求对象的持久化
<br>去重的持久化
<br>和实现分布式
<br><br><br>“scrapy_redis.assets/scrapy的流程.png” could not be found.<br>那么，在这个基础上，如果需要实现分布式，即多台服务器同时完成一个爬虫，需要怎么做呢？<br><br>
<br>在scrapy_redis中，所有的带抓取的对象和去重的指纹都存在所有的服务器公用的redis中
<br>所有的服务器公用一个redis中的request对象
<br>所有的request对象存入redis前，都会在同一个redis中进行判断，之前是否已经存入过
<br>在默认情况下所有的数据会保存在redis中
<br>具体流程如下：<br>“scrapy_redis.assets/scrapy_redis的流程.png” could not be found.<br><br>
由于时间关系,大家对redis的命令遗忘的差不多了, 但是在scrapy_redis中需要使用redis的操作命令,所有需要回顾下redis的命令操作
<br><br>redis是一个开源的内存型数据库，支持多种数据类型和结构，比如列表、集合、有序集合等,同时可以使用redis-manger-desktop等客户端软件查看redis中的数据，关于redis-manger-desktop的使用可以参考扩展阅读<br><br>
<br>redis-server.exe redis-windwos.conf 启动服务端
<br>redis-cli 客户端启动
<br><br>
<br>select 1 切换db
<br>keys * 查看所有的键
<br>type 键 查看键的类型
<br>flushdb 清空db
<br>flushall 清空数据库
<br><br>redis的命令很多，这里我们简单复习后续会使用的命令<br>“scrapy_redis.assets/redis命令的复习.png” could not be found.<br><br>scarpy_redis的分布式工作原理<br>
<br>在scrapy_redis中，所有的带抓取的对象和去重的指纹都存在所有的服务器公用的redis中
<br>所有的服务器公用一个redis中的request对象
<br>所有的request对象存入redis前，都会在同一个redis中进行判断，之前是否已经存入过
<br><br><br>配置完成使用分布式爬虫<br><br>分布式爬虫<br>
<br>使用多台机器搭建一个分布式的机器，然后让他们联合且分布的对同一组资源进行数据爬取
<br>原生的scrapy框架是无法实现分布式爬虫？

<br>原因：调度器，管道无法被分布式机群共享


<br>如何实现分布式

<br>借助：scrapy-redis组件
<br>作用：提供了可以被共享的管道和调度器
<br>只可以将爬取到的数据存储到redis中


<br><br>
<br>scrapy  startproject fbsPro
<br>cd fbsPro
<br>scrapy genspider -t crawl fbs <a data-tooltip-position="top" aria-label="http://www.xxx.com" rel="noopener" class="external-link" href="http://www.xxx.com" target="_blank">www.xxx.com</a>
<br><br>
<br>
(必须). 使用了scrapy_redis的去重组件，在redis数据库里做去重
PythonDUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
复制

<br>
(必须). 使用了scrapy_redis的调度器，在redis里分配请求
PythonSCHEDULER = "scrapy_redis.scheduler.Scheduler"
复制

<br>
(可选). 在redis中保持scrapy-redis用到的各个队列，从而允许暂停和暂停后恢复，也就是不清理redis queues
SCHEDULER_PERSIST = True
复制

<br>
(必须). 通过配置RedisPipeline将item写入key为 spider.name : items 的redis的list中，供后面的分布式处理item 这个已经由 scrapy-redis 实现，不需要我们写代码，直接使用即可
ITEM_PIPELINES = {
　　 'scrapy_redis.pipelines.RedisPipeline': 100 ,
}
复制

<br>
(必须). 指定redis数据库的连接参数
REDIS_HOST = '127.0.0.1' 
REDIS_PORT = 6379
REDIS_DB = 0  # (不指定默认为0)
#  设置密码
REDIS_PARAMS = {'password': '123456'}
复制

<br><br>settings.py<br>这几行表示scrapy_redis中重新实现的了去重的类，以及调度器，并且使用的RedisPipeline<br>需要添加redis的地址，程序才能够使用redis<br>在settings.py文件修改pipelines，增加scrapy_redis。<br>Python# 配置分布式
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
SCHEDULER_PERSIST = True

ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 400,
}


# 或者使用下面的方式
REDIS_HOST = "127.0.0.1"
REDIS_PORT = 6379
REDIS_PARAMS = {'password': '123456'}
复制<br>注意：scrapy_redis的优先级要调高<br><br>Pythonfrom scrapy_redis.spiders import RedisCrawlSpider

# 注意  一定要继承RedisCrawlSpider
class FbsSpider(RedisCrawlSpider):
    name = 'fbs'
    # allowed_domains = ['www.xxx.com']
    # start_urls = ['http://www.xxx.com/']
    redis_key = 'fbsQueue'  # 使用管道名称（课根据实际功能起名称）
复制<br><br>scrapy-redis中都是用key-value形式存储数据，其中有几个常见的key-value形式：<br>1、 “项目名:items”  --&gt;list 类型，保存爬虫获取到的数据item 内容是 json 字符串<br>2、 “项目名:dupefilter”   --&gt;set类型，用于爬虫访问的URL去重 内容是 40个字符的 url 的hash字符串<br>3、 “项目名:requests”   --&gt;zset类型，用于scheduler调度处理 requests 内容是 request 对象的序列化 字符串<br><br>
<br>
网址
阳光问政为例
<a rel="noopener" class="external-link" href="https://wz.sun0769.com/political/index/politicsNewest" target="_blank">https://wz.sun0769.com/political/index/politicsNewest</a>

<br>
settings.py
PythonBOT_NAME = 'fbsPro'

SPIDER_MODULES = ['fbsPro.spiders']
NEWSPIDER_MODULE = 'fbsPro.spiders'

USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# 配置分布式
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
SCHEDULER_PERSIST = True

ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 400,
}

# 或者使用下面的方式
REDIS_HOST = "127.0.0.1"
REDIS_PORT = 6379
REDIS_PARAMS = {'password': '123456'}
复制

<br>
fbs.py
实现方式就是之前的crawlspider类型的爬虫
Pythonimport scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from scrapy_redis.spiders import RedisCrawlSpider

# 注意  一定要继承RedisCrawlSpider
class FbsSpider(RedisCrawlSpider):
    name = 'fbs'
    # allowed_domains = ['www.xxx.com']
    # start_urls = ['http://www.xxx.com/']
    redis_key = 'fbsQueue'  # 使用管道名称
    link = LinkExtractor(allow=r'/political/politics/index?id=\d+')
    rules = (
        Rule(link, callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        item = {}
        yield item
复制

<br>
redis中

<br>
redis-windwos.conf  

<br>56行添加注释 取消绑定127.0.0.1  # bind 127.0.0.1
<br>75行  修改保护模式为no  protected-mode no


<br>
启动redis

<br>
队列中添加url地址
添加：lpush fbsQueue <a rel="noopener" class="external-link" href="https://wz.sun0769.com/political/index/politicsNewest" target="_blank">https://wz.sun0769.com/political/index/politicsNewest</a>
查看：lrange fbsQueue 0 -1 



<br>
运行
scrapy crawl fbs

<br>
去redis中查看存储的数据

]]></description><link>爬虫\爬虫文档\Scrapy06-scrapy_redis.html</link><guid isPermaLink="false">爬虫/爬虫文档/Scrapy06-scrapy_redis.md</guid><pubDate>Mon, 16 Oct 2023 15:12:22 GMT</pubDate></item><item><title><![CDATA[selenium]]></title><description><![CDATA[<a class="tag" href="?query=tag:框架" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#框架</a> 
 <br><br><a href=".?query=tag:框架" class="tag" target="_blank" rel="noopener">#框架</a><br><br><br>selenium本身是一个自动化测试工具。它可以让python代码调用浏览器。并获取到浏览器中加载的各种资源。 我们可以利用selenium提供的各项功能。 帮助我们完成数据的抓取。<br><br>
<br>掌握 selenium发送请求，加载网页的方法
<br>掌握 selenium简单的元素定位的方法
<br>掌握 selenium的基础属性和方法
<br>掌握 selenium退出的方法
<br><br>安装：pip install selenium<br>它与其他库不同的地方是他要启动你电脑上的浏览器, 这就需要一个驱动程序来辅助. <br>这里推荐用chrome浏览器<br>chrome驱动地址:<a rel="noopener" class="external-link" href="http://chromedriver.storage.googleapis.com/index.html" target="_blank">http://chromedriver.storage.googleapis.com/index.html</a><br>“selenium.assets/image-20210125174618013.png” could not be found.<br>“selenium.assets/image-20210125174658971.png” could not be found.<br>根据你电脑的不同自行选择吧.  win64选win32即可.<br>然后关键的来了. 把你下载的浏览器驱动放在python解释器所在的文件夹<br>Windwos:  py -0p     查看Python路径<br>Mac: open + 路径<br>例如：open /usr/local/bin/<br>“selenium.assets/image-20210125175328245.png” could not be found.<br> 前期准备工作完毕.  上代码看看 感受一下selenium<br>Pythonfrom selenium.webdriver import Chrome  # 导入谷歌浏览器的类


# 创建浏览器对象
web = Chrome()  # 如果你的浏览器驱动放在了解释器文件夹

web.get("http://www.baidu.com")  # 输入网址
print(web.title)  # 打印title
复制<br>运行一下你会发现神奇的事情发生了. 浏览器自动打开了. 并且输入了网址. 也能拿到网页上的title标题. <br>“selenium.assets/image-20210125175906255.png” could not be found.<br><br><br>selenium通过控制浏览器，所以对应的获取的数据都是elements中的内容<br>Pythonfrom selenium import webdriver
from selenium.webdriver.common.by import By

driver = webdriver.Chrome()
# 访问百度
driver.get("http://www.baidu.com/")
# 截图
driver.save_screenshot("baidu.png")
复制<br><br>Python# 搜索关键字 杜卡迪
driver.find_element(By.ID, "kw").send_keys("杜卡迪")
# 点击id为su的搜索按钮
driver.find_element(By.ID, "su").click()
复制<br><br>Pythondriver.page_source   # 获取页面内容
driver.get_cookies()
driver.current_url
复制<br><br>Pythondriver.close()  # 退出当前页面
driver.quit()   # 退出浏览器
复制<br><br>
<br>selenium的导包:from selenium import webdriver
<br>selenium创建driver对象:driver = webdriver.Chrome()
<br>selenium请求数据:driver.get("http://www.baidu.com/")
<br>selenium查看数据: driver.page_source
<br>关闭浏览器: driver.quit()
<br>根据id定位元素: driver.find_element_by_id("kw")/driver.find_element(By.ID, "kw")
<br>操作点击事件: click()
<br>给输入框赋值:send_keys()
<br><br><br>
<br>掌握 selenium定位元素的方法
<br>掌握 selenium从元素中获取文本和属性的方法
<br>
通过selenium的基本使用可以简单定位元素和获取对应的数据,接下来我们再来学习下 定位元素的其他方法
<br><br>
<br>
元素定位的两种写法：

<br>
直接调用型
Python el = driver.find_element_by_xxx(value)
 # xxx是定位方式，后面我们会讲，value为该方式对应的值
复制

<br>
使用By类型(需要导入By)  建议使用这种方式
Python # 直接掉用的方式会在底层翻译成这种方式
from selenium.webdriver.common.by import By
driver.find_element(By.xxx,value)
复制



<br>
元素定位的两种方式:

<br>
精确定位一个元素,返回结果为一个element对象,定位不到则报错
Pythondriver.find_element(By.xx, value)  # 建议使用
driver.find_element_by_xxx(value)
复制

<br>
定位一组元素,返回结果为element对象列表,定位不到返回空列表
Pythondriver.find_elements(By.xx, value)  # 建议使用
driver.find_elements_by_xxx(value)
复制



<br>
元素定位的八种方法:
以下方法在element之后添加s就变成能够获取一组元素的方法

<br>
By.ID  使用id值定位
Pythonel = driver.find_element(By.ID, '')
el = driver.find_element_by_id()            
复制

<br>
By.XPATH 使用xpath定位
Pythonel = driver.find_element(By.XPATH, '')
el = driver.find_element_by_xpath()         
复制

<br>
By.TAG_NAME. 使用标签名定位
Pythonel = driver.find_element(By.TAG_NAME, '')
el = driver.find_element_by_tag_name()     
复制

<br>
By.LINK_TEXT使用超链接文本定位
Pythonel = driver.find_element(By.LINK_TEXT, '')
el = driver.find_element_by_link_text() 
复制

<br>
By.PARTIAL_LINK_TEXT  使用部分超链接文本定位
Pythonel = driver.find_element(By.PARTIAL_LINK_TEXT  , '')
el = driver.find_element_by_partial_link_text()
复制

<br>
By.NAME   使用name属性值定位
Pythonel = driver.find_element(By.NAME, '')
el = driver.find_element_by_name()
复制

<br>
By.CLASS_NAME     使用class属性值定位
Pythonel = driver.find_element(By.CLASS_NAME, '')   
el = driver.find_element_by_class_name()
复制

<br>
By.CSS_SELECTOR   使用css选择器定位
Pythonel = driver.find_element(By.CSS_SELECTOR, '')  
el = driver.find_element_by_css_selector()
复制



<br>注意：<br>
<br>
建议使用find_element/find_elements

<br>
find_element和find_elements的区别 

<br>
by_link_text和by_partial_link_text的区别：
全部文本和包含某个文本

<br>
<br>
使用： 以豆瓣为例
Pythonimport time
from selenium import webdriver
from selenium.webdriver.common.by import By

driver = webdriver.Chrome()
driver.implicitly_wait(10)  # 等待节点加载完成
driver.get("https://www.douban.com/search?q=%E6%9D%B0%E6%A3%AE%E6%96%AF%E5%9D%A6%E6%A3%AE")
time.sleep(2)
# 使用id的方式获取右上角的搜索框
# ret1 = driver.find_element(By.ID, 'inp-query')
# ret1 = driver.find_element(By.ID, 'inp-query').send_keys("杰森斯坦森")
# ret1 = driver.find_element_by_id("inp-query")
# print(ret1)

# 输出为：&lt;selenium.webdriver.remote.webelement.WebElement (session="ea6f94544ac3a56585b2638d352e97f3", element="0.5335773935305805-1")&gt;

# 搜索输入框  使用find_elements进行获取
# ret2 = driver.find_elements(By.ID, "inp-query")
# ret2 = driver.find_elements_by_id("inp-query")
# print(ret2)
#输出为：[&lt;selenium.webdriver.remote.webelement.WebElement (session="ea6f94544ac3a56585b2638d352e97f3", element="0.5335773935305805-1")&gt;]

# 搜索按钮  使用xpath进行获取
# ret3 = driver.find_elements(By.XPATH, '//*[@id="inp-query"]')
# ret3 = driver.find_elements_by_xpath("//*[@id="inp-query"]")
# print(len(ret3))
# print(ret3)

# 匹配图片标签
ret4 = driver.find_elements(By.TAG_NAME, 'img')
for url in ret4:
    print(url.get_attribute('src'))

 #ret4 = driver.find_elements_by_tag_name("img")
print(len(ret4))

ret5 = driver.find_elements(By.LINK_TEXT, "浏览发现")
# ret5 = driver.find_elements_by_link_text("浏览发现")
print(len(ret5))
print(ret5)

ret6 = driver.find_elements(By.PARTIAL_LINK_TEXT, "浏览发现")
# ret6 = driver.find_elements_by_partial_link_text("浏览发现")
print(len(ret6))
# 使用class名称查找
ret7 = driver.find_elements(By.CLASS_NAME, 'nbg')
print(ret7)
driver.close()
复制

<br>注意：<br>find_element与find_elements区别<br>
<br>只查找一个元素的时候:可以使用find_element(),find_elements()
find_element()会返回一个WebElement节点对象,但是没找到会报错,而find_elements()不会,之后返回一个空列表
<br>查找多个元素的时候:只能用find_elements(),返回一个列表,列表里的元素全是WebElement节点对象
<br>找到都是节点(标签)
<br>如果想要获取相关内容(只对find_element()有效,列表对象没有这个属性)  使用  .text
<br>如果想要获取相关属性的值(如href对应的链接等,只对find_element()有效,列表对象没有这个属性):使用   .get_attribute("href")                                        
<br><br>
find_element_by_xxx方法仅仅能够获取元素对象，接下来就可以对元素执行以下操作 从定位到的元素中提取数据的方法
<br>
<br>从定位到的元素中获取数据
<br>Pythonel.get_attribute(key)           # 获取key属性名对应的属性值
el.text                        	# 获取开闭标签之间的文本内容
复制<br>
<br>对定位到的元素的操作
<br>Pythonel.click()                      # 对元素执行点击操作

el.submit()                     # 对元素执行提交操作

el.clear()                      # 清空可输入元素中的数据

el.send_keys(data)              # 向可输入元素输入数据
复制<br>使用示例：<br>Pythonfrom selenium import webdriver
from selenium.webdriver.common.by import By

driver =webdriver.Chrome()

driver.get("https://www.douban.com/")
# 打印页面内容 （获取到以后可以进行后续的xpath,bs4 或者存储等）
print(driver.page_source)

ret4 = driver.find_elements(By.TAG_NAME, "h1")
print(ret4[0].text)
#输出：豆瓣

ret5 = driver.find_elements(By.LINK_TEXT, "下载豆瓣 App")
print(ret5[0].get_attribute("href"))
#输出：https://www.douban.com/doubanapp/app?channel=nimingye

driver.close()
复制<br><br>
<br>根据xpath定位元素:driver.find_elements(By.XPATH,"//*[@id='s']/h1/a")
<br>根据class定位元素:driver.find_elements(By.CLASS_NAME, "box")
<br>根据link_text定位元素:driver.find_elements(By.LINK_TEXT, "下载豆瓣 App")
<br>根据tag_name定位元素:driver.find_elements(By.TAG_NAME, "h1")
<br>获取元素文本内容:element.text
<br>获取元素标签属性: element.get_attribute("href")
<br>向输入框输入数据: element.send_keys(data)
<br><br><br>
<br>掌握 selenium处理cookie等方法
<br>掌握 selenium中switch的使用
<br>掌握selenium中无头浏览器的设置
<br><br>我们已经基本了解了selenium的基本使用了. 但是呢, 不知各位有没有发现, 每次打开浏览器的时间都比较长. 这就比较耗时了. 我们写的是爬虫程序. 目的是数据. 并不是想看网页. 那能不能让浏览器在后台跑呢? 答案是可以的<br>Pythonfrom selenium.webdriver import Chrome
from selenium.webdriver.chrome.options import Options

opt = Options()
opt.add_argument("--headless")
opt.add_argument('--disable-gpu')
opt.add_argument("--window-size=4000,1600")  # 设置窗口大小

web = Chrome(options=opt)
复制<br><br>通过driver.get_cookies()能够获取所有的cookie<br>
<br>
获取cookie
PythondictCookies = driver.get_cookies()
复制

<br>
设置cookie
Pythondriver.add_cookie(dictCookies)
复制

<br>
删除cookie
Python#删除一条cookie
driver.delete_cookie("CookieName")
# 删除所有的cookie
driver.delete_all_cookies()
复制

<br><br>
<br>
为什么需要等待
如果网站采用了动态html技术，那么页面上的部分元素出现时间便不能确定，这个时候就可以设置一个等待时间，强制等待指定时间，等待结束之后进行元素定位，如果还是无法定位到则报错 

<br>
页面等待的三种方法

<br>
强制等待
Pythonimport time
time.sleep(n)      # 阻塞等待设定的秒数之后再继续往下执行
复制

<br>
显式等待(自动化web测试使用，爬虫基本不用)
Pythonfrom selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

WebDriverWait(driver, 10,0.5).until( EC.presence_of_element_located((By.ID, "myDynamicElement"))
# 显式等待指定某个条件，然后设置最长等待时间10，在10秒内每隔0.5秒使用指定条件去定位元素，如果定位到元素则直接结束等待，如果在10秒结束之后仍未定位到元素则报错
复制

<br>
隐式等待 隐式等待设置之后代码中的所有元素定位都会做隐式等待
Pythondriver.implicitly_wait(10)    # 在指定的n秒内每隔一段时间尝试定位元素，如果n秒结束还未被定位出来则报错
复制



<br>注意：<br>Selenium显示等待和隐式等待的区别
1、selenium的显示等待
原理：显示等待，就是明确要等到某个元素的出现或者是某个元素的可点击等条件，等不到，就一直等，除非在规定的时间之内都没找到，就会跳出异常Exception<br>(简而言之，就是直到元素出现才去操作，如果超时则报异常)<br>2、selenium的隐式等待<br>原理：隐式等待，就是在创建driver时，为浏览器对象创建一个等待时间，这个方法是得不到某个元素就等待一段时间，直到拿到某个元素位置。
注意：在使用隐式等待的时候，实际上浏览器会在你自己设定的时间内部断的刷新页面去寻找我们需要的元素<br><br><br>Python也可以使用 window_handles 方法来获取每个窗口的操作对象。例如：

# 1. 获取当前所有的窗口
current_windows = driver.window_handles

# 2. 根据窗口索引进行切换
driver.switch_to.window(current_windows[1])

driver.switch_to.window(web.window_handles[-1])  # 跳转到最后一个窗口
driver.switch_to.window(current_windows[0])  # 回到第一个窗口
复制<br><br>Pythondriver.switch_to.frame(name/el/id)     传入的参数可以使iframe对应的id值，也可以是用元素定位之后的元素对象
复制<br>动手：qq邮箱<br>在使用selenium登录qq邮箱的过程中，我们会发现，无法在邮箱的登录input标签中输入内容，通过观察源码可以发现，form表单在一个frame中，所以需要切换到frame中<br><br>Pythonalert = driver.switch_to_alert()
复制<br><br>Pythondriver.forward()     # 前进
driver.back()        # 后退
driver.refresh() 		 # 刷新
driver.close()       # 关闭当前窗口
复制<br><br>driver.maximize_window()  #最大化浏览器窗口
复制<br><br>
<br>优点

<br>selenium能够执行页面上的js，对于js渲染的数据和模拟登陆处理起来非常容易
<br>使用难度简单
<br>爬取速度慢，爬取频率更像人的行为，天生能够应对一些反爬措施


<br>缺点

<br>由于selenium操作浏览器，因此会将发送所有的请求，因此占用网络带宽
<br>由于操作浏览器，因此占用的内存非常大(相比较之前的爬虫)
<br>速度慢，对于效率要求高的话不建议使用


<br><br>
<br>获取cookie: get_cookies()
<br>删除cookie: delete_all_cookies()
<br>切换窗口:switch_to.window()
<br>切换iframe: switch_to.frame()
<br><br><a rel="noopener" class="external-link" href="https://blog.csdn.net/qq_35999017/article/details/123922952" target="_blank">https://blog.csdn.net/qq_35999017/article/details/123922952</a><br><a rel="noopener" class="external-link" href="https://blog.csdn.net/qq_27109535/article/details/125468643" target="_blank">https://blog.csdn.net/qq_27109535/article/details/125468643</a>]]></description><link>爬虫\爬虫文档\selenium.html</link><guid isPermaLink="false">爬虫/爬虫文档/selenium.md</guid><pubDate>Mon, 16 Oct 2023 15:11:55 GMT</pubDate></item><item><title><![CDATA[urllib与requests]]></title><description><![CDATA[ 
 <br><br><br><br>了解urllib的基本使用 <br><br><br>除了requests模块可以发送请求之外, urllib模块也可以实现请求的发送,只是操作方法略有不同!<br>urllib在python中分为urllib和urllib2，在python3中为urllib<br>下面以python3的urllib为例进行讲解<br><br><br>
<br>
构造简单请求
Pythonimport urllib
#构造请求
request = urllib.request.Request("http://www.baidu.com")
#发送请求获取响应
response = urllib.request.urlopen(request)
复制

<br>
传入headers参数
Pythonimport urllib
#构造headers
headers = {"User-Agent" : "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)"} 
#构造请求
request = urllib.request.Request(url, headers=headers)
#发送请求
response = urllib.request.urlopen(request)
复制

<br>
传入data参数 实现发送post请求（示例）
Pythonimport urllib.request
import urllib.parse
import json

url = 'https://ifanyi.iciba.com/index.php?c=trans&amp;m=fy&amp;client=6&amp;auth_user=key_ciba&amp;sign=99730f3bf66b2582'

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15',

}
data = {
    'from': 'zh',
    'to': 'en',
    'q': 'lucky 是一个帅气的老'
}
# 使用post方式
# 需要
data = urllib.parse.urlencode(data).encode('utf-8')
req = urllib.request.Request(url, data=data, headers=headers)
res = urllib.request.urlopen(req)
print(res.getcode())
print(res.geturl())
data = json.loads(res.read().decode('utf-8'))
print(data)
复制

<br><br>获取响应的html字符串,bytes类型<br>Python#发送请求
response = urllib.request.urlopen("http://www.baidu.com")
#获取响应
response.read()
复制<br><br>Pythonimport urllib
import json

url = 'http://www.baidu.com'
#构造headers
headers = {"User-Agent" : "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)"}
#构造请求
request = urllib.request.Request(url, headers = headers)
#发送请求
response = urllib.request.urlopen(request)
#获取html字符串
html_str = response.read().decode('utf-8')
print(html_str)
复制<br><br>
<br>urllib.request中实现了构造请求和发送请求的方法
<br>urllib.request.Request(url,headers,data)能够构造请求
<br>urllib.request.urlopen能够接受request请求或者url地址发送请求，获取响应
<br>response.read()能够实现获取响应中的bytes字符串
<br><br><br><br>
<br>了解 requests模块的介绍
<br>掌握 requests的基本使用
<br>掌握 response常见的属性
<br>掌握 requests.text和content的区别
<br>掌握 解决网页的解码问题
<br>掌握 requests模块发送带headers的请求
<br>掌握 requests模块发送带参数的get请求
<br><br><br>
<br>企业中用的最多的就是requests
<br>requests的底层实现就是urllib
<br>requests在python2 和python3中通用，方法完全一样
<br>requests简单易用
<br><br>作用：发送网络请求，返回响应数据<br>命令： pip install requests  <br>requests模块发送简单的get请求、获取响应<br>需求：通过requests向百度首页发送请求，获取百度首页的数据<br>Pythonimport requests

# 目标url
url = 'https://www.baidu.com'

# 向目标url发送get请求
response = requests.get(url)

# 打印响应内容
print(response.text)
复制<br>response的常用属性：<br>
<br>
response.text 响应体 str类型

<br>
response.encoding  从HTTP　header中猜测的响应内容的编码方式

<br>
respones.content 响应体 bytes类型

<br>
response.status_code 响应状态码

<br>
response.request.headers 响应对应的请求头

<br>
response.headers 响应头

<br>
response.cookies 响应的cookie（经过了set-cookie动作）

<br>
response.url  获取访问的url

<br>
response.json()  获取json数据 得到内容为字典 (如果接口响应体的格式是json格式时)

<br>
response.ok 
如果status_code小于等于200，response.ok返回True。
如果status_code大于200，response.ok返回False。

<br>思考：text是response的属性还是方法呢？<br>
<br>一般来说名词，往往都是对象的属性，对应的动词是对象的方法
<br><br>
<br>response.text

<br>类型：str
<br>解码类型： requests模块自动根据HTTP 头部对响应的编码作出有根据的推测，推测的文本编码
<br>如何修改编码方式：response.encoding="gbk/UTF-8"


<br>response.content

<br>类型：bytes
<br>解码类型： 没有指定
<br>如何修改编码方式：response.content.deocde("utf8")


<br>获取网页源码的通用方式：<br>
<br>response.content.decode()
<br>response.content.decode("UTF-8")
<br>response.text
<br>以上三种方法从前往后尝试，能够100%的解决所有网页解码的问题<br>所以：更推荐使用response.content.deocde()的方式获取响应的html页面<br><br>
我们来把www.baidu.com的图片保存到本地
<br>思考：<br>
<br>以什么方式打开文件
<br>保存什么格式的内容
<br>分析：<br>
<br>图片的url: <a rel="noopener" class="external-link" href="https://www.baidu.com/img/bd_logo1.png" target="_blank">https://www.baidu.com/img/bd_logo1.png</a>
<br>利用requests模块发送请求获取响应
<br>以2进制写入的方式打开文件，并将response响应的二进制内容写入
<br>Pythonimport requests

# 图片的url
url = 'https://www.baidu.com/img/bd_logo1.png'

# 响应本身就是一个图片,并且是二进制类型
response = requests.get(url)

# print(response.content)

# 以二进制+写入的方式打开文件
with open('baidu.png', 'wb') as f:
    # 写入response.content bytes二进制类型
    f.write(response.content)
复制<br><br>
我们先写一个获取百度首页的代码
<br>Pythonimport requests

url = 'https://www.baidu.com'

response = requests.get(url)

print(response.content)

# 打印响应对应请求的请求头信息
print(response.request.headers)
复制<br><br>对比浏览器上百度首页的网页源码和代码中的百度首页的源码，有什么不同？<br>代码中的百度首页的源码非常少，为什么？<br><br>模拟浏览器，欺骗服务器，获取和浏览器一致的内容<br><br>Pythonheaders = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}
复制<br><br>Pythonrequests.get(url, headers=headers)
复制<br><br>Pythonimport requests

url = 'https://www.baidu.com'

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

# 在请求头中带上User-Agent，模拟浏览器发送请求
response = requests.get(url, headers=headers)

# print(response.content)

# 打印请求头信息
print(response.request.headers)
复制<br><br>
我们在使用百度搜索的时候经常发现url地址中会有一个 ?，那么该问号后边的就是请求参数，又叫做查询字符串
<br><br>例1： <a rel="noopener" class="external-link" href="http://www.webkaka.com/tutorial/server/2015/021013/" target="_blank">http://www.webkaka.com/tutorial/server/2015/021013/</a><br>例2：<a rel="noopener" class="external-link" href="https://www.baidu.com/s?wd=python&amp;a=c" target="_blank">https://www.baidu.com/s?wd=python&amp;a=c</a><br>例1中没有请求参数！例2中?后边的就是请求参数<br><br>Pythonkw = {'wd':'长城'}
复制<br><br>Pythonrequests.get(url,params=kw)
复制<br><br>在url地址中， 很多参数是没有用的，比如百度搜索的url地址，其中参数只有一个字段有用，其他的都可以删除 如何确定那些请求参数有用或者没用：挨个尝试！ 对应的,在后续的爬虫中，越到很多参数的url地址，都可以尝试删除参数<br><br>
<br>
对https://www.baidu.com/s?wd=python发起请求可以使用requests.get(url, params=kw)的方式
Python# 方式一：利用params参数发送带参数的请求
import requests

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

# 这是目标url
# url = 'https://www.baidu.com/s?wd=python'

# 最后有没有问号结果都一样
url = 'https://www.baidu.com/s?'

# 请求参数是一个字典 即wd=python
kw = {'wd': 'python'}

# 带上请求参数发起请求，获取响应
response = requests.get(url, headers=headers, params=kw)

# 当有多个请求参数时，requests接收的params参数为多个键值对的字典，比如 '?wd=python&amp;a=c'--&gt;{'wd': 'python', 'a': 'c'}

print(response.content)
复制

<br>
也可以直接对https://www.baidu.com/s?wd=python完整的url直接发送请求，不使用params参数
Python# 方式二：直接发送带参数的url的请求
import requests

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

url = 'https://www.baidu.com/s?wd=python'

# kw = {'wd': 'python'}

# url中包含了请求参数，所以此时无需params
response = requests.get(url, headers=headers)
复制

<br><br>
<br>requests模块的介绍：能够帮助我们发起请求获取响应
<br>requests的基本使用：requests.get(url)
<br>以及response常见的属性：

<br>response.text 响应体 str类型
<br>respones.content 响应体 bytes类型
<br>response.status_code 响应状态码
<br>response.request.headers 响应对应的请求头
<br>response.headers 响应头
<br>response.request._cookies 响应对应请求的cookie
<br>response.cookies 响应的cookie（经过了set-cookie动作）


<br>掌握 requests.text和content的区别：text返回str类型，content返回bytes类型
<br>掌握 解决网页的解码问题：

<br>response.content.decode()
<br>response.content.decode("UTF-8")
<br>response.text


<br>掌握 requests模块发送带headers的请求：requests.get(url, headers={})
<br>掌握 requests模块发送带参数的get请求：requests.get(url, params={})
<br><br><br>
<br>能够应用requests发送post请求的方法
<br>能够应用requests模块使用代理的方法
<br>了解代理ip的分类
<br><br><br>
思考：哪些地方我们会用到POST请求？
<br>
<br>登录注册（ POST 比 GET 更安全）
<br>需要传输大文本内容的时候（ POST 请求对数据长度没有要求）
<br>所以同样的，我们的爬虫也需要在这两个地方回去模拟浏览器发送post请求<br><br>
<br>
用法：
Pythonresponse = requests.post("http://www.baidu.com/", data = data, headers=headers)
复制

<br>
data 的形式：字典

<br><br>下面面我们通过金山翻译的例子看看post请求如何使用：<br>地址：<a rel="noopener" class="external-link" href="https://www.iciba.com/fy" target="_blank">https://www.iciba.com/fy</a><br><br>
<br>
抓包确定请求的url地址
![截屏2022-04-20 下午3.22.11](requests1.assets/截屏2022-04-20 下午3.22.11.png)

<br>
确定请求的参数
![截屏2022-04-20 下午3.23.07](requests1.assets/截屏2022-04-20 下午3.23.07.png)

<br>
确定返回数据的位置
“requests1.assets/image-20220420152404175.png” could not be found.

<br>
模拟浏览器获取数据
Pythonimport requests
import json

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

url = 'https://ifanyi.iciba.com/index.php?c=trans&amp;m=fy&amp;client=6&amp;auth_user=key_ciba&amp;sign=99730f3bf66b2582'

data = {
    'from': 'zh',
    'to': 'en',
    'q': 'lucky 是一个帅气的老师'
}

res = requests.post(url, headers=headers, data=data)
# print(res.status_code)

# 返回的是json字符串 需要在进行转换为字典
data = json.loads(res.content.decode('UTF-8'))
# print(type(data))
print(data)
print(data['content']['out'])
复制

<br><br>在模拟登陆等场景，经常需要发送post请求，直接使用requests.post(url,data)即可<br><br><br>
<br>让服务器以为不是同一个客户端在请求
<br>防止我们的真实地址被泄露，防止被追究
<br><br>“requests1.assets/使用代理的过程.png” could not be found.<br><br>“requests1.assets/正向代理和反向代理的区别.png” could not be found.<br>通过上图可以看出：<br>
<br>正向代理：对于浏览器知道服务器的真实地址，例如VPN
<br>反向代理：浏览器不知道服务器的真实地址，例如nginx
<br>详细讲解：<br>正向代理是客户端与正向代理客户端在同一局域网，客户端发出请求，正向代理 替代客户端向服务器发出请求。服务器不知道谁是真正的客户端，正向代理隐藏了真实的请求客户端。
反向代理：服务器与反向代理在同一个局域网，客服端发出请求，反向代理接收请求 ，反向代理服务器会把我们的请求分转发到真实提供服务的各台服务器Nginx就是性能非常好的反向代理服务器，用来做负载均衡<br>“requests1.assets/20190715113403480.png” could not be found.<br><br>
<br>
用法：
Pythonrequests.get("http://www.baidu.com",  proxies = proxies)
复制

<br>
proxies的形式：字典

<br>
例如：
Json proxies = {
      "http": "http://12.34.56.79:9527",
      "https": "https://12.34.56.79:9527",
}
复制

<br><br>根据代理ip的匿名程度，代理IP可以分为下面四类：<br>
<br>透明代理(Transparent Proxy)：透明代理的意思是客户端根本不需要知道有代理服务器的存在，但是它传送的仍然是真实的IP。使用透明代理时，对方服务器是可以知道你使用了代理的，并且他们也知道你的真实IP。你要想隐藏的话，不要用这个。透明代理为什么无法隐藏身份呢?因为他们将你的真实IP发送给了对方服务器，所以无法达到保护真实信息。
<br>匿名代理(Anonymous Proxy)：匿名代理隐藏了您的真实IP，但是向访问对象可以检测是使用代理服务器访问他们的。会改变我们的请求信息，服务器端有可能会认为我们使用了代理。不过使用此种代理时，虽然被访问的网站不能知道你的ip地址，但仍然可以知道你在使用代理，当然某些能够侦测ip的网页也是可以查到你的ip。（<a data-tooltip-position="top" aria-label="https://wenku.baidu.com/view/9bf7b5bd3a3567ec102de2bd960590c69fc3d8cf.html%EF%BC%89" rel="noopener" class="external-link" href="https://wenku.baidu.com/view/9bf7b5bd3a3567ec102de2bd960590c69fc3d8cf.html%EF%BC%89" target="_blank">https://wenku.baidu.com/view/9bf7b5bd3a3567ec102de2bd960590c69fc3d8cf.html）</a>
<br>高匿代理(Elite proxy或High Anonymity Proxy)：高匿名代理不改变客户机的请求，这样在服务器看来就像有个真正的客户浏览器在访问它，这时客户的真实IP是隐藏的，完全用代理服务器的信息替代了您的所有信息，就象您就是完全使用那台代理服务器直接访问对象，同时服务器端不会认为我们使用了代理。IPDIEA覆盖全球240＋国家地区ip高匿名代理不必担心被追踪。
<br>在使用的使用，毫无疑问使用高匿代理效果最好<br>从请求使用的协议可以分为：<br>
<br>http代理
<br>https代理
<br>socket代理等
<br>不同分类的代理，在使用的时候需要根据抓取网站的协议来选择<br><br>
<br>
反反爬
使用代理ip是非常必要的一种反反爬的方式
但是即使使用了代理ip，对方服务器任然会有很多的方式来检测我们是否是一个爬虫，比如：

<br>
一段时间内，检测IP访问的频率，访问太多频繁会屏蔽

<br>
检查Cookie，User-Agent，Referer等header参数，若没有则屏蔽

<br>
服务方购买所有代理提供商，加入到反爬虫数据库里，若检测是代理则屏蔽
所以更好的方式在使用代理ip的时候使用随机的方式进行选择使用，不要每次都用一个代理ip



<br>
代理ip池的更新
购买的代理ip很多时候大部分(超过60%)可能都没办法使用，这个时候就需要通过程序去检测哪些可用，把不能用的删除掉。

<br>
<br>
代理服务器平台的使用：
当然还有很多免费的，但是大多都不可用需要自己尝试

<br><a rel="noopener" class="external-link" href="http://www.66ip.cn" target="_blank">http://www.66ip.cn</a>
<br><a rel="noopener" class="external-link" href="https://ip.jiangxianli.com/?page=1" target="_blank">https://ip.jiangxianli.com/?page=1</a>
<br><a rel="noopener" class="external-link" href="https://www.zdaye.com" target="_blank">https://www.zdaye.com</a>
<br><a rel="noopener" class="external-link" href="https://www.kuaidaili.com/free" target="_blank">https://www.kuaidaili.com/free</a>


<br><br>
<br>
浏览器配置代理
右边三点==&gt; 设置==&gt; 高级==&gt; 代理==&gt; 局域网设置==&gt; 为LAN使用代理==&gt; 输入ip和端口号即可
参考网址：<a rel="noopener" class="external-link" href="https://jingyan.baidu.com/article/a681b0dece76407a1843468d.html" target="_blank">https://jingyan.baidu.com/article/a681b0dece76407a1843468d.html</a>

<br>
代码配置
urllib
Pythonhandler = urllib.request.ProxyHandler({'http': '114.215.95.188:3128'})
opener = urllib.request.build_opener(handler)
# 后续都使用opener.open方法去发送请求即可
复制
requests
Python# 用到的库
import requests
# 写入获取到的ip地址到proxy
# 一个ip地址
proxy = {
    'http':'http://221.178.232.130:8080'
}
"""
# 多个ip地址
proxy = [
  {'http':'http://221.178.232.130:8080'},
  {'http':'http://221.178.232.130:8080'}
]
import random
proxy = random.choice(proxy)
"""

# 使用代理
proxy = {
    'http': 'http://58.20.184.187:9091'
}

result = requests.get("http://httpbin.org/ip", proxies=proxy)

print(result.text)
复制

<br><br>
<br>requests发送post请求使用requests.post方法，带上请求体，其中请求体需要时字典的形式，传递给data参数接收
<br>在requests中使用代理，需要准备字典形式的代理，传递给proxies参数接收
<br>不同协议的url地址，需要使用不同的代理去请求
]]></description><link>爬虫\爬虫文档\urllib与requests1.html</link><guid isPermaLink="false">爬虫/爬虫文档/urllib与requests1.md</guid><pubDate>Fri, 15 Sep 2023 11:26:21 GMT</pubDate></item><item><title><![CDATA[urllib与requests2]]></title><description><![CDATA[ 
 <br><br>学习目标<br>掌握requests处理cookie的三种方法<br><br>
为了能够通过爬虫获取到登录后的页面，或者是解决通过cookie的反扒，需要使用request来处理cookie相关的请求
<br><br>
<br>带上cookie的好处

<br>能够访问登录后的页面
<br>能够实现部分反反爬


<br>带上cookie的坏处

<br>一套cookie往往对应的是一个用户的信息，请求太频繁有更大的可能性被对方识别为爬虫
<br>那么上面的问题如何解决 ?使用多个账号


<br><br>使用requests处理cookie有三种方法：<br>
<br>cookie字符串放在headers中
<br>把cookie字典放传给请求方法的cookies参数接收
<br>使用requests提供的session模块
<br><br><br>“requests.assets/headers中的cookie.png” could not be found.<br>
<br>headers中的cookie：

<br>使用分号(;)隔开
<br>分号两边的类似a=b形式的表示一条cookie
<br>a=b中，a表示键（name），b表示值（value）
<br>在headers中仅仅使用了cookie的name和value


<br><br>“requests.assets/cookie的具体字段.png” could not be found.<br>由于headers中对cookie仅仅使用它的name和value，所以在代码中我们仅仅需要cookie的name和value即可<br><br>复制浏览器中的cookie到代码中使用<br>Pythonheaders = {
"User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36",
"Cookie":" Pycharm-26c2d973=dbb9b300-2483-478f-9f5a-16ca4580177e; Hm_lvt_98b9d8c2fd6608d564bf2ac2ae642948=1512607763; Pycharm-26c2d974=f645329f-338e-486c-82c2-29e2a0205c74; _xsrf=2|d1a3d8ea|c5b07851cbce048bd5453846445de19d|1522379036"}

requests.get(url,headers=headers)
复制<br><br>cookie有过期时间 ，所以直接复制浏览器中的cookie可能意味着下一程序继续运行的时候需要替换代码中的cookie，对应的我们也可以通过一个程序专门来获取cookie供其他程序使用；当然也有很多网站的cookie过期时间很长，这种情况下，直接复制cookie来使用更加简单<br><br>
<br>cookies的形式：字典
<br>Jsoncookies = {"cookie的name":"cookie的value"}
复制<br>
<br>使用方法：
<br>Pythonrequests.get(url,headers=headers,cookies=cookie_dict)
复制<br>
<br>
实例（爬取雪球网）
在网络中找到当前请求的网址 点击cookies 将当前的k,value复制到代码中
“requests.assets/image-20220420190426741.png” could not be found.
Pythoncookie_dict = {
    'u': '1990923459',
    'bid': '1f110dfd43538f4b8362dfcd21ffbb64_l27g4lfl',
    'xq_is_login': '1',
    'xq_r_token': '5dcbe83944f0b75325f91246061d4a2a01999367'
}
复制
完整代码
Pythonimport requests

# 携带cookie登录雪球网  抓取完善个人资料页面
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36',
    'Referer': 'https://xueqiu.com/u/1990923459',
    'Host': 'xueqiu.com',
}
url = 'https://xueqiu.com/users/connectnew?redirect=/setting/user'

cookie_dict = {
    'u': '1990923459',
    'bid': '1f110dfd43538f4b8362dfcd21ffbb64_l27g4lfl',
    'xq_is_login': '1',
    'xq_r_token': '5dcbe83944f0b75325f91246061d4a2a01999367'
}
res = requests.get(url, headers=headers, cookies=cookie_dict)
with open('雪球网.html', 'w') as f:
    f.write(res.content.decode('UTF-8'))
    print(res.content.decode('UTF-8'))
复制
成果
“requests.assets/image-20220420190612671.png” could not be found.

<br><br>
前面使用手动的方式使用cookie，那么有没有更好的方法在requets中处理cookie呢？
<br>requests 提供了一个叫做session类，来实现客户端和服务端的会话保持<br>会话保持有两个内涵：<br>
<br>保存cookie，下一次请求会带上前一次的cookie
<br>实现和服务端的长连接，加快请求速度
<br><br>Pythonsession = requests.session()
response = session.get(url,headers)
复制<br>session实例在请求了一个网站后，对方服务器设置在本地的cookie会保存在session中，下一次再使用session请求对方服务器的时候，会带上前一次的cookie<br><br>
<br>
17k小说网    <a rel="noopener" class="external-link" href="https://passport.17k.com/" target="_blank">https://passport.17k.com/</a>

<br>
古诗文：<a rel="noopener" class="external-link" href="https://so.gushiwen.cn" target="_blank">https://so.gushiwen.cn</a>

<br>
打码平台  
图鉴   <a rel="noopener" class="external-link" href="http://www.ttshitu.com/" target="_blank">http://www.ttshitu.com/</a>

<br><br>
<br>准备url地址和请求参数
<br>构造session发送post请求
<br>使用session请求个人主页，观察是否请求成功
<br><br>
<br>cookie字符串可以放在headers字典中，键为Cookie，值为cookie字符串
<br>可以把cookie字符串转化为字典，使用请求方法的cookies参数接收
<br>使用requests提供的session模块，能够自动实现cookie的处理，包括请求的时候携带cookie，获取响应的时候保存cookie
<br><br><br>
<br>掌握requests中cookirJar的处理方法
<br>掌握requests解决https证书错误的问题
<br>掌握requests中超时参数的使用
<br><br>
使用request获取的resposne对象，具有cookies属性，能够获取对方服务器设置在本地的cookie，但是如何使用这些cookie呢？
<br><br>
<br>response.cookies是CookieJar类型
<br>使用requests.utils.dict_from_cookiejar，能够实现把cookiejar对象转化为字典
<br><br>Pythonimport requests

url = "http://www.baidu.com"
#发送请求，获取resposne
response = requests.get(url)
print(type(response.cookies))

#使用方法从cookiejar中提取数据  等同于  dict(response.cookies)
cookies = requests.utils.dict_from_cookiejar(response.cookies)
print(cookies)
复制<br>输出为:<br>Python&lt;class 'requests.cookies.RequestsCookieJar'&gt;
{'BDORZ': '27315'}
复制<br><br>在前面的requests的session类中，我们不需要处理cookie的任何细节，如果有需要，我们可以使用上述方法来解决<br><br>
经常我们在网上冲浪时，经常能够看到下面的提示：
<br>“requests.assets/12306ssl错误.png” could not be found.<br>出现这个问题的原因是：ssl的证书不安全导致<br><br>那么如果在代码中请求会怎么样呢？<br>Pythonimport requests

url = "https://www.12306.cn/mormhweb/"
response = requests.get(url)
复制<br>返回证书错误，如下：<br>ssl.CertificateError ...
复制<br><br>为了在代码中能够正常的请求，我们修改添加一个参数<br>Pythonimport requests

url = "https://www.12306.cn/mormhweb/"
response = requests.get(url, verify=False)
复制<br><br>
在平时网上冲浪的过程中，我们经常会遇到网络波动，这个时候，一个请求等了很久可能任然没有结果
在爬虫中，一个请求很久没有结果，就会让整个项目的效率变得非常低，这个时候我们就需要对请求进行强制要求，让他必须在特定的时间内返回结果，否则就报错
<br><br>Pythonresponse = requests.get(url,timeout=3)
复制<br>通过添加timeout参数，能够保证在3秒钟内返回响应，否则会报错<br><br>这个方法还能够拿来检测代理ip的质量，如果一个代理ip在很长时间没有响应，那么添加超时之后也会报错，对应的这个ip就可以从代理ip池中删除]]></description><link>爬虫\爬虫文档\urllib与requests2.html</link><guid isPermaLink="false">爬虫/爬虫文档/urllib与requests2.md</guid><pubDate>Sat, 16 Sep 2023 08:25:47 GMT</pubDate></item><item><title><![CDATA[xpath]]></title><description><![CDATA[<a class="tag" href="?query=tag:xpath" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#xpath</a> 
 <br><br><a href=".?query=tag:xpath" class="tag" target="_blank" rel="noopener">#xpath</a> <br><br>安装<br>
<br>
安装lxml库
pip install lxml -i pip源

<br><br>解析流程<br>
<br>实例化一个etree的对象，把即将被解析的页面源码加载到该对象
<br>调用该对象的xpath方法结合着不同形式的xpath表达进行标签定位和数据提取
<br>使用<br>
<br>
导入lxml.etree
from lxml import etree

<br>
etree.parse()
解析本地html文件
html_tree = etree.parse('XX.html')

<br>
etree.HTML()(建议)
解析网络的html字符串
html_tree = etree.HTML(html字符串)

<br>
html_tree.xpath()
使用xpath路径查询信息，返回一个列表

<br>注意：如果lxml解析本地HTML文件报错可以按照如下添加参数<br>Pythonparser = etree.HTMLParser(encoding="utf-8")
selector = etree.parse('./lol_1.html',parser=parser)
result=etree.tostring(selector)
复制<br><br>XPath 是一门在 XML 文档中查找信息的语言。XPath 用于在 XML 文档中通过元素和属性进行导航。<br>
<br>
路径表达式

实例
在下面的表格中，我们已列出了一些路径表达式以及表达式的结果：


<br>
谓语（Predicates）
谓语用来查找某个特定的节点或者包含某个指定的值的节点。
谓语被嵌在方括号中。
实例
在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果：


<br>
选取未知节点
XPath 通配符可用来选取未知的 XML 元素。

实例
在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：

选取若干路径
通过在路径表达式中使用“|”运算符，您可以选取若干个路径。
实例
在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：


<br>
逻辑运算

<br>
查找所有id属性等于head并且class属性等于s_down的div标签
Python//div[@id="head" and @class="s_down"]
复制

<br>
选取文档中的所有 title 和 price 元素。
//title | //price
复制
注意: “|”两边必须是完整的xpath路径



<br>
属性查询

<br>
查找所有包含id属性的div节点
//div[@id]
复制

<br>
查找所有id属性等于maincontent的div标签
//div[@id="maincontent"]
复制

<br>
查找所有的class属性
//@class
复制

<br>
//@attrName
//li[@name="xx"]//text()  # 获取li标签name为xx的里面的文本内容
复制



<br>
获取第几个标签  索引从1开始
tree.xpath('//li[1]/a/text()')  # 获取第一个
tree.xpath('//li[last()]/a/text()')  # 获取最后一个
tree.xpath('//li[last()-1]/a/text()')  # 获取倒数第二个
复制

<br>
内容查询
查找所有div标签下的直接子节点h1的内容
//div/h1/text()
复制

<br>
属性值获取
//div/a/@href   获取a里面的href属性值
复制

<br>
获取所有
//*  #获取所有
//*[@class="xx"]  #获取所有class为xx的标签
复制

<br>
获取节点内容转换成字符串
c = tree.xpath('//li/a')[0]
result=etree.tostring(c, encoding='utf-8')
print(result.decode('UTF-8'))
复制

<br>练习<br>抓取豆瓣的标题，封面图片标签，图片的src，简介，评分<br>当前包含虚构类和非虚构类]]></description><link>爬虫\爬虫文档\xpath.html</link><guid isPermaLink="false">爬虫/爬虫文档/xpath.md</guid><pubDate>Mon, 16 Oct 2023 15:13:05 GMT</pubDate></item><item><title><![CDATA[Scrapy框架学习]]></title><description><![CDATA[<a class="tag" href="?query=tag:scrapy" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scrapy</a> <a class="tag" href="?query=tag:框架" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#框架</a> 
 <br><br><a href=".?query=tag:scrapy" class="tag" target="_blank" rel="noopener">#scrapy</a> <a href=".?query=tag:框架" class="tag" target="_blank" rel="noopener">#框架</a><br><br><br>Pythonpip install scrapy
复制<br><br>Pythonscrapy startproject: 创建新的爬虫项目。

scrapy genspider: 创建新的爬虫。

scrapy crawl: 运行爬虫。

scrapy shell: 启动 Scrapy shell，可以在交互式命令行中测试爬虫代码。

scrapy view: 打开给定 URL 的页面，方便调试。

scrapy fetch: 使用 Scrapy 的下载器下载给定 URL 的页面。
#配置日志打印级别 settings.py
L0G_LEVEL='ERROR' #当有报错ERROR了会执行打印
或者用以下命令启动
scrapy crawl dzx -s LOG_LEVEL=ERROR 
复制<br>
pipelines.py  ——&gt;管道，保存数据
settings.py    ——&gt;设置文件，UA,启动管道
spiders          ——&gt;自己定义的spider的文件夹
<br><br>
<br>
ROBOTSTXT_OBEY = False
robots是一种反爬协议。在协议中规定了哪些身份的爬虫无法爬取的资源有哪些。
在配置文件中setting，取消robots的监测：

<br>
在配置文件中配置全局的UA：USER_AGENT='xxxx'

<br>
在配置文件中加入日志等级：LOG_LEVEL = 'ERROR'  只输出错误信息
其它日志级别

<br>CRITICAL  严重错误
<br>ERROR  错误
<br>WARNING  警告
<br>INFO  消息
<br>DEBUG   调试


<br><img alt="image-20230919204633965" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919204633965.png" referrerpolicy="no-referrer"><br><br><br>首先在目标文件夹下打开cmd或者终端<br>Pythonscrapy startproject duanzhi(项目名字自定义)
cd duanzhi
scrapy genspider dzx duanzixing.com 
复制<br><img alt="image-20230919204329743" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919204329743.png" referrerpolicy="no-referrer"><br><img alt="image-20230919204358812" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919204358812.png" referrerpolicy="no-referrer"><br>设置一下输出日志  当有报错ERROR了会执行打印<br><img alt="image-20230919204716206" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919204716206.png" referrerpolicy="no-referrer"><br>写一部分代码来观察<br>Pythonclass DzxSpider(scrapy.Spider):
    name = "dzx"
    # allowed_domains = ["duanzixing.com"]
    start_urls = ["https://duanzixing.com"]

    def parse(self, response, **kwargs):
        print(response)
        # 获取响应的内容
        # print(response.text)
        # 当前响应的url
        print('response.url:', response.url)
        # 当前响应对应的请求的url
        print('response.request.url:', response.request.url)
        # 响应头
        print('response.headers:', response.headers)
        # 响应的请求头
        print('response.request.headers:', response.request.headers)
        # 响应体
        print('response.body:', response.body)
        # 返回响应的内容
        print('response.text:', response.text)
        # 响应的状态码
        print('response.status:', response.status)
        # 获取响应的json数据 如果响应的内容不是json格式的数据，会报错
        # print('response.json():', response.json())
        
复制<br><br><img alt="image-20230919211800212" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919211800212.png" referrerpolicy="no-referrer"><br><img alt="image-20230919211843983" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919211843983.png" referrerpolicy="no-referrer"><br>可以得到<br>标题和正文<br><br>Pythonimport requests
from lxml import etree

url = "https://duanzixing.com/"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',
}

res = requests.get(url, headers=headers)

html = etree.HTML(res.text)

# 获取所有的段子
excerpt_list = html.xpath('/html/body/section/div/div/article')
# print(len(excerpt_list))
for tree in excerpt_list:
    title = tree.xpath('./header/h2/a/text()')
    text = tree.xpath('./p[@class="note"]/text()')
    print(title)
    print(text)
复制<br><img alt="image-20230919212233825" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919212233825.png" referrerpolicy="no-referrer"><br><br>Python    def parse(self, response, **kwargs):
        excerpt_list=response.xpath('/html/body/section/div/div/article')
        data={}
        for tree in excerpt_list:
            title = tree.xpath('./header/h2/a/text()').extract()
            text = tree.xpath('./p[@class="note"]/text()').extract()
            print(title)
            print(text)
复制<br><img alt="image-20230919212454370" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919212454370.png" referrerpolicy="no-referrer"><br>Url复制<br><br><br>在配置文件中存放一些公共变量，在后续的项目中方便修改，如：本地测试数据库和部署服务器的数据库不一致<br><br>
<br>变量名一般全部大写
<br>导入即可使用
<br><br>
<br>
USER_AGENT 设置ua

<br>
ROBOTSTXT_OBEY 是否遵守robots协议，默认是遵守

<br>
CONCURRENT_REQUESTS 设置并发请求的数量，默认是16个

<br>
DOWNLOAD_DELAY 下载延迟，默认无延迟 （下载器在从同一网站下载连续页面之前应等待的时间（以秒为单位）。这可以用来限制爬行速度，以避免对服务器造成太大影响）

<br>
COOKIES_ENABLED 是否开启cookie，即每次请求带上前一次的cookie，默认是开启的

<br>
DEFAULT_REQUEST_HEADERS 设置默认请求头，这里加入了USER_AGENT将不起作用

<br>
SPIDER_MIDDLEWARES 爬虫中间件，设置过程和管道相同

<br>
DOWNLOADER_MIDDLEWARES 下载中间件

<br>
LOG_LEVEL 控制终端输出信息的log级别，终端默认显示的是debug级别的log信息

<br>LOG_LEVEL = "WARNING"

<br>CRITICAL  严重
<br>ERROR  错误
<br>WARNING  警告
<br>INFO  消息
<br>DEBUG   调试




<br>
LOG_FILE 设置log日志文件的保存路径，如果设置该参数，终端将不再显示信息
LOG_FILE = "./test.log"

<br>
其他设置参考：<a rel="noopener" class="external-link" href="https://www.jianshu.com/p/df9c0d1e9087" target="_blank">https://www.jianshu.com/p/df9c0d1e9087</a>

<br><br><br>Pythonclass ITSpider(scrapy.Spider):
    name = 'ITSpider'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://duanzixing.com/page/1/']

    # 通过终端写入文件的方式
    def parse(self, response):
        article_list = response.xpath('/html/body/section/div/div/article')
        # 创建列表， 存储数据
        all_data = []
        for article in article_list:
            title = article.xpath('./header/h2/a/text()').extract_first()
            con = article.xpath('./p[2]/text()').extract_first()
            dic = {
                'title': title,
                'con': con
            }
            all_data.append(dic)
        return all_data
复制<br>
<br>
终端命令
scrapy crawl 爬虫名称-o 文件名.csv  
scrapy crawl ITSpider -o ITSpider.csv  
将文件存储到ITSpider.csv  文件中

<br><br>pipeline中常用的方法：<br>
<br>process_item(self,item,spider):实现对item数据的处理
<br>open_spider(self, spider): 在爬虫开启的时候仅执行一次
<br>close_spider(self, spider): 在爬虫关闭的时候仅执行一次
<br><br>Pythonclass DuanzhiPipeline:

    # 在爬虫开启的时候仅执行一次
    def open_spider(self, item):
        print("爬虫开始了")
        # 进行文件的打开
        self.f = open('duanzhi.txt', 'w', encoding='utf-8')

    # 实现对item数据的处理
    def process_item(self, item, spider):
        self.f.write(item['title'] + '\n')
        self.f.write(item['text'] + '\n')
        return item

    # 在爬虫关闭的时候仅执行一次
    def close_spider(self, item):
        print("爬虫结束了")
        self.f.close()
复制<br><br><br>Python    def parse(self, response, **kwargs):
        item = DuanzhiItem()
        excerpt_list = response.xpath('/html/body/section/div/div/article')
        for tree in excerpt_list:
            title = tree.xpath('./header/h2/a/text()').extract()
            text = tree.xpath('./p[@class="note"]/text()').extract()
            item['title'] = title
            item['text'] = text
            yield item
复制<br><br>Pythonclass DuanzhiItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title = scrapy.Field()
    text = scrapy.Field()
复制<br><br>Pythonclass DuanzhiPipeline:

    # 在爬虫开启的时候仅执行一次
    def open_spider(self, item):
        print("爬虫开始了")
        # 进行文件的打开
        self.f = open('duanzhi.txt', 'w', encoding='utf-8')

    # 实现对item数据的处理
    def process_item(self, item, spider):
        if item['title'] and item['text']:
            self.f.write(item['title'][0] + '\n')
            self.f.write(item['text'][0] + '\n')
        return item

    # 在爬虫关闭的时候仅执行一次
    def close_spider(self, item):
        print("爬虫结束了")
        self.f.close()
复制<br><br><img alt="image-20230919221631883" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919221631883.png" referrerpolicy="no-referrer"><br><img alt="image-20230919221653448" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919221653448.png" referrerpolicy="no-referrer"><br>没有文件产生别文件settings.py 打开管道<br><img alt="image-20230919223320970" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919223320970.png" referrerpolicy="no-referrer"><br><br><br><img alt="image-20230919224632808" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919224632808.png" referrerpolicy="no-referrer"><br><img alt="image-20230919224652855" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919224652855.png" referrerpolicy="no-referrer"><br>可以得知是post 请求 且数据为json且存储在list下面 <br>我们只需获取以下数据：<br>
prodName: "大白菜"
avgPrice: "0.53"
unitInfo: "斤"
pubDate: "2023-09-19 00:00:00"
<br>实战开始：<br><br>Pythonscrapy startproject xingfadi
cd xingdadi
scrapy genspider xfd xinfadi.com.cn
复制<br><br>设置日志<br>PythonLOG_LEVEL = "WARNING"
复制<br>设置robots<br>PythonROBOTSTXT_OBEY =False
复制<br>打开管道<br>PythonITEM_PIPELINES = {
   "xingfadi.pipelines.XingfadiPipeline": 300,
}
复制<br><br>Pythonimport scrapy
from xingfadi.items import XingfadiItem


class XfdSpider(scrapy.Spider):
    name = "xfd"
    # allowed_domains = ["xinfadi.com.cn"]
    start_urls = ["http://www.xinfadi.com.cn	/getPriceData.html"]

    def parse(self, response):
        item = XingfadiItem()
        html = response.json()
        all_list = html["list"]
        for i in all_list:
            prodName = i["prodName"]
            avgPrice = i["avgPrice"]
            unitInfo = i["unitInfo"]
            pubDate = i["pubDate"]
            item["prodName"] = prodName
            item["avgPrice"] = avgPrice
            item["unitInfo"] = unitInfo
            item["pubDate"] = pubDate
            yield item
复制<br><br>Pythonimport scrapy


class XingfadiItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    prodName = scrapy.Field()
    avgPrice = scrapy.Field()
    unitInfo = scrapy.Field()
    pubDate = scrapy.Field()
复制<br><br>Pythonclass XingfadiPipeline:
    def open_spider(self, spider):
        print("爬虫开始了")
        self.f = open("新发地价格.txt", "w", encoding="utf-8")

    def process_item(self, item, spider):
        if item["prodName"] and item["avgPrice"] and item["unitInfo"] and item["pubDate"]:
            self.f.write(
                item["prodName"] + ":    " + item["avgPrice"] + item["unitInfo"] + "     时间:" + item["pubDate"] + "\n")

        return item

    def close_spider(self, spider):
        print("爬虫结束了")
        self.f.close()
复制<br><br><img alt="image-20230919225532926" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919225532926.png" referrerpolicy="no-referrer"><br><br>
1.创建好项目以后先要配置setting里面的内容
2.无论是post还是get请求都可以直接写
3.以前需要导入的包已经自动导入不需要再次导入
4.items.py 的属性要和 爬虫.py 里面的属性一一对应否则会报错
5.pipelines.pt  一定要if 在进行写入否则不会成功 我也不知道为什么:smile:
乱写:ok_hand:
<br><br><br><br>Mysqlcreate database spider;
use spider;
CREATE TABLE dzx (
    title VARCHAR(255),
    text TEXT
);
复制<br><br>基于第一个案例的dzx来说进行一个把数据存储到数据库中<br>Pythonimport pymysql


class DuanzhiPipeline:
    def open_spider(self, spider):
        # 在爬虫开启的时候仅执行一次
        print("爬虫开始了")
        self.db = pymysql.connect(host='localhost', user='root', password='1234',
                                  port=3306, db='spider', charset='utf8')
        self.cursor = self.db.cursor()

    def process_item(self, item, spider):
        # 实现对item数据的处理
        try:
            title = item['title']
            text = item['text']
            # 构建SQL语句
            sql = "INSERT INTO dzx (title, text) VALUES (%s, %s)"
            # 执行SQL语句，并传递参数
            self.cursor.execute(sql, (title, text))
            # 提交事务
            self.db.commit()
        except Exception as e:
            # 处理数据库出错的情况
            print("数据库出错：" + str(e))
            # 回滚事务
            self.db.rollback()
        return item

    def close_spider(self, spider):
        # 在爬虫关闭的时候仅执行一次
        print("爬虫结束了")
        self.cursor.close()
        self.db.close()
复制<br><br><img alt="image-20230920180314611" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230920180314611.png" referrerpolicy="no-referrer"><br><img alt="image-20230920180339626" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230920180339626.png" referrerpolicy="no-referrer"><br><br>
sql 语句要正确 尤其是对添加内容的时候
<br><br><br>用上面的新发地的案例进行修改<br>Pythonfrom pymongo import MongoClient

class XingfadiPipeline:
    def open_spider(self, spider):
        print("爬虫开始了")
        # 连接数据库
        self.con = MongoClient(host='127.0.0.1', port=27017)
        # 创建库
        self.db = self.con['spider']
        # 创建集合
        self.myset = self.db['xfd']


    def process_item(self, item, spider):
        # 把item转换成字典
        result = self.myset.insert_one(dict(item))
        print(result.inserted_id)
        return item

    def close_spider(self, spider):
        print("爬虫结束了")
        self.con.close()
复制<br><img alt="image-20230920182823849" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230920182823849.png" referrerpolicy="no-referrer"><br>对于MongoDB来说 无需提前进行对数据库的创建 <br>但有必要进行一次测试连接<br>Pythonfrom pymongo import MongoClient

# 连接数据库
client = MongoClient(host='127.0.0.1', port=27017)

# 测试连接
try:
    client.admin.command('ping')
    print("成功连接到MongoDB！")
except Exception as e:
    print("连接失败:", e)

# 关闭连接
client.close()
复制<br><br>对于pycharm来说 刷新看一下或者打开勾选选择数据库<br><img alt="image-20230920183025807" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230920183025807.png" referrerpolicy="no-referrer"><br><br><br>PythonITEM_PIPELINES = {
    "desk.pipelines.DeskPipeline": 300,
    "desk.pipelines.ImgPipeline": 400, # 图片下载
}

IMAGES_STORE = "./img" # 图片存储路径
复制<br>
1.要打开对应的下载图片的管道
2.要设置一个下载图片的存储地址
<br><br>Pythonimport scrapy
from urllib.parse import urljoin


class ImgSpider(scrapy.Spider):
    name = 'img'
    # allowed_domains = ['desk.zol.com.cn/dongman']
    start_urls = ['http://desk.zol.com.cn/dongman/']

    def parse(self, resp, **kwargs):
        # 先抓取到每个图片详情的url
        url_list = resp.xpath('//ul[@class="pic-list2  clearfix"]/li/a/@href').extract()
        # 获取到url列表后 进行循环进行每一个url详情页的请求
        for url in url_list:
            # 因为抓取到的url并不完整，需要进行手动拼接
            # urljoin('https://desk.zol.com.cn/dongman/', '/bizhi/8301_103027_2.html')
            url = urljoin('https://desk.zol.com.cn/dongman/', url)
            # 拼凑完发现当前url中有下载exe的url，将其去除
            if url.find('exe') != -1:
                continue
            yield scrapy.Request(url, callback=self.parse_detail)

    # 对详情页进行解析
    def parse_detail(self, resp):
        # 获取当前详情页中最大尺寸图片的url
        max_img_url = resp.xpath('//dd[@id="tagfbl"]/a/@href').extract()
        # 判断当前最大图片的url地址，为倒数第二个，如果当前图片列表url长度小于2 则当前证明不是图片的url
        if len(max_img_url) &gt; 2:
            max_img_url = urljoin('https://desk.zol.com.cn/', max_img_url[0])
            # 对url页面进行请求 获取最终大图的页面
            yield scrapy.Request(max_img_url, callback=self.parse_img_detail)

    def parse_img_detail(self, resp):
        # 解析出大图的url
        img_src = resp.xpath("//img[1]/@src").extract_first()
        return {'img_src': img_src}
复制<br>img_src是固定的名称 或者可以自定义修改 ，要pipelines.py和img.py 传递的参数要一一对应<br><br>Pythonfrom itemadapter import ItemAdapter
from scrapy.pipelines.images import ImagesPipeline
import scrapy

class DeskPipeline:
    def process_item(self, item, spider):
        return item
class Imgspipline(ImagesPipeline):
    # 1. 发送请求(下载图片, 文件, 视频,xxx)
    def get_media_requests(self, item, info):
      	# 获取到图片的url
        url = item['img_src']
        # 进行请求
        yield scrapy.Request(url=url, meta={"url": url})  # 直接返回一个请求对象即可

    # 2. 图片存储路径
    def file_path(self, request, response=None, info=None, *, item=None):
        # 当前获取请求的url的方式有2种
        # 获取到当前的url 用于处理下载图片的名称
        file_name = item['img_src'].split("/")[-1]  # 用item拿到url
        # file_name = request.meta['url'].split("/")[-1]  # 用meta传参获取
        return file_name

    # 3. 可能需要对item进行更新
    def item_completed(self, results, item, info):
        # print('results', results)
        for r in results:
            # 获取每个图片的路径
            print(r[1]['path'])
        return item  # 一定要return item 把数据传递给下一个管道
复制<br><br>Pythonimport scrapy
from urllib.parse import urljoin


class ImgSpider(scrapy.Spider):
    name = 'img'
    # allowed_domains = ['desk.zol.com.cn/dongman']
    start_urls = ['http://desk.zol.com.cn/dongman/']

    def parse(self, resp, **kwargs):
        # 先抓取到每个图片详情的url
        url_list = resp.xpath('//ul[@class="pic-list2  clearfix"]/li/a/@href').extract()
        # 获取到url列表后 进行循环进行每一个url详情页的请求
        for url in url_list:
            # 因为抓取到的url并不完整，需要进行手动拼接
            # urljoin('https://desk.zol.com.cn/dongman/', '/bizhi/8301_103027_2.html')
            url = urljoin('https://desk.zol.com.cn/dongman/', url)
            # 拼凑完发现当前url中有下载exe的url，将其去除
            if url.find('exe') != -1:
                continue
            yield scrapy.Request(url, callback=self.parse_detail)

    # 对详情页进行解析
    def parse_detail(self, resp):
        # 获取当前详情页中最大尺寸图片的url
        max_img_url = resp.xpath('//dd[@id="tagfbl"]/a/@href').extract()
        # 判断当前最大图片的url地址，为倒数第二个，如果当前图片列表url长度小于2 则当前证明不是图片的url
        if len(max_img_url) &gt; 2:
            max_img_url = urljoin('https://desk.zol.com.cn/', max_img_url[0])
            # 对url页面进行请求 获取最终大图的页面
            yield scrapy.Request(max_img_url, callback=self.parse_img_detail)

    def parse_img_detail(self, resp):
        # 解析出大图的url
        img_src = resp.xpath("//img[1]/@src").extract_first()
        return {'img_src': img_src}
复制<br><br>
1.对应最后的传参一点要对应好相对的元素
2.记得要设置好存储图片的路径以及打开对应的管道
3.报错多问G哥
<br><br><br><br>
<br>直接携带cookies请求页面
<br>找url地址，发送post请求存储cookie
<br><br>
<br>找到对应的input标签，输入文本点击登陆
<br><br>
<br>直接携带cookies
<br>找url地址，发送post请求存储cookie
<br>找到对应的form表单，自动解析input标签，自动解析post请求的url地址，自动带上数据，自动发送请求
<br><br>17k小说网<br>https://user.17k.com/
复制<br><br>
<br>cookie过期时间很长，常见于一些不规范的网站
<br>能在cookie过期之前把搜有的数据拿到
<br>配合其他程序使用，比如其使用selenium把登陆之后的cookie获取到保存到本地，scrapy发送请求之前先读取本地cookie
<br><br>settings.py<br>PythonDEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en',
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36',
  'Cookie': 'ASP.NET_SessionId=n4lwamv5eohaqcorfi3dvzcv; xiaohua_visitfunny=103174; xiaohua_web_userid=120326; xiaohua_web_userkey=r2sYazfFMt/rxUn8LJDmUYetwR2qsFCHIaNt7+Zpsscpp1p6zicW4w=='
}
复制<br>注意：需要打开COOKIES_ENABLED，否则上面设定的cookie将不起作用<br>Python# Disable cookies (enabled by default)
COOKIES_ENABLED = False
复制<br>xiaoshuo.py<br>Pythonimport scrapy


class DengluSpider(scrapy.Spider):
    name = 'denglu'
    allowed_domains = ['17k.com']
    start_urls = ['https://user.17k.com/ck/user/mine/readList?page=1&amp;appKey=2406394919']

    def parse(self, res):
        print(res.text)
复制<br>局限性：<br>当前设定方式虽然可以实现携带cookie保持登录，但是无法获取到新cookie，也就是当前cookie一直是固定的<br>如果cookie是经常性变化，那么当前不适用<br><br>scrapy中start_url是通过start_requests来进行处理的，其实现代码如下<br>Pythondef start_requests(self):
    cls = self.__class__
    if method_is_overridden(cls, Spider, 'make_requests_from_url'):
        warnings.warn(
            "Spider.make_requests_from_url method is deprecated; it "
            "won't be called in future Scrapy releases. Please "
            "override Spider.start_requests method instead (see %s.%s)." % (
                cls.__module__, cls.__name__
            ),
        )
        for url in self.start_urls:
            yield self.make_requests_from_url(url)
    else:
        for url in self.start_urls:
            yield Request(url, dont_filter=True)
复制<br>所以对应的，如果start_url地址中的url是需要登录后才能访问的url地址，则需要重写start_request方法并在其中手动添加上cookie<br>settings.py<br>Pythonimport scrapy


class DengluSpider(scrapy.Spider):
    name = 'denglu'
    # allowed_domains = ['https://user.17k.com/ck/user/mine/readList?page=1']
    start_urls = ['https://user.17k.com/ck/user/mine/readList?page=1&amp;appKey=2406394919']

    def start_requests(self):
        cookies = 'GUID=796e4a09-ba11-4ecb-9cf6-aad19169267d; Hm_lvt_9793f42b498361373512340937deb2a0=1660545196; c_channel=0; c_csc=web; accessToken=avatarUrl%3Dhttps%253A%252F%252Fcdn.static.17k.com%252Fuser%252Favatar%252F18%252F98%252F90%252F96139098.jpg-88x88%253Fv%253D1650527904000%26id%3D96139098%26nickname%3D%25E4%25B9%25A6%25E5%258F%258BqYx51ZhI1%26e%3D1677033668%26s%3D8e116a403df502ab; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2296139098%22%2C%22%24device_id%22%3A%22181d13acb2c3bd-011f19b55b75a8-1c525635-1296000-181d13acb2d5fb%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_referrer_host%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%7D%2C%22first_id%22%3A%22796e4a09-ba11-4ecb-9cf6-aad19169267d%22%7D; Hm_lpvt_9793f42b498361373512340937deb2a0=1661483362'
        cookie_dic = {}
        for i in cookies.split(';'):
            v = i.split('=')
            cookie_dic[v[0]] = v[1]
        # {i.split('=')[0]:i.split('=')[1] for i in cookies_str.split('; ')}   # 简写
        for url in self.start_urls:
            yield scrapy.Request(url, cookies=cookie_dic)


    def parse(self, response):
        print(response.text)
复制<br><br>
<br>scrapy中cookie不能够放在headers中，在构造请求的时候有专门的cookies参数，能够接受字典形式的coookie
<br>在setting中设置ROBOTS协议、USER_AGENT
<br><br>
我们知道可以通过scrapy.Request()指定method、body参数来发送post请求；那么也可以使用scrapy.FormRequest()来发送post请求
<br><br>通过scrapy.FormRequest能够发送post请求，同时需要添加fromdata参数作为请求体，以及callback<br>Pythonlogin_url = 'https://passport.17k.com/ck/user/login'      
yield scrapy.FormRequest(
            url=login_url, 
            formdata={'loginName': '17346570232', 'password': 'xlg17346570232'}, 
            callback=self.do_login
)
复制<br><br><br>
<br>找到post的url地址：点击登录按钮进行抓包，然后定位url地址为<a rel="noopener" class="external-link" href="https://www.xiaohua.com/Handler/Login.ashx" target="_blank">https://www.xiaohua.com/Handler/Login.ashx</a>
<br>找到请求体的规律：分析post请求的请求体，其中包含的参数均在前一次的响应中
<br>否登录成功：通过请求个人主页，观察是否包含用户名
<br><br>Pythonimport scrapy


class DlSpider(scrapy.Spider):
    name = "dl"
    allowed_domains = ["17k.com"]
    start_urls = ["https://user.17k.com/ck/user/myInfo/96139098?appKey=2406394919"]

    # 在请求start_urLs之前先登陆登陆以后就有了登录后的cookie此刻cookie是自动抓取的
    def start_requests(self):
        # 先在要在这里处理登陆的功能因为scrapy一执行自动调用当前方法
        # for url in self.start_urls:
        #     yield scrapy.Request(url=url)
        login_url = 'https://passport.17k.com/ck/user/login'
        # yield scrapy.Request(login_url, method='POST',
        #                      body='loginName=17346570232&amp;password=xlg17346570232',
        #                      callback=self.do_start_requests)

        # FormRequest 是Scrapy发送POST请求的方法
        yield scrapy.FormRequest(login_url,
                                 formdata={
                                     'loginName': '17346570232',
                                     'password': 'xlg17346570232'
                                 },
                                 callback=self.do_start_requests)

    # 登录成功后再去请求start_urls
    def do_start_requests(self, response, **kwargs):
        for url in self.start_urls:
            yield scrapy.Request(url=url)

    def parse(self, response, **kwargs):
        print(response.text)
复制<br><br><br>
通过爬取段子页面的信息,学习如何实现翻页请求
地址：<a rel="noopener" class="external-link" href="https://duanzixing.com/" target="_blank">https://duanzixing.com/</a>
<br><br>
<br>获取首页的数据
<br>寻找下一页的地址，进行翻页，获取数据
<br><br>
<br>
可以在settings中设置ROBOTS协议
Python# False表示忽略网站的robots.txt协议，默认为True
ROBOTSTXT_OBEY = False
复制

<br>
可以在settings中设置User-Agent：
Python# scrapy发送的每一个请求的默认UA都是设置的这个User-Agent
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'
复制

<br><br>Pythonimport scrapy


class DzSpider(scrapy.Spider):
    name = "dz"
    allowed_domains = ["duanzixing.com"]
    start_urls = ["https://duanzixing.com/"]

    def parse(self, response, **kwargs):
        url = 'https://duanzixing.com/page/%d/'
        for i in range(1, 11):
            new_url = url % i
            # print(new_url)
            yield scrapy.Request(new_url, callback=self.parse_page, meta={
                'url': new_url})

    def parse_page(self, response, **kwargs):
        url = response.meta['url']
        article_list = response.xpath('//article[@class="excerpt"]')
        for article in article_list:
            data = {}
            title = article.xpath('./header/h2/a/text()').extract_first()
            con = article.xpath('./p[@class="note"]/text()').extract_first()
            data['title'] = title
            data['con'] = con
            print(data, url)
            yield data
复制<br><br><br>
<br>中括号中的参数为可选参数
<br>callback：表示当前的url的响应交给哪个函数去处理
<br>meta：实现数据在不同的解析函数中传递，meta默认带有部分数据，比如下载延迟，请求深度等
<br>dont_filter:默认为False，会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以把它设置为Ture，比如贴吧的翻页请求，页面的数据总是在变化;start_urls中的地址会被反复请求，否则程序不会启动
<br>method：指定POST或GET请求
<br>headers：接收一个字典，其中不包括cookies
<br>cookies：接收一个字典，专门放置cookies
<br>body：接收一个字典，为POST的数据
<br><br><br><br>在爬虫文件的parse方法中，提取详情页增加之前callback指定的parse_detail函数：<br>Pythondef parse(self,response):
    ...
    yield scrapy.Request(detail_url, callback=self.parse_detail,meta={"item":item})
...

def parse_detail(self,response):
    #获取之前传入的item
    item = resposne.meta["item"]
复制<br><br>
<br>meta参数是一个字典
<br>meta字典中有一个固定的键proxy
]]></description><link>爬虫\Scrapy.html</link><guid isPermaLink="false">爬虫/Scrapy.md</guid><pubDate>Mon, 16 Oct 2023 15:12:48 GMT</pubDate><enclosure url="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919204633965.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20230919204633965.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Mysql]]></title><description><![CDATA[<a class="tag" href="?query=tag:scrapy" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scrapy</a> 
 <br><br><a href=".?query=tag:scrapy" class="tag" target="_blank" rel="noopener">#scrapy</a>
python连接mysql可以用pymysql模块<br>Pythonpip install pymysql
复制<br><br>Pythonimport pymysql

# 链接数据库
conn = pymysql.connect(
    host='localhost',
    port=3306,
    user='root',
    password='test123456',
    database='spider_back'
)
# 创建游标
cursor = conn.cursor()
# 接下来就可以用游标去执行各种操作了
复制<br><br><br>Pythontry:
    cursor = conn.cursor()
    result = cursor.execute("insert into stu(sname, address, gender) values ('李嘉诚', '八宝山', 1)")
    print(cursor.lastrowid)  # 获取自增的ID值
    print(result)  # result是该sql会影响多少条数据
    conn.commit()  # 提交
except:
    conn.rollback()  # 回滚
复制<br><br>Pythontry:
    cursor = conn.cursor()
    result = cursor.execute("update stu set gender = 2 where sid = 12")
    print(result)  # result是该sql会影响多少条数据
    conn.commit()  # 提交
except:
    conn.rollback()  # 回滚
复制<br><br>Pythontry:
    cursor = conn.cursor()
    result = cursor.execute("delete from stu where sid = 12")
    print(result)  # result是该sql会影响多少条数据
    conn.commit()  # 提交
except:
    conn.rollback()  # 回滚
复制<br><br>Python# 查询
from pymysql.cursors import DictCursor
# cursor = conn.cursor(DictCursor)  # 使用字典游标. 查询出的结果自动保存在字典中
cursor = conn.cursor()  # 默认游标. 查询出的结果自动保存在元组中

sql = """
    select * from stu
"""
ret_num = cursor.execute(sql)
# result = cursor.fetchall()  # 获取全部结果
# result = cursor.fetchmany(5)  # 获取部分结果
result = cursor.fetchone()  # 获取单个结果
print(result)
result = cursor.fetchone()  # 获取单个结果, 可以连续获取
print(result)
复制<br>注意, 一个游标如果被拿空了. 则不能再次获取内容. ]]></description><link>爬虫\spider-mysql.html</link><guid isPermaLink="false">爬虫/spider-mysql.md</guid><pubDate>Mon, 16 Oct 2023 14:19:44 GMT</pubDate></item><item><title><![CDATA[2023-10-14]]></title><description><![CDATA[ 
 <br><br>
<br>😆 
<br>😭
<br>🥰
<br>当前温度：18℃  天气：阴 温度范围：14 ~ 18℃
湿度：65% 风向：东北风 1级 紫外线：无
空气质量：优 PM: 12 日出: 06:51 日落: 18:27<br>一言
 这个世界很大，告别的方式有多少种，人生的正确答案就有多少个。  —— 《南唐闲人》 · 佚名
<br><br><br><br><br>&lt;&lt; <a data-href="2023-10-13" href="\2023-10-13" class="internal-link" target="_self" rel="noopener">2023-10-13</a> | <a data-href="2023-10-15" href="\日记\2023-10-15.html" class="internal-link" target="_self" rel="noopener">2023-10-15</a> &gt;&gt;]]></description><link>日记\2023-10-14.html</link><guid isPermaLink="false">日记/2023-10-14.md</guid><pubDate>Sat, 14 Oct 2023 12:18:40 GMT</pubDate></item><item><title><![CDATA[2023-10-15]]></title><description><![CDATA[<a class="tag" href="?query=tag:scala" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#scala</a> <a class="tag" href="?query=tag:hive" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hive</a> <a class="tag" href="?query=tag:数据库" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#数据库</a> 
 <br><br>
<br>😆 
<br>😭
<br>🥰
<br>当前温度：21℃  天气：晴 温度范围：13 ~ 21℃
湿度：49% 风向：西南风 1级 紫外线：强
空气质量：良 PM: 14 日出: 06:52 日落: 18:26<br>一言
 被酒莫惊春睡重，赌书消得泼茶香，当时只道是寻常。  —— 《浣溪沙·谁念西风独自凉》 · 纳兰性德
<br><br><br>早上： 练车车
下午：
学习 scala    {先学了基础的语法09，后面的有时间慢慢看}
学习hive<br><br>记录时间: 2023/10/15/2:42 PM 当前天气: 晴 🔆 • 当前温度: 25°C • 体感温度: 24°C
太™无聊了，一练车就™大太阳，不去就下雨<br><a href=".?query=tag:scala" class="tag" target="_blank" rel="noopener">#scala</a>
记录时间: 2023/10/15/3:16 PM 当前天气: 晴 🔆 • 当前温度: 25°C • 体感温度: 24°C<br><br>重点
定义的变量必须赋值
例如 ： val  name = "Scala"
在scala中所有的属性默认都是私有的
private不需要自己填写.当我们手动填写private后,字段和get,set方法都变成了私有的.
在scala中get,set方法都是自动生成的.
如果默认值是null,必须显示的指定类型
var name1:String = null
<br>Scalapackage scala03

//单例对象/实例对象/伴生对象
object Class05 extends App {
  val p = new Person()
  //age是常量
//  p.age = 10
  p.name = "chen"  //默认调用的set方法
  //height对应的set方法是私有的,所以不能直接调用
  //p.height = 10
}

//伴生类
class  Class05{

}

class Person{
  //
//  private String name;
  //在scala中所有的属性默认都是私有的,private不需要自己填写.当我们手动填写private后,字段和get,set方法都变成了私有的.
  //在scala中get,set方法都是自动生成的.
  //直接给name属性赋值bing,如果不指定类型,默认进行类型推断
  var name = "bing"
  //如果默认值是null,必须显示的指定类型
  var name1:String = null
  //_表示占位符,代表对应的值,必须显示的指定类型
  var name2:String = _
  /*
  区别var/val
  1.var的变量会自动生成getter,setter方法
  2.val的变量只能生成getter
  3.val类型的字段不能使用占位符_
   */
  val age = 10
  //val age1:Int = _
  //当我们手动填写private后,字段和get,set方法都变成了私有的.但是在半生对象中可以使用
  private var height = 10

  //字段是私有的,并且Person类的方法只能访问当前对象的字段
  private [this] var weight = 100

  def testPrivate(p:Person)={
    //p.weight = 10
    this.weight = 10
  }
}

object Person{
  val p1 = new Person
  p1.height = 10
}

复制<br>Scalapackage scala03

import scala.beans.BeanProperty

/*
//重要提示：自己手动创建变量的getter和setter方法需要遵循以下原则：
//0) 将成员变量定义为私有
//1) 字段属性名以“_”作为前缀，如定义：_x
//2) getter方法定义为：def x = _x
//3) setter方法定义时，方法名为属性名去掉前缀，并加上后缀，后缀是：“x_=”，如例子所示
 */
class  Point{
  //需要使用scala自带的get,set
  private var _x = 0
  private var _y = 0
  //由@BeanProperty自动生成了四个方法：java的get,set和scala的get,set
  @BeanProperty var name = "bing"

  def x = _x
  def x_= (newValue:Int):Unit={ _x = newValue}
  def y = _y
  def y_= (newVale:Int)={_y = newVale}
}
object Class06 extends App {
//  val p = new Point
//  p.x = 10
//  println(p.x)
//  p.name = "chen"
//  println(p.getName)
}

object Point{
  def main(args: Array[String]): Unit = {
    val p = new Point
    p.x = 10
    println(p.x)
    p.name = "chen"
    println(p.getName)
  }
}

复制<br>Scalapackage scala03

object Class07  {
  def main(args: Array[String]): Unit = {
    val d = new Dog("labuladuo",10)
    d.name = "bing"
    //age被private修饰,所以访问权限只有当前的类或者伴生对象
    //d.age = 10
    val d1 = new Dog1("金毛",20)
    //如果主构造器中的属性没有使用var,val修饰,就不会自动保存成字段,当对象创建完,直接消失.
    //如果在类中任何一个方法中调用过一次,这个属性也会自动变成字段,不过这个字段是私有的
    //相当于 private [this] var name
//    d1.name = "haha"
    d1.test(3)
   // d1 = "ha"

    //通过辅助构造器创建对象
    val d2 = new Dog2("lala",20,20)
    println(d2.name)
    println(d2.age)
  }
}

//主构造器  放在类名的后面
/*
特点:
1.会执行类里面的所有语句
2.如果主构造器中的属性没有使用var,val修饰,就不会自动保存成字段,当对象创建完,直接消失.
   //如果在类中任何一个方法中调用过一次,这个属性也会自动变成字段,不过这个字段是私有的
    //相当于 private [this] var name
3.//private var name: String
//私有字段，私有的getter和setter方法
//var name: String
//私有字段，公有的getter和setter方法
//private val name: String
//私有字段，私有的getter方法
//val name: String
//私有字段，公有的getter方法

当我们将主构造器私有化后,不能再通过主构造器创建对象,只能借助于辅助构造器继续创建对象
//私有的主构造器,不能被外界直接调用.相当于添加了对应的属性
 */
class Dog(var name:String,private var age:Int){
//  var name:String = _
  //  var age = 10

  println("haha")
}

class Dog1(name:String,age:Int){
  def test(a:Int)={
    val str = name
  }
}

//私有的主构造器,不能被外界直接调用.相当于添加了两个属性 :name和age
class Dog2 private (var name:String,var age:Int){
  self=&gt;  //self 等价与  this ,但是必须放在类的第一行,不能替代this作为方法
  //  var name:String = _
  //  var age = 10
  var color = "蓝色"

  /*
  辅助构造器
  当字段已经在主构造器中定义完成,辅助构造器中不能加var
  辅助构造器的声明不能和主构造器的声明一致,会发生错误(即构造器名重复)
  辅助构造器的第一行必须是以主构造器或者另一个辅助构造器的调用开始
   */
  def this(name:String,age:Int,weight:Int){
    this(name,age)
  }

  def this(height:Int,weight:Int,name:String,age:Int){
    this(name,age)
  }

  def test={
    this.color = "绿色"
    self.color = "红色"
  }

  println("haha")
}

复制<br><br>Scala 中所有的类都继承自一个父类。像前一节的&nbsp;Complex&nbsp;这种没有指定的例子，Scala 会默认使用&nbsp;scala.AnyRef。<br>Scala 中可以重写继承自父类的函数。但是为了避免意外重写，必须加上&nbsp;override&nbsp;修饰字来明确表示要重写函数。我们以重写&nbsp;Complex&nbsp;类中来自&nbsp;Object&nbsp;的&nbsp;toString&nbsp;作为示例。<br>Scalaclass Complex(real: Double, imaginary: Double) { 
def re = real 
def im = imaginary 
override def toString() = "" + re + (if (im &lt; 0) "" else "+") + im + "i" }
复制<br><a href=".?query=tag:hive" class="tag" target="_blank" rel="noopener">#hive</a> <a href=".?query=tag:数据库" class="tag" target="_blank" rel="noopener">#数据库</a> <br><br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310151704151.png" referrerpolicy="no-referrer"><br><br>开始的时候要先启动 在hive文件内<br>Linuxnohup bin/hive --service hiveserver2 &gt;&gt; logs/hiveserver2.log 2&gt;&amp;1 &amp;
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310151738929.png" referrerpolicy="no-referrer">
<img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310151737844.png" referrerpolicy="no-referrer"><br>Hive数据库的基本操作和MYSQL差不多<br>内部表和外部表在创建时的差别<br>
<br>内部表
<br>SqlCRAATE TABLE T_INNER(ID INT);
复制<br>
<br>外部表
<br>SqlCREATE EXTERNAL TABLE T_OUTER(ID INT) LOCATION 'HDFS:///AA/BB/XX';
复制<br>Hive表创建时要做的两件事：
1.在HDFS下创建表目录
2.在元数据库MySq|创建相应表的描述数据（元数据）<br>drop时有不同的特性：
1.drop时，元数据都会被清除
2.drop时，内部表的表目录会被删除，但是外部表的表目录不会被删除。<br>使用场景<br>1.内部表：平时用来测试或者少量数据，并且自己可以随时修改删除数据.<br>2.外部表：使用后数据不想被删除的情况使用外部表（推荐使用）所以，整个数据仓库的最底层的表使用外部<br><br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310151749621.png" referrerpolicy="no-referrer"><br><br>&lt;&lt; <a data-href="2023-10-14" href="\日记\2023-10-14.html" class="internal-link" target="_self" rel="noopener">2023-10-14</a> | <a data-href="2023-10-16" href="\日记\2023-10-16.html" class="internal-link" target="_self" rel="noopener">2023-10-16</a> &gt;&gt;]]></description><link>日记\2023-10-15.html</link><guid isPermaLink="false">日记/2023-10-15.md</guid><pubDate>Mon, 16 Oct 2023 15:09:35 GMT</pubDate><enclosure url="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310151704151.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310151704151.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2023-10-16]]></title><description><![CDATA[<a class="tag" href="?query=tag:hive" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hive</a> <a class="tag" href="?query=tag:数据库" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#数据库</a> <a class="tag" href="?query=tag:sql" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#sql</a> 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：21℃  天气：晴 温度范围：11 ~ 23℃
湿度：47% 风向：东南风 2级 紫外线：强
空气质量：优 PM: 7 日出: 06:52 日落: 18:25<br>一言
 回首池塘青欲遍，绝似梦中芳草。  —— 《南浦·春水》 · 张炎
<br><br><br>头昏难顶
下午学习 hive基础
晚上学习 分桶表<br><br>记录时间: 2023/10/16/1:24 PM 当前天气: 晴 🔆 • 当前温度: 20°C • 体感温度: 17°C
<a href=".?query=tag:hive" class="tag" target="_blank" rel="noopener">#hive</a> <br><br>Hive就是对某个具体的路径进行统一的管理<br>Hive加载一般分为两种：<br>
<br>一种是从本地Linux上加载到Hive中
(注意：如果使用的客户端beeline或者ldea操作，必须保证数据路径与Hive服务器hiveserver2在同一台主机）
<br>另外一种是从HDFS加载到Hive中
<br>上传方式：<br>
<br>本地上传
<br>Sqlload data local inpath '/路径/文件名' into table 数据库.表名
复制<br>
<br>hdfs 
<br>Linuxhdfs dfs - put '文件名' /user/hive/warehouse/test
复制<br>加载数据的本质：
1.如果数据在本地，加载数据的本质就是将数据copy到HDFS上的表目录下。
2.如果数据在HDFS上，加载数据的本质是将数据移动到HDFS的表目录下。<br>重点
注意上传数据要保证数据格式是正确的否则会NULL
一定要检查数据的 分隔符
注意：Hive使用的是严格的读时模式：加载数据时不检查数据的完整性，读时发现数据不对则使用NULL来代替。
而MySq|使用的是写时模式：在写入数据时就进行检查
<br><br><a href=".?query=tag:数据库" class="tag" target="_blank" rel="noopener">#数据库</a> <br><br>先创建一个和旧表结构一样的表：
例如 test1(old) 和 test2(new)
开始：<br>Sqlinsert into test2
select * from  test1 (在这里可以添加条件)
复制<br>一定要记住数据都存在hdfs系统里面<br><br>从test1 克隆到 test2 <br>Sqlcreate table if not exists test1 like test2;
复制<br><br>如果当前数据库是test那么在下面warehouse后要加上当前数据库的名字 test.db
与原来的数据库使用相同的原来的数据保存路径<br>Sqlcreate table if not exists test2 like test1 location '/user/hive/warehouse/test';
复制<br>注意location后面接的一定是HDFS中的目录，不是文件<br><br>用as是更灵活的方式，跟创建表的方式一样，元数据和目录都会创建新的。<br>Sqlcreate table test2
as 
select * from test1;

--查看库描述：
 desc/describe database [extended] class11;
 
--查看表描述：
 desc/describe [extended] test2;
 
--查看表结构信息
 desc formatted test2;
 
--查看表创建语句，比较全
show create table test2;
复制<br><br>启动：<br>Sqlnohup bin/hive --service metastore &gt;&gt; logs/metastore.log 2&gt;&amp;1 &amp;
nohup bin/hive --service hiveserver2 &gt;&gt; logs/hiveserver2.log 2&gt;&amp;1 &amp;

复制<br>文件：
 log1.txt
创建数据库：<br>Sqlcreat database db1;
复制<br>创建表：<br>SqlCREATE TABLE logtest(
id string COMMENT'this is id column',
phonenumber bigint,
mac string,
ip string,
url string,
status1 string,
status2 string,
upflow int,
downflow int ,
status3 string,
dt string
)
COMMENT 'this is log table'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '
LINES TERMINATED BY '\n'
stored as textfile;
复制<br>解释：

<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ': 这一行指定了表的行格式，它告诉数据库如何解析文本文件中的每一行。在这里，FIELDS TERMINATED BY ' '表示每一行的字段是由空格字符分隔的。
<br>LINES TERMINATED BY '\n': 这一行指定了文本文件中行的终止符。在这里，'\n'表示行以换行符（newline）结束。
<br>stored as textfile;: 这一行指定了数据存储格式。在这里，数据将以文本文件的形式存储。这意味着表中的数据将被存储在文本文件中，每一行都包含表的一个记录，并且字段之间由空格分隔。

<br>导入数据：<br>Sql--本地上传
load dara local inpath '/export/test/log1.txt' into table db1.logtest;
复制<br><br><br>Sqlselect 1.phonenumber,
round(sum(1.upflow + 1.downflow) / 1024.0,2)as total
from logtest 1
group by 1 . phonenumber ;
复制<br><br>Sqlselect 1 . url url ,
count(1.url)as urlcount
from log1 1
group by 1.url
order by urlcount desc
limit 3;
复制<br><br><a href=".?query=tag:sql" class="tag" target="_blank" rel="noopener">#sql</a> <br><br>rename to<br>Sql alter table t7 rename to al;
复制<br><br>change column<br>Sqlalter table al change column name namel string;
复制<br><br>change column<br>Sqlalter table log1 change column ip ip string after status1;
alter table log1 change column mac mac string first;
复制<br><br>change column<br>Sqlalter table al change column namel name string;
复制<br><br>add columns<br>Sqlalter table al add columns (sex int);
复制<br><br>replace columns<br>Sqlalter table al replace columns(
id int,
name string,
size int,
pic string
ou
);
复制<br><br>Sqlalter table al set tblproperties('EXTERNAL'='TRUE');##内部表转外部表，true一定要大写；
alter table al set tblproperties('EXTERNAL'='false');##false大小写都没有关系
复制<br><br>随着系统运行时间的增加，表的数据量会越来越大，而Hive查询数据的数据的时候通常使用的是「全表扫描」，这样将会导致大量不必要的数据进行扫描，从而查询效率会大大的降低。 从而Hive引进了分区技术，使用分区技术，「避免Hive全表扫描，提升查询效率」
分区是:
将整个表的数据在存储时划分成多个子目录来存储(子目录就是以分区名来名称)
规则：
根据业务来，通常就是使用 年，月，日，地区等<br><br>Sqlpartitioned by(分区名 数据类型)
复制<br>Hive的分区种类:<br>重点

<br>静态分区：「加载数据」的时候指定分区的值
<br>动态分区：数据未知,根据分区的值确定建立分区
<br>混合分区：静态+动态

<br><br>Sqlcreate table if not exists part1
 (
     uid int,
     uname string,
     uage int
 )
 partitioned by(country string) //用于分区的字段，并不会出现在表中
 row format delimited fields terminated by '\t';
复制<br>Sql //在加载的过程中，指定加载到china分区，这是静态分区的特点
 load data local inpath '/root/user_china.txt' into table part1 partition(country='china');
 load data local inpath '/root/user_japan.txt' into table part1 partition(country='japan');
复制<br>分区查询:<br>Sql select * from 表名 where 分区变量名=实际分区的名字;
复制<br>我们创建的分区会在对应的HDFS上生成一个目录，目录名=实际分区名<br><br>Sqlcreate table if not exists part2
 (
     uid int,
     uname string,
     uage int
 )
 partitioned by(year string, month string)
 row format delimited fields terminated by '\t';
 load data local inpath '/root/2017-08.txt' into table part2 partition(year='2017', month='08');
 select * from part2 where year='2017';
 select * from part2 where month='06';
 select * from part2 where year='2018' and month='10';
复制<br>显示分区<br>Sql show partitions 表名;
复制<br>增加单个分区（增加同级分区）<br>Sql alter table 表名 add partition(分区字段名称=实际的分区值)
 alter table part1 add partition(country='UK');
复制<br>增加多个分区<br>Sql alter table 表名 add partition(分区字段名称=实际的分区值),partition(分区字段名称=实际的分区值)
 alter table part1 add partition(country='UK') partition(country='UN');
复制<br>增加分区的同时添加数据<br>Sql alter table 表名 add partition(分区字段名称=实际分区的值) location 'HDFS路径'
 ps:location后面需要是一个存储数据的路径文件夹而不是到文件 
 alter table part1 add partition(country='MS') location '/user/hive/warehouse/bd1814.db/part1/country=china';
复制<br>删除单个分区<br>Sql alter table 表名 drop partition(分区字段名=实际分区名);
 alter table part1 drop partition(country='UK');
复制<br>删除多分区<br>Sql alter table 表名 drop partition(分区字段名=实际分区名),partition(分区字段名=实际分区名)
复制<br>以上是静态分区
系统推荐静态分区<br><br>以下是动态分区<br>hive-site.xml关键配置<br>Xml&lt;property&gt;
     &lt;name&gt;hive.exec.dynamic.partition&lt;/name&gt;
     &lt;value&gt;true&lt;/value&gt;
     &lt;description&gt;Whether or not to allow dynamic partitions in DML/DDL.&lt;/description&gt;
 &lt;/property&gt;
 ​
  &lt;property&gt;
     &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt;
     &lt;value&gt;strict&lt;/value&gt;
     &lt;description&gt;
       In strict mode, the user must specify at least one static partition
       in case the user accidentally overwrites all partitions.
       In nonstrict mode all partitions are allowed to be dynamic.
     &lt;/description&gt;
 &lt;/property&gt;
 &lt;property&gt;
     &lt;name&gt;hive.exec.max.dynamic.partitions&lt;/name&gt;
     &lt;value&gt;1000&lt;/value&gt;
     &lt;description&gt;Maximum number of dynamic partitions allowed to be created in total.&lt;/description&gt;
 &lt;/property&gt;
 &lt;property&gt;
     &lt;name&gt;hive.exec.max.dynamic.partitions.pernode&lt;/name&gt;
     &lt;value&gt;100&lt;/value&gt;
     &lt;description&gt;Maximum number of dynamic partitions allowed to be created in each mapper/reducer node.&lt;/description&gt;
 &lt;/property&gt;
复制<br>可以看出，Hive的动态分区默认是开启的，但是还有一个hive.exec.dynamic.partition.mode 属性来控制动态分区的类型，如果为 strict 则要求在创建动态分区之前必须先创建一个静态分区，这个其实就是后面我们所说的混合分区，所以，如果你想要创建动态分区，就设置为 nostrict
动态分区属性设置 <br>Sql动态分区的属性：
hive.exec.dynamic.partition = true
hive.exec.dynamic.partition.mode=nonstrict
hive.exec.max.dynamic.partitions=1000
hive.exec.max.dynamic.partitions.pernode = 100
复制<br>建表：<br>Sqlcreate table dy_part1(
id int ,
name string
)
partitioned by (dt string)
row format delimited fields terminated by ',';
复制<br>动态分区加载数据不能使用load方式加载
动态分区使用insert into的方式加载数据<br>先建立临时表：<br>Sqlcreate table temp_part(
id int ,
name string,
dt string
)
row format delimited fields terminated by ',';
复制<br>导入数据到临时表（temp_part）:<br>Sqlload data local inpath '/export/test/student.txt' into table temp_part;
复制<br>导入数据到动态表<br>Sqlinsert into dy_part1 partition(dt)
select id, name, dt from temp_part;
复制<br>设置Hive任务执行选择本地模式(local mode)。当Hive的输入数据量非常小的时候，分布式计算的时间消耗可能会比实际作业的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间会明显被缩短。<br>Xml&lt;property&gt;
     &lt;name&gt;hive.exec.mode.local.auto&lt;/name&gt;
     &lt;value&gt;false&lt;/value&gt;
     &lt;description&gt;Let Hive determine whether to run in local mode automatically&lt;/description&gt;
 &lt;/property&gt;
复制<br>动态分区和静态分区的区别<br>
<br>加载数据的方式：静态分区可以通过load命令，向不同的分区加载数据，加载数据时要指定分区的值；静态分区只能通过select加载数据，并且不需要指定分区的名字，而是根据伪列的值，动态的确定分区值
<br>确定分区值的方式：两者在创建表的时候命令完全一致，只是在确定分区值的时候不同，静态分区需要手动指定分区值，而动态分区会自动识别伪列的属性，动态生成分区值
<br><br>&lt;&lt; <a data-href="2023-10-15" href="\日记\2023-10-15.html" class="internal-link" target="_self" rel="noopener">2023-10-15</a> | <a data-href="2023-10-17" href="\日记\2023-10-17.html" class="internal-link" target="_self" rel="noopener">2023-10-17</a> &gt;&gt;]]></description><link>日记\2023-10-16.html</link><guid isPermaLink="false">日记/2023-10-16.md</guid><pubDate>Tue, 17 Oct 2023 12:45:41 GMT</pubDate></item><item><title><![CDATA[2023-10-17]]></title><description><![CDATA[<a class="tag" href="?query=tag:hive" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hive</a> <a class="tag" href="?query=tag:分桶表" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#分桶表</a> <a class="tag" href="?query=tag:hive" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hive</a> <a class="tag" href="?query=tag:分桶表" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#分桶表</a> <a class="tag" href="?query=tag:sql" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#sql</a> <a class="tag" href="?query=tag:hive" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hive</a> <a class="tag" href="?query=tag:sql" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#sql</a> 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：24℃  天气：晴 温度范围：12 ~ 25℃
湿度：46% 风向：南风 3级 紫外线：无
空气质量：优 PM: 9 日出: 06:53 日落: 18:24<br>一言
 洪荒万年，独对穹苍灭绝，谓之大寂寞可也。  —— 《目送》 · 龙应台
<br><br><br>早上 ： 学分区
下午 ： 练车
晚上 ： 学分桶<br><br>记录时间: 2023/10/17/9:56 AM 当前天气: 晴 🔆 • 当前温度: 23°C • 体感温度: 22°C
<a href=".?query=tag:hive" class="tag" target="_blank" rel="noopener">#hive</a> <br><br>创建一个混合表<br>Sqlcreate table dy_part2(
id int,
name string
)
partitioned by (year string,month string,day string)
row format delimited fields terminated by ',' ;
复制<br>创建临时表<br>Sqlcreate table temp_part2(
id int,
name string,
year string,
month string,
day string
)
row format delimited fields terminated by ',' ;

-- 加载数据
load data loacl inpath '/export/test/student.txt' into table temp_part2;
复制<br>导数数据到分区表<br>Sql错误用法：
insert into dy_part2 partition (year='2018', month, day)
select * from temp_part2;
正确用法：
insert into dy_part2 partition (year='2018', month, day)
select id, name, month, day from tEmp_part2;
复制<br>因为指定了year深渊不能用*<br>注意事项
1.Hive的分区使用的是表外字段，分区字段是一个伪列，但是分区字段是可以做查询过滤。
2.分区字段不建议使用中文
3.一般不建议使用动态分区，因为动态分区会使用MapReduce来进行查询数据，如果分区数据过多，导致
NameNode和ResourceManager的性能瓶颈。所以建议在使用动态分区前尽可能预知分区数量。
4.分区属性的修改都可以使用修改元数据和HDFS数据内容。
Hive分区和MySql分区的区别
<br><br>有一个用户u.txt<br>01#bingbing#23
02#liying#20
03#mali#24
04#xiaoli#18
05#wangli#20
06#mumu#23<br>要求：
1.将数据存入数据仓库users.myuser(id,name,age),需要将数据放在hdfs的/data下面
H
2.查找年龄最大的三个人，年龄相同再按照姓名排序<br>建表<br>Sqlcreate external table if not exists myuser (
     id string,
     name string,
     age int
     )
     row format delimited fields terminated by ',' 
     location '/data/myuser';

load data local inpath '/export/test/u.txt' into table myuser ;
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310171037725.png" referrerpolicy="no-referrer"><br>Sqlselect * from myuser order by age desc,name desc limit 3;
复制<br>记录时间: 2023/10/17/11:07 AM 当前天气: 晴 🔆 • 当前温度: 24°C • 体感温度: 24°C<br><br><a href=".?query=tag:分桶表" class="tag" target="_blank" rel="noopener">#分桶表</a> <a href=".?query=tag:hive" class="tag" target="_blank" rel="noopener">#hive</a>
<img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310162023083.png" referrerpolicy="no-referrer"><br>当单个的分区或者表的数据量过大，分区不能更细粒度的划分数据，就需要使用分桶技术将数据划分成更细的粒度<br><br>Sql[CLUSTERED BY (COLUMNNAME COLUMNTYPE [COMMENT 'COLUMN COMMENT'],...) 
[SORTED BY (COLUMNNAME [ASC|DESC])...] INTO NUM_BUCKETS BUCKETS]
复制<br><br>bucket
分桶的原理:跟MR中的HashPartitioner原理一样:都是key的hash值取模reduce的数量 <br>
<br>MR中：按照key的hash值除以reductTask取余 
<br>Hive中：按照分桶字段的hash值去模除以分桶的个数
<br><br>Note
为了保存分桶查询的分桶结构（数据按照分桶字段进行保存hash散列）
分桶表的数据进行抽样和JOIN时可以提高查询效率,一般是用来抽样查询
<br><br>分桶表创建流程<br>重点

<br>首先要创建带分桶定义的表（分桶表） 
<br>然后创建一个临时表（普通表） 
<br>从临时表中使用分桶查询将查询到的数据插入到分桶表中

<br><br>Sqlcreate table if not exists buc1( 
id int, 
name string, 
age int 
) 
clustered by (id) into 4 buckets
row format delimited fields terminated by ','; 

数据：从(/export/test/buc1.txt加载如下格式数据) 
id,name,age
复制<br><br>Sqlcreate table if not exists temp_buc1( 
id int, 
name string, 
age int 
)
row format delimited fields terminated by ',';
复制<br><br>Sqlload data local inpath '/export/test/buc1.csv' into table buc1;
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310171101353.png" referrerpolicy="no-referrer"><br>利用取余进行了分桶<br><br>Sqlload data local inpath '/export/test/buc1.csv' into table temp_buc1;
复制<br><br>Sqlinsert overwrite table buc1 
select id,name,age from temp_buc1 
cluster by (id);
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310171106886.png" referrerpolicy="no-referrer"><br>这里还是无法分桶,需要进行下面的强制设置<br><br>Sql&lt;!-- 如果要分桶,就要打开分桶的强制模式 --&gt; 
set hive.enforce.bucketing=false/true &lt;name&gt;hive.enforce.bucketing&lt;/name&gt;
&lt;value&gt;false&lt;/value&gt;
&lt;description&gt;
Whether bucketing is enforced. If true, while inserting into the table, bucketing is enforced.
&lt;/description&gt; 

最好把强制排序也打开 
set hive.enforce.sorting=true;
复制<br><br>！！！！！！！！！！！！！！！！<br>Sqlset mapreduce.job.reduces=-1 #-1表示可以根据实际需要来定制reduces的数量
复制<br>分桶的个数就是reduces的个数
cluster by (id);
这个根据id分桶和排序<br>创建指定排序字段的分桶表<br>Sqlcreate table if not exists buc3(
id int, 
name string, 
age int 
) 
clustered by (id) 
sorted by (id desc) into 4 buckets
row format delimited fields terminated by ',';
复制<br>导入数 据<br>Sqlinsert overwrite table buc3 
select id,name,age from temp_buc1 
distribute by (id) sort by (name asc); 
和下面的语句效果一样 
insert overwrite table buc3 
select id,name,age from temp_buc1 
cluster by (id);
注意:定义和导数据时同时设置了分桶和排序,以导入数据时优先
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310171123087.png" referrerpolicy="no-referrer"><br><br>Sql---默认
select * from buc3;
--查第四桶
select * from buc3 tablesample(bucket 1 out of 4 on id);
复制<br>Sql--语法解释: 
tablesample(bucket x out of y on id) 
x:代表从第几桶开始查询 
y:可以是总桶数,总桶数的倍数或者是因子。x不能大于y。
y如果是4 ,已经有的总桶数是4 那么要取的数据 4/4= 1个桶 
y如果是2 ,已经有的总桶数是4,那么要取的数据4/y 4/2=2个桶 
on后面跟的是分桶的字段 

--hive根据y的大小，决定抽样的比例。
   例如，table总共分了64份，当y=32时，抽取(64/32=)2个bucket的数 据，当y=128时，抽取(64/128=)1/2个bucket的数据。
   x表示从哪个bucket开始抽取。例如，table总bucket数为32，tablesample(bucket 3 out of 16)，表示总 共抽取（32/16=）2个bucket的--数据，分别为第3个bucket和第（3+16=）第19个bucket的数据。
案例分析: 
--一共4桶,取得第一桶的数据 
select * from buc3 tablesample(bucket 1 out of 4 on id); 
取得第二桶的数据 
--y如果是2 ,已经有的总桶数是4,那么要取的数据4/y 4/2=2个桶 
select * from buc3 tablesample(bucket 2 out of 2 on id);
--从x开始取,取的数量是 (4/2)=2,当x==1时代表取的是1,3桶 
select * from buc3 tablesample(bucket 1 out of 2 on id); 
-- 4/y=1/2,代表每一桶被分成2份,当x=1和5时表示第一桶,x=1表示取的是第一桶的前半部分. 
--小技巧:区分id是那一部分的方法:id/y值,例如当前的案例:id=8,y=8,id/y=0,因为商+1=1=x,桶中的部分就是 前半部分,比如:id=12,y=8,id/y=4,商+1=5=x,所以是桶的后半部分 
select * from buc3 tablesample(bucket 1 out of 8 on id);

复制<br><br>Sql1.定义
  clustered by (id) ---指定分桶的字段 
  sorted by (id asc|desc) ---指定数据的排序规则，表示咱们预期的数据是以这种规则进行的排序 
2.导入数据*
  cluster by (id) ---指定getPartition以哪个字段来进行hash，并且排序字段也是指定的字段，排序是 以asc排列 
  distribute by (id) ---- 指定getPartition以哪个字段来进行hash
  sort by (name asc | desc) ---指定排序字段 
  order by --- 全域排序(只能有一个reduce)
  区别：cluster by 这种方式可以分别指定getPartition和sort的字段 
  导数据时： insert overwrite table buc3 
  select id,name,age from temp_buc1 
  distribute by (id) sort by (id asc); 
  和下面的语句效果一样 
  insert overwrite table buc4 
  select id,name,age from temp_buc1 
  cluster by (id) ;
复制<br>注意

<br>分区使用的是表外字段，分桶使用的是表内字段 
<br>分桶更加用于细粒度的管理数据，更多的是使用来做抽样、join

<br><br>记录时间: 2023/10/17/7:28 PM 当前天气: 晴 🔆 • 当前温度: 17°C • 体感温度: 17°C
还是学习分桶
<a href=".?query=tag:分桶表" class="tag" target="_blank" rel="noopener">#分桶表</a>
启动启动启动！！！<br>Linuxstart-all.sh
nohup bin/hive --service metastore &gt;&gt; logs/metastore.log 2&gt;&amp;1 &amp;
nohup bin/hive --service hiveserver2 &gt;&gt; logs/hiveserver2.log 2&gt;&amp;1 &amp;
mapred --daemon start historyserver
复制<br><br>熟悉Select各种查询
需求：按照性别分区（1男2女），在分区中按照id的奇偶进行分桶：
id,name,sex
分两桶
建立表<br>Sqlcreate table if not exists stu(
     id  int ,
     name string
     )
     partitioned by (sex string)
     clustered by (id) into 2 buckets
     row format delimited fields terminated by ',';

复制<br>查看建表情况内容<br>Sqldesc formatted 表名；
复制<br>创建临时表<br>Sql create table temp_stu(
     id  int ,
     name string,
     sex string
     )
          row format delimited fields terminated by ',';
复制<br> 添加数据<br>Sqlload data local inpath '/export/test/user.txt' into table temp_stu; 
复制<br>关闭严格模式<br>Sqlhive&gt; set hive.exec.dynamic.partition.mode=nonstrict; 
复制<br> 导入数据<br>Sql insert overwrite table stu partition(sex) 
    select * from temp_stu cluster by id;
复制<br>先分区后分桶
sex为区 id为桶
<img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310172031517.png" referrerpolicy="no-referrer"><br>查询性别为女，学号为奇数的学生<br>Sqlselect * from stu tablesample(bucket 2 out of 2 on id) where sex = '2';
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310172037186.png" referrerpolicy="no-referrer"><br>Sqlselect * from stu where id % 2=1 and sex = '2';

复制<br><img src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310172037024.png" referrerpolicy="no-referrer">
记录时间: 2023/10/17/8:38 PM 当前天气: 晴 🔆 • 当前温度: 16°C • 体感温度: 16°C<br><br><a href=".?query=tag:sql" class="tag" target="_blank" rel="noopener">#sql</a> <br>Sqlselect selection_list  查询的列
from table             要查询的表
join on                连接的表
where                  查询条件
group by               分组查询
having                 分组条件过滤
order by               字段排序
sort by                结果排序
limit                  限制结果数
union/union all        合并表
复制<br><br>SqlFROM
&lt;left_table&gt;
ON
&lt;join_condition&gt;
&lt;join_type&gt;
JOIN
&lt;right_table&gt;
WHERE
&lt;where_condition&gt;
GROUP BY
&lt;group_by_list&gt;
HAVING
&lt;having_condition&gt;
SELECT
DISTINCT
&lt;select_list&gt;
ORDER BY
&lt;order_by_condition&gt;
LIMIT
&lt;limit_number&gt;
复制<br><br>Sql尽量不要使用子查询、尽量不要使用in not in
select * from aa1
where id in ( select id from bb ) ;
查询尽量避免join连接查询，但是这种操作咱们是永远避免不了的。
查询永远是小表驱动大表（永远是小结果集驱动大结果集）
复制<br><br>错综复杂的数据，需要建立模型，才能储存在数据库。所谓"模型"就是两样东西：实体（entity）+ 关系 （relationship）ER图。实体指的是那些实际的对象，带有自己的属性，可以理解成一组相关属性的容器。关系就 是实体之间的联系，通常可以分成"一对一"、"一对多"和"多对多"等类型。<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310171944738.png" referrerpolicy="no-referrer"><br>Sql表的连接分成好几种类型。
内连接（inner join)
外连接（outer join)
左连接（left join)
右连接（right join)
全连接（full join)
复制<br>所谓“连接”，就是两张表根据关联字段，组合成一个数据集。问题是，两张表的关联字段的值往往是不一致的，如
果关联字段不匹配，怎么处理？比如，表A包含张三和李四，表B包含李四和王五，匹配的只有李四这一条记
录。
很容易看出，一共有四种处理方法。
只返回两张表匹配的记录，这叫内连接（inner join)。
返回匹配的记录，以及表A多余的记录，这叫左连接（left join)。
返回匹配的记录，以及表B多余的记录，这叫右连接（right join)。
返回匹配的记录，以及表A和表B各自的多余记录，这叫全连接（full join)。
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310172050319.png" referrerpolicy="no-referrer"><br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310172050052.png" referrerpolicy="no-referrer"><br><br><a href=".?query=tag:hive" class="tag" target="_blank" rel="noopener">#hive</a>   <a href=".?query=tag:sql" class="tag" target="_blank" rel="noopener">#sql</a> <br>重点
在Hive中，有一种专有的join操作,left semi join,我们称之为半开连接。它是left join的一种优化形式，只能查询左 表的信息，主要用于解决Hive中左表的数据是否存在的问题。相当于exists关键字的用法。
<br>建表u1 u2<br>Sqlcreate table if not exists u1(
id int, 
name string 
) 
row format delimited fields terminated by ',';

create table if not exists u2(
id int, 
name string 
) 
row format delimited fields terminated by ',';
复制<br>导入数据<br>Sqlload data local inpath '/export/test/u1.txt' into table u1;
load data local inpath '/export/test/u2.txt' into table u2;
复制<br>查询<br>Sql select * 
    from u1
    left semi join u2 on u1.id=u2.id;
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310172058774.png" referrerpolicy="no-referrer">
查询拥有相同的<br>Sqlhive&gt; select u1.* from u1 where exists (select 1 from u2 where u2.id = u1.id);
复制<br>上面两个结果是一样的<br>所有join连接，只支持等值连接(= 和 and )。不支持 != 、 &lt; 、&gt; 、 &lt;&gt; 、&gt;=、 &lt;= 、or<br><a data-tooltip-position="top" aria-label="Hive02.pdf > page=16&amp;selection=0,7,0,21" data-href="Hive02.pdf#page=16&amp;selection=0,7,0,21" href="\大数据\PDF\Hive02.pdf#page=16&amp;selection=0,7,0,21" class="internal-link" target="_self" rel="noopener">Hive02, page 16</a>
随便看看就行<br>
查询子句
<a data-tooltip-position="top" aria-label="Hive02.pdf > page=17&amp;selection=19,0,19,4" data-href="Hive02.pdf#page=17&amp;selection=19,0,19,4" href="\大数据\PDF\Hive02.pdf#page=17&amp;selection=19,0,19,4" class="internal-link" target="_self" rel="noopener">Hive02, page 17</a>
<br>记录时间: 2023/10/17/8:54 PM 当前天气: 晴 🔆 • 当前温度: 16°C • 体感温度: 16°C<br><br>&lt;&lt; <a data-href="2023-10-16" href="\日记\2023-10-16.html" class="internal-link" target="_self" rel="noopener">2023-10-16</a> | <a data-href="2023-10-18" href="\日记\2023-10-18.html" class="internal-link" target="_self" rel="noopener">2023-10-18</a> &gt;&gt;]]></description><link>日记\2023-10-17.html</link><guid isPermaLink="false">日记/2023-10-17.md</guid><pubDate>Tue, 17 Oct 2023 13:12:32 GMT</pubDate><enclosure url="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310171037725.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310171037725.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2023-10-18]]></title><description><![CDATA[<a class="tag" href="?query=tag:sql" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#sql</a> <a class="tag" href="?query=tag:hive" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hive</a> 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：18℃  天气：晴 温度范围：17 ~ 23℃
湿度：76% 风向：东南风 2级 紫外线：中等
空气质量：良 PM: 17 日出: 06:53 日落: 18:23<br>一言
 一辈子，能坚持做好一件事就很伟大了！  —— 《网易云音乐》 · 柯钊
<br><br><br>早上 ： 调整ob界面和上课
下午 ： hive sql学习<br><br>记录时间: 2023/10/18/2:07 PM 当前天气: 晴 🔆 • 当前温度: 26°C • 体感温度: 26°C
<a href=".?query=tag:sql" class="tag" target="_blank" rel="noopener">#sql</a> <a href=".?query=tag:hive" class="tag" target="_blank" rel="noopener">#hive</a> <br><br>
<br>union ：将多个结果集合并，去重，排序 
<br>union all ：将多个结果集合并，不去重，不排序
<br>-- 单个union 语句不支持：orderBy、clusterBy、distributeBy、sortBy、limit
-- 单个union语句字段的个数要求相同，字段的顺序要求相同。
如果想去重查询中的某列数据,可以使用 distinct关键字<br><br>以下是复杂基本类型:<br><br>特殊基本类型
Java中有的而Hive中没有的：<br>
<br>long 
<br>char 
<br>short 
<br>byte
<br><br>复杂类型分为三种,分别是数组array,键值对map,和结构体struc<br>Sqlarray : col array&lt;基本类型&gt; ,下标从0开始，越界不报错，以NULL代替 
map : column map&lt;string,string&gt; 
struct: col struct
复制<br><a data-tooltip-position="top" aria-label="Hive02.pdf > page=21&amp;selection=2,0,2,7" data-href="Hive02.pdf#page=21&amp;selection=2,0,2,7" href="\大数据\PDF\Hive02.pdf#page=21&amp;selection=2,0,2,7" class="internal-link" target="_self" rel="noopener">Hive02, page 21</a><br>记录时间: 2023/10/18/2:34 PM 当前天气: 晴 🔆 • 当前温度: 27°C • 体感温度: 27°C<br><br>Sql--设置本地hive
set hive.exec.mode.local.auto=true;

复制<br>创建表<br>Sqlcreate table if not exists t_order(
name string,
orderdate string,
cost int
)
row format delimited fields terminated by ',';
复制<br>导入数据<br>Sqlload data local inpath '/export/test/order.txt' into table t_order;
复制<br>
<br>窗口函数又名开窗函数，属于分析函数的一种。用于解决复杂报表统计需求的功能强大的函数。窗口函数用于计算基于组的某种聚合值一它和聚合函数的不同之处是：对于每个组返回多行，而聚合函数对于每个组只返回一行。
<br>开窗函数指定了分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变化而变化。
<br><br>使用窗口函数之前一般要通过over()进行开窗，简单可以写成函数+over简单的写法如下：<br>Sql--1.不使用窗口函数
--查询所有明细
select * from t order ;
#查询总量
select count ( * ) from t order :
--2.使用窗口函数
select * , count ( * ) over ( ) from t order ;
复制<br>注意：
窗口函数是针对每一行数据的.
如果over中没有参数，默认的是全部结果集
<br><br>Sqlselect *
from t_order
where substring(orderdate,1,7)='2018-1';

复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310181501218.png" referrerpolicy="no-referrer"><br>Sqlselect *,count(*) over()
 from t_order
 where substring(orderdate,1,7) = '2018-1';
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310181509077.png" referrerpolicy="no-referrer"><br><br><br>Sqlselect * ,sum(cost) over(partition by month(orderdate)) from t_order;
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310181517706.png" referrerpolicy="no-referrer"><br><br>Sqlselect * ,sum(cost) over(partition by month(orderdate) order by orderdate) from t_order;
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310181520658.png" referrerpolicy="no-referrer"><br><br>
<br>PRECEDING:往前
<br>FOLLOWING:往后
<br>CURRENT ROW:当前行
<br>UNBOUNDED:起点，PNBOUNDED PRECEDING表示从前面的起点，UNBOUNDED FOLLOWING:表示到后面的终点
<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310181520501.png" referrerpolicy="no-referrer"><br>一般window子句都是rows开头<br>例子：
<img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310181521120.png" referrerpolicy="no-referrer"><br>记录时间: 2023/10/18/3:22 PM 当前天气: 晴 🔆 • 当前温度: 27°C • 体感温度: 27°C<br><br>&lt;&lt; <a data-href="2023-10-17" href="\日记\2023-10-17.html" class="internal-link" target="_self" rel="noopener">2023-10-17</a> | <a data-href="2023-10-19" href="\日记\2023-10-19.html" class="internal-link" target="_self" rel="noopener">2023-10-19</a> &gt;&gt;]]></description><link>日记\2023-10-18.html</link><guid isPermaLink="false">日记/2023-10-18.md</guid><pubDate>Wed, 18 Oct 2023 07:33:19 GMT</pubDate><enclosure url="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310181501218.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310181501218.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2023-10-19]]></title><description><![CDATA[<a class="tag" href="?query=tag:java" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#java</a> 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：24℃  天气：阴 温度范围：13 ~ 24℃
湿度：61% 风向：北风 1级 紫外线：很弱
空气质量：良 PM: 14 日出: 06:54 日落: 18:22<br>一言
 呐，做人呢，最重要的是开心，你饿不饿，我煮碗面给你吃啊。  —— 《大内密探零零发》 · 周星驰
<br><br><br>早上上课：
下午看点视频
记录时间: 2023/10/19/2:23 PM 当前天气: 多云 🌥️ • 当前温度: 28°C • 体感温度: 28°C
我好想你<br><br><br><a href=".?query=tag:java" class="tag" target="_blank" rel="noopener">#java</a>
<a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV15H4y1f78A/?spm_id_from=333.1007.tianma.1-1-1.click&amp;vd_source=43e56318cc2b14ddb96ad55ad99d99df" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV15H4y1f78A/?spm_id_from=333.1007.tianma.1-1-1.click&amp;vd_source=43e56318cc2b14ddb96ad55ad99d99df" target="_blank">一小时学完Java设计模式_哔哩哔哩_bilibili</a>
<a data-tooltip-position="top" aria-label="重学Java设计模式.pdf > page=7&amp;selection=0,0,0,3" data-href="重学Java设计模式.pdf#page=7&amp;selection=0,0,0,3" href="\Java\PDF\重学Java设计模式.pdf#page=7&amp;selection=0,0,0,3" class="internal-link" target="_self" rel="noopener">重学Java设计模式, page 7</a><br>今天明天就休息一下<br><br>&lt;&lt; <a data-href="2023-10-18" href="\日记\2023-10-18.html" class="internal-link" target="_self" rel="noopener">2023-10-18</a> | <a data-href="2023-10-20" href="\2023-10-20" class="internal-link" target="_self" rel="noopener">2023-10-20</a> &gt;&gt;]]></description><link>日记\2023-10-19.html</link><guid isPermaLink="false">日记/2023-10-19.md</guid><pubDate>Sat, 21 Oct 2023 06:55:11 GMT</pubDate></item><item><title><![CDATA[2023-10-21]]></title><description><![CDATA[<a class="tag" href="?query=tag:java" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#java</a> 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：20℃  天气：晴 温度范围：10 ~ 21℃
湿度：35% 风向：西北风 2级 紫外线：强
空气质量：优 PM: 7 日出: 06:55 日落: 18:20<br>一言
 变好的过程都不太舒服，试试再努力点。  —— 《网易云音乐》 · pony
<br><br><br>早上 ： 计算机网络
下午 ： 开始 <a href=".?query=tag:java" class="tag" target="_blank" rel="noopener">#java</a> 学习<br><br><br>Servlet定义：Servlet是基于Java技术的Web组件，由容器管理并产生动态的内容。Servlet与客户端通过Servlet
容器实现的请求/响应模型进行交互。<br><br>Ajax即Asynchronous Javascript And XML(异步JavaScript和XML)在2005年被Jesse James Garrett提出的新术语，用来描述一种使用现有技术集合的新方法，包括：HTML或XHTML,CSS,JavaScript,DOM,XML,
XSLT,以及最重要的XMLHttpRequest。使用Ajax技术网页应用能够快速地将增量更新呈现在用户界面上，而不需要重载（刷新）整个页面，这使得程序能够更快地回应用户的操作。<br><br>记录时间: 2023/10/21/4:41 PM 当前天气: 晴 🔆 • 当前温度: 17°C • 体感温度: 17°C
官网<a data-tooltip-position="top" aria-label="https://mybatis.org/mybatis-3/zh/getting-started.html" rel="noopener" class="external-link" href="https://mybatis.org/mybatis-3/zh/getting-started.html" target="_blank">mybatis – MyBatis 3 | 入门</a><br>对于idea来说创建一个mybatis工程 要先导包<br>Xml&lt;dependency&gt;  
  &lt;groupId&gt;org.mybatis&lt;/groupId&gt;  
  &lt;artifactId&gt;mybatis&lt;/artifactId&gt;  
  &lt;version&gt;3.5.3&lt;/version&gt;  
&lt;/dependency&gt;  
&lt;dependency&gt;  
  &lt;groupId&gt;mysql&lt;/groupId&gt;  
  &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;  
  &lt;version&gt;8.0.30&lt;/version&gt;  
&lt;/dependency&gt;
复制<br>数据库的包选择对应版本就行
项目结果图：
<img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310211644578.png" referrerpolicy="no-referrer"><br><br>主要字段<br>Sqlid int 
name string
age int
复制<br><br>要使用mybatis就要先创建一个配置文件 在官网进行复制<br>Xml&lt;?xml version="1.0" encoding="UTF-8" ?&gt;  
&lt;!DOCTYPE configuration  
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"  
        "https://mybatis.org/dtd/mybatis-3-config.dtd"&gt;  
&lt;configuration&gt;  
    &lt;environments default="development"&gt;  
        &lt;environment id="development"&gt;  
            &lt;transactionManager type="JDBC"/&gt;  
            &lt;dataSource type="POOLED"&gt;  
                &lt;property name="driver" value="com.mysql.jdbc.Driver"/&gt;  
                &lt;property name="url" value="jdbc:mysql://127.0.0.1:3306/java_2023?serverTimezone=GMT%2B8"/&gt;  
                &lt;property name="username" value="root"/&gt;  
                &lt;property name="password" value="1234"/&gt;  
            &lt;/dataSource&gt;  
        &lt;/environment&gt;  
    &lt;/environments&gt;  
    &lt;mappers&gt;  
        &lt;mapper resource="mapper/userMapper.xml"/&gt;  
    &lt;/mappers&gt;  
&lt;/configuration&gt;
复制<br><br>Javapackage com.lfl.entity;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/21 16:19  
 * 文件名 : UserEntity  
 * 描述 :  
 */public class UserEntity {  
private int id;  
private String name;  
private Integer age;  
  
    public void setId(int id) {  
        this.id = id;  
    }  
  
    @Override  
    public String toString() {  
        return "UserEntity{" +  
                "id=" + id +  
                ", name='" + name + '\'' +  
                ", age=" + age +  
                '}';  
    }  
  
    public void setName(String name) {  
        this.name = name;  
    }  
  
    public void setAge(Integer age) {  
        this.age = age;  
    }  
  
    public int getId() {  
        return id;  
    }  
  
    public String getName() {  
        return name;  
    }  
  
    public Integer getAge() {  
        return age;  
    }  
}
复制<br><br>对于我们要操作进行mybatis 要进行mapper文件进行配置 都可以在官网复制<br>Xml&lt;?xml version="1.0" encoding="UTF-8" ?&gt;  
&lt;!DOCTYPE mapper  
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"  
        "https://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;  
&lt;mapper namespace="FirstTest"&gt;  
    &lt;select id="getByUsersAll" resultType="com.lfl.entity.UserEntity"&gt;  
        select * from java_2023.test_users  
    &lt;/select&gt;  
&lt;/mapper&gt;
复制<br>最后测试类的创建<br>Javaimport com.lfl.entity.UserEntity;  
import org.apache.ibatis.io.Resources;  
import org.apache.ibatis.session.SqlSessionFactory;  
import org.apache.ibatis.session.SqlSessionFactoryBuilder;  
  
import java.io.IOException;  
import java.io.InputStream;  
import java.util.List;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/21 16:24  
 * 文件名 : Test01  
 * 描述 :测试mybatis-config.xml是否能够被读取  
 */  
public class Test01 {  
    public static void main(String[] args) throws IOException {  
        // mybatis-config.xml 目录位置  
        String resource = "mybatis-config.xml";  
        //1.解析mybatis-config.xm2得到数据库相关的配置信息  
        InputStream inputStream = Resources.getResourceAsStream(resource);  
        //2.创建得到一个sqLSessionFactory  
        SqlSessionFactory sql = new SqlSessionFactoryBuilder().build(inputStream);  
        //查询数据库的数据  
        List&lt;UserEntity&gt; users = sql.openSession().selectList("getByUsersAll","com.lfl.entity.UserEntity");  
        //打印查询结果  
        System.out.println(users);  
    }  
}
复制<br>在代码编写的过程中给的类最好是 完整路径 的名称
例如<br>Javacom.lfl.entity.UserEntity
复制<br>结果：
<img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310211650367.png" referrerpolicy="no-referrer"><br><br>方式1：
通过 as 进行反向注解  <br>方式2：
resultMap
定义数据库表中字段名称与类中成员属性名称关联映射
数据库字段：flight_id---类中成员名称flightld
定义数据库中字段名称与我们类中成员属性值关联映射<br><br>namespace+id
全局唯一<br><br>Sqlcreate table test_student_complex  
(  
    student_id   int auto_increment  
        primary key,  
    student_name varchar(20) not null,  
    student_age  int         not null  
);
复制<br><br>Sqlpackage com.lfl.entity;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/21 17:41  
 * 文件名 : StudentEntity  
 * 描述 :  
 */public class StudentEntity {  
    private Integer studentId;  
  
    @Override  
    public String toString() {  
        return "StudentEntity{" +  
                "studentId=" + studentId +  
                ", studentName='" + studentName + '\'' +  
                ", studentAge=" + studentAge +  
                '}';  
    }  
  
    public Integer getStudentId() {  
        return studentId;  
    }  
  
    public void setStudentId(Integer studentId) {  
        this.studentId = studentId;  
    }  
  
    public String getStudentName() {  
        return studentName;  
    }  
  
    public void setStudentName(String studentName) {  
        this.studentName = studentName;  
    }  
  
    public Integer getStudentAge() {  
        return studentAge;  
    }  
  
    public void setStudentAge(Integer studentAge) {  
        this.studentAge = studentAge;  
    }  
  
    private String studentName;  
    private Integer studentAge;  
}
复制<br><br>Javapackage com.lfl.mapper;  
  
import com.lfl.entity.StudentEntity;  
  
import java.util.List;  
  
public interface StudentMapper {  
    List&lt;StudentEntity&gt; SelectALL();  
}
复制<br><br>Javapackage com.lfl.service;  
  
import com.lfl.entity.StudentEntity;  
import com.lfl.mapper.StudentMapper;  
import org.apache.ibatis.io.Resources;  
import org.apache.ibatis.session.SqlSessionFactory;  
import org.apache.ibatis.session.SqlSessionFactoryBuilder;  
  
import java.io.IOException;  
import java.io.InputStream;  
import java.util.List;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/21 17:54  
 * 文件名 : StudentService  
 * 描述 :  
 */public class StudentService {  
    private StudentMapper studentmapper;  
    public StudentService() throws IOException {  
        // mybatis-config.xml 目录位置  
        String resource = "mybatis-config.xml";  
        //1.解析mybatis-config.xm2得到数据库相关的配置信息  
        InputStream inputStream = Resources.getResourceAsStream(resource);  
        //2.创建得到一个sqLSessionFactory  
        SqlSessionFactory sql = new SqlSessionFactoryBuilder().build(inputStream);  
        //3.获取到sqLSession  
        studentmapper = sql.openSession().getMapper(StudentMapper.class);  
    }  
    public List&lt;StudentEntity&gt; SelectALL(){  
        return studentmapper.SelectALL();  
    }  
}
复制<br><br>Xml&lt;?xml version="1.0" encoding="UTF-8" ?&gt;  
&lt;!DOCTYPE mapper  
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"  
        "https://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;  
&lt;mapper namespace="com.lfl.mapper.StudentMapper"&gt;  
  
    &lt;resultMap id="studentMap" type="com.lfl.entity.StudentEntity"&gt;  
        &lt;id column="student_id" property="studentId"/&gt;  
        &lt;result column="student_name" property="studentName"/&gt;  
        &lt;result column="student_age" property="studentAge"/&gt;  
    &lt;/resultMap&gt;  
  
    &lt;select id="SelectALL" resultMap="studentMap"&gt;  
        select * from java_2023.test_student_complex  
    &lt;/select&gt;  
  
&lt;/mapper&gt;
复制<br><br>Javaimport com.lfl.entity.StudentEntity;  
import com.lfl.service.StudentService;  
  
import java.io.IOException;  
import java.util.List;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/21 18:01  
 * 文件名 : TestStudent  
 * 描述 :  
 */public class TestStudent {  
    public static void main(String[] args) throws IOException {  
        StudentService studentService = new StudentService();  
        List&lt;StudentEntity&gt; students = studentService.SelectALL();  
        for (StudentEntity student : students) {  
            System.out.println(student);  
        }  
    }  
}
复制<br><img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310211811039.png" referrerpolicy="no-referrer"><br><br>select:            映射查询语句
insert:            映射插入语句
update:          映射更新语句
delete:           映射删除语句
sql:                可以重用的sql代码块
resultMap:    最复杂，最有力量的元素，用来描述如何从数据库结果集中加载你的对象
cache:           配置给定命名空间的缓存
cache-ref:     从其他命名空间引用缓存配置<br><br>第一种方法:传递map型；
第一种方法：多个参数如果不封装成Map参数值需要通过，多个参数的时候要使用@Param给指定参数，否则会出现找不到参数的错误
第三种方法：传递pojo;<br><br><br>Javapackage com.lfl.entity;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/21 17:41  
 * 文件名 : StudentEntity  
 * 描述 :  
 */public class StudentEntity {  
    private Integer studentId;  
    private String studentName;  
    private Integer studentAge;  
  
    @Override  
    public String toString() {  
        return "StudentEntity{" +  
                "studentId=" + studentId +  
                ", studentName='" + studentName + '\'' +  
                ", studentAge=" + studentAge +  
                '}';  
    }  
  
    public Integer getStudentId() {  
        return studentId;  
    }  
  
    public StudentEntity() {  
    }  
    public StudentEntity(String studentName, Integer studentAge) {  
        this.studentName = studentName;  
        this.studentAge = studentAge;  
    }  
  
    public void setStudentId(Integer studentId) {  
        this.studentId = studentId;  
    }  
  
    public String getStudentName() {  
        return studentName;  
    }  
  
    public void setStudentName(String studentName) {  
        this.studentName = studentName;  
    }  
  
    public Integer getStudentAge() {  
        return studentAge;  
    }  
  
    public void setStudentAge(Integer studentAge) {  
        this.studentAge = studentAge;  
    }  

}
复制<br><br>Javapackage com.lfl.mapper;  
  
import com.lfl.entity.StudentEntity;  
import org.apache.ibatis.annotations.Mapper;  
import org.apache.ibatis.annotations.Param;  
  
import java.util.List;  
import java.util.Map;  
  
public interface StudentMapper {  
    List&lt;StudentEntity&gt; selectALL();  
    StudentEntity selectById(Integer id);  
  
    int insertStudent(StudentEntity student);  
  
    int deleteStudent(Integer id);  
  
    int updateStudent(StudentEntity student);  
  
    List&lt;StudentEntity&gt; selectMore(@Param("studentName") String studentName, @Param("studentAge") Integer studentAge);  
  
    List&lt;StudentEntity&gt; selectMorePoJo(StudentEntity student);  
  
    List&lt;StudentEntity&gt; dynamicQuery(StudentEntity student);  
}
复制<br><br>Javapackage com.lfl.service;  
  
import com.lfl.entity.StudentEntity;  
import com.lfl.mapper.StudentMapper;  
import org.apache.ibatis.annotations.Param;  
import org.apache.ibatis.io.Resources;  
import org.apache.ibatis.session.SqlSession;  
import org.apache.ibatis.session.SqlSessionFactory;  
import org.apache.ibatis.session.SqlSessionFactoryBuilder;  
  
import java.io.IOException;  
import java.io.InputStream;  
import java.util.List;  
import java.util.Map;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/21 17:54  
 * 文件名 : StudentService  
 * 描述 :  
 */public class StudentService {  
    private StudentMapper studentmapper;  
    private SqlSession sqlSession;  
    public StudentService() throws IOException {  
        // mybatis-config.xml 目录位置  
        String resource = "mybatis-config.xml";  
        //1.解析mybatis-config.xm2得到数据库相关的配置信息  
        InputStream inputStream = Resources.getResourceAsStream(resource);  
        //2.创建得到一个sqLSessionFactory  
        SqlSessionFactory sql = new SqlSessionFactoryBuilder().build(inputStream);  
        //3.获取到sqLSession  
        sqlSession = sql.openSession();  
        studentmapper = sqlSession.getMapper(StudentMapper.class);  
    }  
    public List&lt;StudentEntity&gt; selectALL(){  
        return studentmapper.selectALL();  
    }  
  
    public StudentEntity selectById(Integer id){  
        return studentmapper.selectById(id);  
    }  
  
    public int insertStudent(StudentEntity student){  
        int result = studentmapper.insertStudent(student);  
        sqlSession.commit(); //提交事务  
        return result;  
    }  
  
    public int deleteStudent(Integer id){  
        int result = studentmapper.deleteStudent(id);  
        sqlSession.commit(); //提交事务  
        return result;  
    }  
  
    public int updateStudent(StudentEntity student){  
        int result = studentmapper.updateStudent(student);  
        sqlSession.commit(); //提交事务  
        return result;  
    }  
  
    public List&lt;StudentEntity&gt; selectMore(@Param("studentName") String studentName, @Param("studentAge") Integer studentAge) {  
        return studentmapper.selectMore(studentName,studentAge);  
    }  
  
    public List&lt;StudentEntity&gt; selectMorePoJo(StudentEntity student) {  
        return studentmapper.selectMorePoJo(student);  
    }  
  
    public List&lt;StudentEntity&gt; dynamicQuery(StudentEntity student) {  
        return studentmapper.dynamicQuery(student);  
    }  
}
复制<br><br>Xml&lt;?xml version="1.0" encoding="UTF-8" ?&gt;  
&lt;!DOCTYPE mapper  
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"  
        "https://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;  
&lt;mapper namespace="com.lfl.mapper.StudentMapper"&gt;  
  
    &lt;resultMap id="studentMap" type="com.lfl.entity.StudentEntity"&gt;  
        &lt;id column="student_id" property="studentId"/&gt;  
        &lt;result column="student_name" property="studentName"/&gt;  
        &lt;result column="student_age" property="studentAge"/&gt;  
    &lt;/resultMap&gt;  
  
    &lt;select id="selectALL" resultMap="studentMap"&gt;  
        select * from java_2023.test_student_complex  
    &lt;/select&gt;  
  
    &lt;select id="selectById"  parameterType="int" resultMap="studentMap"&gt;  
        select * from java_2023.test_student_complex where student_id = #{studentId}  
    &lt;/select&gt;  
  
    &lt;insert id="insertStudent" parameterType="com.lfl.entity.StudentEntity"&gt;  
        insert into java_2023.test_student_complex(student_name,student_age)  
        values(#{studentName},#{studentAge})  
    &lt;/insert&gt;  
  
    &lt;delete id="deleteStudent" parameterType="int"&gt;  
        delete from java_2023.test_student_complex where student_id = #{studentId}  
    &lt;/delete&gt;  
  
    &lt;delete id="updateStudent" parameterType="com.lfl.entity.StudentEntity"&gt;  
        update java_2023.test_student_complex  
        set student_name = #{studentName},student_age = #{studentAge}  
        where student_id = #{studentId}  
    &lt;/delete&gt;  
  
    &lt;select id="selectMore" resultMap="studentMap"&gt;  
        select * from java_2023.test_student_complex  
        where student_name=#{studentName} and student_age=#{studentAge}    &lt;/select&gt;  
  
    &lt;select id="selectMorePoJo" parameterType="com.lfl.entity.StudentEntity" resultMap="studentMap"&gt;  
        select * from java_2023.test_student_complex  
        where student_name like concat('%',#{studentName},'%') and student_age=#{studentAge}    &lt;/select&gt;  
  
&lt;!--    dynamicQuery--&gt;  
    &lt;select id="dynamicQuery" parameterType="com.lfl.entity.StudentEntity" resultMap="studentMap"&gt;  
        select * from java_2023.test_student_complex  
        &lt;where&gt;  
            &lt;if test="studentName != null and studentName != ''"&gt;  
                and student_name like concat('%',#{studentName},'%')  
            &lt;/if&gt;  
            &lt;if test="studentAge != null and studentAge != ''"&gt;  
                and student_age = #{studentAge}  
            &lt;/if&gt;  
        &lt;/where&gt;  
    &lt;/select&gt;  
&lt;/mapper&gt;
复制<br><br>Javaimport com.lfl.entity.StudentEntity;  
import com.lfl.service.StudentService;  
import org.junit.Test;  
  
import java.io.IOException;  
import java.util.HashMap;  
import java.util.List;  
import java.util.Map;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/21 18:01  
 * 文件名 : TestStudent  
 * 描述 :  
 */public class TestStudent {  
  
    @Test//测试查询所有  
    public void test1() throws IOException {  
        StudentService studentService = new StudentService();  
        List&lt;StudentEntity&gt; students = studentService.selectALL();  
        for (StudentEntity student : students) {  
            System.out.println(student);  
        }  
    }  
  
    @Test//测试根据id查询  
    public void  test2() throws IOException {  
        StudentService studentService = new StudentService();  
        StudentEntity student = studentService.selectById(3);  
        System.out.println(student);  
    }  
  
    @Test//测试插入  
    public void test3() throws IOException {  
        StudentService studentService = new StudentService();  
        StudentEntity student = new StudentEntity();  
        student.setStudentName("朱恩财");  
        student.setStudentAge(20);  
        int i = studentService.insertStudent(student);  
        if (i&gt;0){  
            System.out.println("插入成功");  
        }else {  
            System.out.println("插入失败");  
        }  
    }  
  
    @Test//测试删除  
    public void test4() throws IOException {  
        StudentService studentService = new StudentService();  
        int i = studentService.deleteStudent(14);  
        if (i&gt;0){  
            System.out.println("删除成功");  
        }else {  
            System.out.println("删除失败");  
        }  
    }  
  
    @Test//测试修改  
    public void test5() throws IOException {  
        StudentService studentService = new StudentService();  
        StudentEntity student = new StudentEntity();  
        student.setStudentId(1);  
        student.setStudentName("朱恩财");  
        student.setStudentAge(20);  
        int i = studentService.updateStudent(student);  
        if (i&gt;0){  
            System.out.println("修改成功");  
        }else {  
            System.out.println("修改失败");  
        }  
    }  
  
    @Test//测试多条件查询  
    public void test6() throws IOException {  
        StudentService studentService = new StudentService();  
        List&lt;StudentEntity&gt; student = studentService.selectMore("朱恩财", 20);  
        for (StudentEntity studentEntity : student) {  
            System.out.println(studentEntity);  
        }  
    }  
  
    @Test//测试多条件查询  
    public void test7() throws IOException {  
        StudentService studentService = new StudentService();  
        StudentEntity student = new StudentEntity();  
        student.setStudentName("朱恩财");  
        student.setStudentAge(20);  
        List&lt;StudentEntity&gt; studentEntities = studentService.selectMorePoJo(student);  
        for (StudentEntity studentEntity : studentEntities) {  
            System.out.println(studentEntity);  
        }  
    }  
  
    @Test//测试动态查询  
    public void test8() throws IOException {  
        StudentService studentService = new StudentService();  
        StudentEntity student = new StudentEntity();  
        student.setStudentName("朱恩财");  
        List&lt;StudentEntity&gt; studentEntities = studentService.dynamicQuery(student);  
        for (StudentEntity studentEntity : studentEntities) {  
            System.out.println(studentEntity);  
        }  
    }  
}
复制<br><br>多看多报错就回了，尤其是记得要写类型的参数<br><br>&lt;&lt; <a data-href="2023-10-20" href="\2023-10-20" class="internal-link" target="_self" rel="noopener">2023-10-20</a> | <a data-href="2023-10-22" href="\日记\2023-10-22.html" class="internal-link" target="_self" rel="noopener">2023-10-22</a> &gt;&gt;]]></description><link>日记\2023-10-21.html</link><guid isPermaLink="false">日记/2023-10-21.md</guid><pubDate>Sat, 21 Oct 2023 13:24:51 GMT</pubDate><enclosure url="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310211644578.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310211644578.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2023-10-22]]></title><description><![CDATA[ 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：19℃  天气：晴 温度范围：8 ~ 22℃
湿度：52% 风向：南风 1级 紫外线：无
空气质量：良 PM: 19 日出: 06:56 日落: 18:19<br>一言
 无聊的并不是时间，而是平庸无奇的我。  —— 《樱花庄的宠物女孩》 · 樱花庄的宠物女孩​
<br><br><br>练车跑全程<br><br><br>&lt;&lt; <a data-href="2023-10-21" href="\日记\2023-10-21.html" class="internal-link" target="_self" rel="noopener">2023-10-21</a> | <a data-href="2023-10-23" href="\日记\2023-10-23.html" class="internal-link" target="_self" rel="noopener">2023-10-23</a> &gt;&gt;]]></description><link>日记\2023-10-22.html</link><guid isPermaLink="false">日记/2023-10-22.md</guid><pubDate>Sun, 22 Oct 2023 12:53:52 GMT</pubDate></item><item><title><![CDATA[2023-10-23]]></title><description><![CDATA[<a class="tag" href="?query=tag:java" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#java</a> 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：15℃  天气：晴 温度范围：12 ~ 25℃
湿度：73% 风向：南风 2级 紫外线：很弱
空气质量：良 PM: 22 日出: 06:56 日落: 18:18<br>一言
 人间忽晚，山河已秋。  —— 《人间忽晚》 · 亦沫不吃鱼
<br><br><br><a href=".?query=tag:java" class="tag" target="_blank" rel="noopener">#java</a><br><br>记录时间: 2023/10/23/9:36 AM 当前天气: 晴 🔆 • 当前温度: 22°C • 体感温度: 22°C
学习spring和补前面的查缺补漏<br>记录时间: 2023/10/23/1:36 PM 当前天气: 晴 🔆 • 当前温度: 25°C • 体感温度: 25°C<br><br><img src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310231337239.png" referrerpolicy="no-referrer">
<img alt="image.png" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310231415424.png" referrerpolicy="no-referrer"><br><br>Javapackage com.lfl.Dynamic_Proxy;  
  
public interface Star {  
    //我们可以把所有想要被代理的方法定义在接口当中  
    public abstract String sing(String name);  
  
    public abstract String dance(String name) ;  
  
    public abstract String rap(String name);  
  
    public abstract String basketball(String name) ;  
}
复制<br>Javapackage com.lfl.Dynamic_Proxy;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 13:38  
 * 文件名 : KunKun  
 * 描述 :  
 */public class BigStar implements Star {  
    private String name;  
  
    public BigStar() {  
    }  
  
    public BigStar(String name) {  
        this.name = name;  
    }  
  
    @Override  
    public String sing(String name) {  
        System.out.println(this.name + "正在唱歌" + "歌名是：" + name);  
        return "唱完了";  
    }  
  
    @Override  
  
    public String dance(String name) {  
        System.out.println(this.name + "正在跳舞" + "舞蹈名是：" + name);  
        return "跳完了";  
    }  
  
    @Override  
  
    public String rap(String name) {  
        System.out.println(this.name + "正在rap"  + "rap名是：" + name);  
        return "rap完了";  
    }  
  
    @Override  
  
    public String basketball(String name) {  
        System.out.println(this.name + "正在打篮球" + "篮球名是：" + name);  
        return "打完了";  
    }  
  
    /**  
     * 获取  
     *  
     * @return name  
     */    public String getName() {  
        return name;  
    }  
  
    /**  
     * 设置  
     *  
     * @param name  
     */    public void setName(String name) {  
        this.name = name;  
    }  
  
    public String toString() {  
        return "BigStar{name = " + name + "}";  
    }  
}
复制<br>Javapackage com.lfl.Dynamic_Proxy;  
  
import java.lang.reflect.InvocationHandler;  
import java.lang.reflect.Method;  
import java.lang.reflect.Proxy;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 13:50  
 * 文件名 : ProxyUtil  
 * 描述 :创建一个代理  
 */  
public class ProxyUtil {  
    //方法的作用：  
    //给一个明星的对象，创建一个代理  
    //形参：  
    //被代理的明星对象  
    //返回值：  
    //给明星创建的代理  
    public static Star createProxy(final BigStar bigStar) {  
        //public static object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h)  
        //参数一：用于指定用哪个类加载器，去加载生成的代理类  
        //参数二：指定接口，这些接口用于指定生成的代理长什么，也就是有哪些方法  
        //参数三：用来指定生成的代理对象要干什么事情*/  
        Star star = (Star) Proxy.newProxyInstance(  
                ProxyUtil.class.getClassLoader(),//找到是哪一个类进行加载  
                new Class[]{Star.class},//能代理那些方法，这些方法都在哪个接口当中，都可以写在这里  
                new InvocationHandler() {//代理都要做什么事情  
                    @Override  
                    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {  
                        //参数一：代理的对象  
                        //参数二：要运行的方法sing  
                        //参数三：调用sing方法时，传递的实参  
                        if ("sing".equals(method.getName())) {  
                            System.out.println("准备话筒，我要收钱");  
                        } else if ("dance".equals(method.getName())) {  
                            System.out.println("准备舞台，我要收钱");  
                        } else if ("rap".equals(method.getName())) {  
                            System.out.println("准备麦克风，我要收钱");  
                        } else if ("basketball".equals(method.getName())) {  
                            System.out.println("准备球场，我要收钱");  
                        }  
                        //去找大明星开始唱歌或者跳舞  
                        //代码的表现形式：调用大明星里面唱歌或者跳舞的方法  
                        return method.invoke(bigStar, args);  
                    }  
                }  
        );  
  
        return star;  
    }  
}
复制<br><br>Javapackage com.lfl.Dynamic_Proxy;  
  
import org.junit.Test;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 13:59  
 * 文件名 : Test01  
 * 描述 :  
 */public class Test01 {  
    @Test  
    public void test1() {  
        BigStar KunKun = new BigStar("KunKun");  
        Star proxy = ProxyUtil.createProxy(KunKun);  
        String result = proxy.sing("只因你太美");  
        System.out.println(result);  
    }  
  
}
复制<br><br>动态代理
类：  实体 代理接口 代理工具

<br>首先实体和代理接口要有一样的方法名，但接口是抽象方法，而实体可以将方法具现化。
<br>其次实体要继承代理接口的方法 且重写方法！！！
<br>而对象的创建需要我们通过代理来进行创建
<br>需要扩展的方法也是在invoke里面进行扩展而不是在实体里面扩展，降低了代码耦合度

<br><br><a data-href="反射&amp;动态代理" href="\Java\反射&amp;动态代理.html" class="internal-link" target="_self" rel="noopener">反射&amp;动态代理</a>
记录时间: 2023/10/23/5:04 PM 当前天气: 晴 🔆 • 当前温度: 22°C • 体感温度: 22°C<br><br>Javapackage com.lfl.myreflect.demo1;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 17:00  
 * 文件名 : MyReflectDemo1  
 * 描述 :  
 */public class MyReflectDemo1 {  
    public static void main(String[] args) throws Exception{  
        //第一种获取反射对象的方式  
        Class&lt;?&gt; clazz1 = Class.forName("com.lfl.myreflect.demo1.Student");  
        //第二种获取反射对象的方式  
        Class&lt;Student&gt; clazz2 = Student.class;  
        //第三种获取反射对象的方式  
        Student student = new Student();  
        Class&lt;? extends Student&gt; clazz3 = student.getClass();  
  
  
    }  
}
复制<br>重点
Class.forName（）是最常用的获取方式
<br><br>Constructor&lt;?&gt;[]getConstructors():返回所有公共构造方法对象的数组
Constructor&lt;?&gt;[]getDeclaredConstructors():返回所有构造方法对象的数组
Constructor&lt;T&gt;getConstructor(Class&lt;?&gt;..parameterTypes):返回单个公共构造方法对象
ConstructorgetDeclaredConstructor(Class&lt;?&gt;...parameterTypes):返回单个构造方法对象
Constructor类中用于创建对象的方法
T newlnstance(Object...initargs):根据指定的构造方法创建对象
setAccessible(boolean flag):设置为true,表示取消访问检查<br><br>Javapackage com.lfl.myreflect.demo2;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 14:34  
 * 文件名 : Student  
 * 描述 :  
 */public class Student {  
    private String name;  
    private int age;  
  
    public Student() {  
    }  
  
    private Student(String name, int age) {  
        this.name = name;  
        this.age = age;  
    }  
  
    protected Student(int age) {  
        this.age = age;  
    }  
  
    public Student(String name) {  
        this.name = name;  
    }  
  
    /**  
     * 获取  
     *  
     * @return name  
     */    public String getName() {  
        return name;  
    }  
  
    /**  
     * 设置  
     *  
     * @param name  
     */    public void setName(String name) {  
        this.name = name;  
    }  
  
    /**  
     * 获取  
     *  
     * @return age  
     */    public int getAge() {  
        return age;  
    }  
  
    /**  
     * 设置  
     *  
     * @param age  
     */    public void setAge(int age) {  
        this.age = age;  
    }  
  
    public String toString() {  
        return "Student{name = " + name + ", age = " + age + "}";  
    }  
}
复制<br>Javapackage com.lfl.myreflect.demo2;  
  
import java.lang.reflect.Constructor;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 14:34  
 * 文件名 : MyReflectDemo1  
 * 描述 :  
 */public class MyReflectDemo2 {  
    /*  
   class类中用于获取构造方法的方法  
   Constructor&lt;?&gt;[] getConstructors()   Constructor&lt;?&gt;[] getDeclaredConstructors()   Constructor&lt;T&gt; getConstructor(Class&lt;?&gt;... parameterTypes)   Constructor&lt;T&gt; getDeclaredConstructor(Class&lt;?&gt;... parameterTypes)   Constructor类中用于创建对象的方法  
   T newInstance(Object...initargs)   setAccessible(boolean flag)   */    public static void main(String[] args) throws Exception {  
        // 获取字节码文件对象  
        Class&lt;?&gt; clazz = Class.forName("com.lfl.myreflect.demo2.Student");  
        // 获取所有公共构造方法  
//        Constructor&lt;?&gt;[] cons = clazz.getConstructors();  
//        for (Constructor&lt;?&gt; con : cons) {  
//            System.out.println(con);  
//        }  
  
//        // 获取所有构造方法  
//        Constructor&lt;?&gt;[] cons = clazz.getDeclaredConstructors();  
//        for (Constructor&lt;?&gt; con : cons) {  
//            System.out.println(con);  
//        }  
  
        // 获取单个公共构造方法  
        // 获取空参  
        Constructor&lt;?&gt; con1 = clazz.getConstructor();  
        System.out.println(con1);  
  
        // 获取单个有参  
        Constructor&lt;?&gt; con2 = clazz.getConstructor(String.class);  
        System.out.println(con2);  
  
        // 获取多个有参  
        Constructor&lt;?&gt; con3 = clazz.getDeclaredConstructor(String.class, int.class);  
        System.out.println(con3);  
  
        // 获取常量字段值  
        int modifiers = con3.getModifiers();  
        System.out.println(modifiers);  
  
        //更具反射获取对象  暴力反射  
        con3.setAccessible(true);  
        Student student = (Student) con3.newInstance("张三", 11);  
        System.out.println(student);  
  
  
  
  
    }  
}
复制<br><br>Class类中用于获取成员变量的方法
Field[]getFields():返回所有公共成员变量对象的数组
Field[]getDeclaredFields():返回所有成员变量对象的数组
Field getField(String name):返回单个公共成员变量对象
Field getDeclaredField(String name):返回单个成员变量对象
Field类中用于创建对象的方法
void set(Object obj,Object value):赋值
Object get(Object obj)获取值。<br><br>Javapackage com.lfl.myreflect.demo3;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 14:34  
 * 文件名 : Student  
 * 描述 :  
 */public class Student {  
    private String name;  
    private int age;  
    public String gender;  
  
    public Student() {  
    }  
    public Student(String name, int age, String gender) {  
        this.name = name;  
        this.age = age;  
        this.gender = gender;  
    }  
  
    /**  
     * 获取  
     * @return name  
     */    public String getName() {  
        return name;  
    }  
  
    /**  
     * 设置  
     * @param name  
     */    public void setName(String name) {  
        this.name = name;  
    }  
  
    /**  
     * 获取  
     * @return age  
     */    public int getAge() {  
        return age;  
    }  
  
    /**  
     * 设置  
     * @param age  
     */    public void setAge(int age) {  
        this.age = age;  
    }  
  
    /**  
     * 获取  
     * @return gender  
     */    public String getGender() {  
        return gender;  
    }  
  
    /**  
     * 设置  
     * @param gender  
     */    public void setGender(String gender) {  
        this.gender = gender;  
    }  
  
    public String toString() {  
        return "Student{name = " + name + ", age = " + age + ", gender = " + gender + "}";  
    }  
}
复制<br>Javapackage com.lfl.myreflect.demo3;  
  
import java.lang.reflect.Field;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 14:34  
 * 文件名 : MyReflectDemo1  
 * 描述 :  
 */public class MyReflectDemo3 {  
    /*  
Class类中用于获取成员变量的方法  
Field[] getFields():  
返回所有公共成员变量对象的数  
Field[] getDeclaredFields():  
返回所有成员变量对象的数组  
Field getField(String name):  
返回单个公共成员变量对象  
Field getDeclaredField(String name):返回单个成员变量对象  
Field类中用于创建对象的方法  
void set(Object obj, Object value):赋值  
Object get(Object obj)  
获取值  
*/  
    public static void main(String[] args) throws Exception {  
        Class&lt;?&gt; clazz = Class.forName("com.lfl.myreflect.demo3.Student");  
        //获取所有成员变量  
//        Field[] fields = clazz.getDeclaredFields();  
//        for (Field field : fields) {  
//            System.out.println(field);  
//        }  
  
        //获取单个成员变量  
//        Field field = clazz.getDeclaredField("name");  
//        System.out.println(field);  
  
//        获取权限修饰符  
        Field field = clazz.getDeclaredField("name");  
        int modifiers = field.getModifiers();  
        System.out.println("权限修饰符:" + modifiers);  
        Class&lt;?&gt; type = field.getType();  
        System.out.println("数据类型:" + type);  
  
//        获取成员变量记录的值  
        Student student = new Student("张三", 23, "男");  
        field.setAccessible(true);  
        Object value = field.get(student);  
        System.out.println("成员变量记录的值:" + value);  
  
        //修改对象里面记录的值  
        field.set(student, "李四");  
        System.out.println("修改后的值:" + student);  
    }  
}
复制<br><br>Class类中用于获取成员方法的方法
Method[]getMethods():返回所有公共成员方法对象的数组，包括继承的
Method[]getDeclaredMethods():返回所有成员方法对象的数组，不包括继承的
Method getMethod(String name,Class&lt;?&gt;..parameterTypes):返回单个公共成员方法对象
Method getDeclaredMethod(String name,Class&lt;?&gt;…parameterTypes):返回单个成员方法对象
Method类中用于创建对象的方法
Object invoke(Object obj,Object...args):运行方法
参数一：用obj对象调用该方法
参数二：调用方法的传递的参数（如果没有就不写）
返回值：方法的返回值（如果没有就不写）<br><br>Javapackage com.lfl.myreflect.demo4;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 15:17  
 * 文件名 : Student  
 * 描述 :  
 */public class Student {  
    private String name;  
    private int age;  
  
    public Student() {  
    }  
    public Student(String name, int age) {  
        this.name = name;  
        this.age = age;  
    }  
  
    /**  
     * 获取  
     * @return name  
     */    public String getName() {  
        return name;  
    }  
  
    /**  
     * 设置  
     * @param name  
     */  
    public void setName(String name) {  
        this.name = name;  
    }  
  
    /**  
     * 获取  
     * @return age  
     */    public int getAge() {  
        return age;  
    }  
  
    /**  
     * 设置  
     * @param age  
     */  
    public void setAge(int age) {  
        this.age = age;  
    }  
  
    public String toString() {  
        return "Student{name = " + name + ", age = " + age + "}";  
    }  
  
    public void sleep() {  
        System.out.println("在睡觉");  
    }  
  
    private void eat(String something) {  
        System.out.println("在吃" + something);  
    }  
}
复制<br>Javapackage com.lfl.myreflect.demo4;  
  
import java.lang.reflect.Method;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 14:34  
 * 文件名 : MyReflectDemo1  
 * 描述 :  
 */public class MyReflectDemo4 {  
    /*  
    利用反射获取成员方法  
            Class类中用于获取成员方法的方法  
    Method[]getMethods():返回所有公共成员方法对象的数组，包括继承的  
    Method[]getDeclaredMethods():返回所有成员方法对象的数组，不包括继承的  
    Method getMethod(String name,Class&lt;?&gt;..parameterTypes):返回单个公共成员方法对象  
    Method getDeclaredMethod(String name,Class&lt;?&gt;…parameterTypes):返回单个成员方法对象  
            Method类中用于创建对象的方法  
    Object invoke(Object obj,Object...args):运行方法  
    参数一：用obj对象调用该方法  
    参数二：调用方法的传递的参数（如果没有就不写）    返回值：方法的返回值（如果没有就不写）  
    */    public static void main(String[] args) throws Exception {  
        Class&lt;?&gt; clazz = Class.forName("com.lfl.myreflect.demo4.Student");  
  
//        获取里面所有的方法对象 包含父类中所有的公共方法  
//        Method[] methods = clazz.getMethods();  
//        for (Method method : methods) {  
//            System.out.println(method);  
//        }  
  
        //获取里面所有的方法对象 不包含父类中所有的公共方法  
//        Method[] declaredMethods = clazz.getDeclaredMethods();  
//        for (Method declaredMethod : declaredMethods) {  
//            System.out.println(declaredMethod);  
//        }  
  
        //获取单个方法对象  
        Method eat = clazz.getDeclaredMethod("eat", String.class);  
        System.out.println(eat);  
        System.out.println("=====================================");  
        //获取方法的修饰符  
        //获取方法的名字  
        //获取方法的形参  
        //获取方法的返回值  
        //获取方法的抛出的异常  
        int modifiers = eat.getModifiers();  
        System.out.println("修饰符:" + modifiers);  
  
  
        String name = eat.getName();  
        System.out.println("方法名:" + name);  
  
  
        Class&lt;?&gt;[] parameterTypes = eat.getParameterTypes();  
        for (Class&lt;?&gt; parameterType : parameterTypes) {  
            System.out.println("形参:" + parameterType);  
        }  
  
  
        Class&lt;?&gt;[] exceptionTypes = eat.getExceptionTypes();  
        for (Class&lt;?&gt; exceptionType : exceptionTypes) {  
            System.out.println("异常:" + exceptionType);  
        }  
  
        //方法运行  
        /*Method类中用于创建对象的方法  
        Object invoke(Object obj, object...args):运行方法  
        参数一：用obj对象调用该方法  
        参数二：调用方法的传递的参数（如果没有就不写）        返回值：方法的返回值（如果没有就不写）*/  
        Student student = new Student();  
        eat.setAccessible(true);  
        eat.invoke(student, "苹果");  
  
        Class&lt;?&gt; returnType = eat.getReturnType();  
        System.out.println("返回值:" + returnType);  
  
    }  
  
}
复制<br>总结
多写代码在后续很需要
<br>记录时间: 2023/10/23/10:40 PM 当前天气: 晴 🔆 • 当前温度: 17°C • 体感温度: 18°C
学完了IOC
还有我好想你🥺<br><br>&lt;&lt; <a data-href="2023-10-22" href="\日记\2023-10-22.html" class="internal-link" target="_self" rel="noopener">2023-10-22</a> | <a data-href="2023-10-24" href="\日记\2023-10-24.html" class="internal-link" target="_self" rel="noopener">2023-10-24</a> &gt;&gt;]]></description><link>日记\2023-10-23.html</link><guid isPermaLink="false">日记/2023-10-23.md</guid><pubDate>Mon, 23 Oct 2023 14:40:42 GMT</pubDate><enclosure url="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310231337239.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://typora-yxh.oss-cn-chengdu.aliyuncs.com/202310231337239.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2023-10-24]]></title><description><![CDATA[<a class="tag" href="?query=tag:java" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#java</a> <a class="tag" href="?query=tag:spring" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#spring</a> <a class="tag" href="?query=tag:分桶表" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#分桶表</a> <a class="tag" href="?query=tag:spring" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#spring</a> 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：18℃  天气：阴 温度范围：18 ~ 23℃
湿度：83% 风向：南风 2级 紫外线：很弱
空气质量：优 PM: 5 日出: 06:57 日落: 18:18<br>一言
 人皆荒醉，但唱观音。  —— 《唐大和上东征传》 · 淡海三
<br><br><br>早上： 复习 IOC 和 学习AOP<br><br>记录时间: 2023/10/24/9:34 AM 当前天气: 晴 🔆 • 当前温度: 23°C • 体感温度: 24°C
<a href=".?query=tag:java" class="tag" target="_blank" rel="noopener">#java</a> <a href=".?query=tag:spring" class="tag" target="_blank" rel="noopener">#spring</a><br><br>通过bean创建进行获取我们需要的对象 <br>Xml&lt;?xml version="1.0" encoding="UTF-8"?&gt;  
&lt;beans xmlns="http://www.springframework.org/schema/beans"  
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt;  
  
    &lt;!--  
    scope:    
    1. singleton (default) 是单例模式，一个容器中只有一个实例  
    2. prototype  是多例模式，每次从容器中getBean都会产生一个新的实例  
  
    init-method:    
    1. 在bean创建完成并且属性赋值完成后，执行初始化方法  
    2. 可以指定初始化方法  
  
    destroy-method:    
    1. 在容器关闭时，执行销毁方法  
    2. 可以指定销毁方法  
  
    depends-on:    
    1. 指定bean的依赖关系，当依赖的bean创建完成后，才会创建当前bean  
    2. 可以指定多个依赖的bean  
  
    --&gt;    
    &lt;bean id="car" class="study.IOC.bean.Car" /&gt;  
    &lt;bean id="student" class="study.IOC.bean.Student" depends-on="teacher"/&gt;  
    &lt;bean id="teacher" class="study.IOC.bean.Teacher" depends-on="car"/&gt;  
&lt;/beans&gt;
复制<br><br>Javapackage study.IOC;  
  
import org.junit.Test;  
import org.springframework.context.support.ClassPathXmlApplicationContext;  
import study.IOC.bean.Car;  
import study.IOC.bean.Student;  
import study.IOC.bean.Teacher;  
  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 21:01  
 * 文件名 : Test01  
 * 描述 :  
 */public class Test01 {  
    @Test//测试单例和多例  
    public void test01() {  
        ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext("study/IOC/springIOC.xml");  
        Student student1 = (Student) context.getBean("student");  
        Student student2 = (Student) context.getBean("student");  
        System.out.println(student1);  
        System.out.println(student2);  
        System.out.println(student1 == student2);  
    }  
  
    @Test//测试初始化和销毁方法  
    public void test02() {  
        ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext("study/IOC/springIOC.xml");  
        Student student = (Student) context.getBean("student");  
        System.out.println(student);  
        context.close();  
    }  
  
    @Test//选择谁先初始化  
    public void test03() {  
        ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext("study/IOC/springIOC.xml");  
        Student student = context.getBean("student", Student.class);  
        Car car = context.getBean("car", Car.class);  
        Teacher teacher = context.getBean("teacher", Teacher.class);  
        System.out.println(student);  
        System.out.println(car);  
        System.out.println(teacher);  
    }  
}
复制<br>总结
掌握bean的基本注入以及对应的部分参数
<br><br><a href=".?query=tag:分桶表" class="tag" target="_blank" rel="noopener">#分桶表</a> <br>Xml&lt;?xml version="1.0" encoding="UTF-8"?&gt;  
&lt;beans xmlns="http://www.springframework.org/schema/beans"  
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt;  
    &lt;!--  
    autowire: 自动装配  
    1. byName: 根据属性名称自动装配  
    2. byType: 根据属性类型自动装配  
    3. constructor: 根据构造器自动装配  
    4. no: 不自动装配  
  
    --&gt;  
    &lt;bean id="student" class="study.relyOn.bean.Student" autowire="byType"&gt;  
        &lt;!--  
        property: 用于注入属性 (基本类型和String类型)  
        1. name: 指定注入属性的名称  
        2. value: 指定注入属性的值  
        3. ref: 指定注入属性的对象  
  
        --&gt;  
        &lt;property name="name" value="lfl"/&gt;  
        &lt;property name="age" value="18"/&gt;  
        &lt;property name="list"&gt;  
            &lt;list&gt;  
                &lt;!--  
                  list: 用于注入属性 (集合类型)  
                    1. name: 指定注入属性的名称  
                    2. value: 指定注入属性的值  
                    3. ref: 指定注入属性的对象  
                --&gt;  
                &lt;value&gt;aaa&lt;/value&gt;  
                &lt;value&gt;bbb&lt;/value&gt;  
                &lt;value&gt;ccc&lt;/value&gt;  
            &lt;/list&gt;  
        &lt;/property&gt;  
        &lt;property name="score"&gt;  
            &lt;map&gt;  
                &lt;!--  
                  map: 用于注入属性 (集合类型)  
                    1. name: 指定注入属性的名称  
                    2. value: 指定注入属性的值  
                    3. ref: 指定注入属性的对象  
                --&gt;  
                &lt;entry key="语文" value="100"/&gt;  
                &lt;entry key="数学" value="90"/&gt;  
                &lt;entry key="英语" value="80"/&gt;  
            &lt;/map&gt;  
        &lt;/property&gt;  
    &lt;/bean&gt;  
  
    &lt;bean id="car" class="study.relyOn.bean.Car"/&gt;  
  
    &lt;bean id="coder" class="study.relyOn.bean.Coder"&gt;  
        &lt;!--  
        constructor-arg: 用于注入属性 (构造器) 1.name: 指定注入属性的名称 2.value: 指定注入属性的值 3.ref: 指定注入属性的对象  
        type: 指定注入属性的类型 index: 指定注入属性的索引 (从0开始)  
        name 和 index 不能同时使用  
        name index type 就是指定构造器的参数类型  
        --&gt;  
        &lt;constructor-arg type="java.lang.String" value="小明"/&gt;  
        &lt;constructor-arg  value="18" index="1"/&gt;  
    &lt;/bean&gt;  
&lt;/beans&gt;
复制<br>在对我们需要的实体类进行注入时候，一定要给予Set方法，用lombok的形式写注解<br><br>Javapackage study.relyOn.bean;  
  
import lombok.Setter;  
import lombok.ToString;  
  
import java.util.List;  
import java.util.Map;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 21:48  
 * 文件名 : Student  
 * 描述 :  
 */@ToString  
@Setter  
public class Student {  
    private String name;  
    private int age;  
    Car car;  
    List&lt;String&gt; list;  
    Map&lt;String, Integer&gt; score;  
}
复制<br>Javapackage study.relyOn.bean;  
  
import lombok.Data;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 22:28  
 * 文件名 : Coder  
 * 描述 :  
 */  
public class Coder {  
    private String name;  
    private int age;  
  
    public Coder() {  
    }  
    public Coder(String name, int age) {  
        this.name = name;  
        this.age = age;  
    }  
  
    public Coder(int age) {  
        this.age = age;  
    }  
  
    public Coder(String name) {  
        this.name = name;  
    }  
  
    /**  
     * 获取  
     * @return name  
     */    public String getName() {  
        return name;  
    }  
  
    /**  
     * 设置  
     * @param name  
     */  
    public void setName(String name) {  
        this.name = name;  
    }  
  
    /**  
     * 获取  
     * @return age  
     */    public int getAge() {  
        return age;  
    }  
  
    /**  
     * 设置  
     * @param age  
     */  
    public void setAge(int age) {  
        this.age = age;  
    }  
  
    public String toString() {  
        return "Coder{name = " + name + ", age = " + age + "}";  
    }  
}
复制<br><br>Javapackage study.relyOn;  
  
import org.junit.Test;  
import org.springframework.context.support.ClassPathXmlApplicationContext;  
import study.relyOn.bean.Coder;  
import study.relyOn.bean.Student;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/23 21:51  
 * 文件名 : Test01  
 * 描述 :  
 */public class Test01 {  
    @Test//测试property标签 和 注入集合 以及自动装配  
    public void test01() {  
        ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext("study/relyOn/springRelyOn.xml");  
        //这里没有创建car对象，但是car对象已经被创建了  
        //但还是需要在Student类中添加Car属性，否则会报错 以及配置文件中添加car bean属性  
        Student student = context.getBean("student", Student.class);  
        System.out.println(student);  
    }  
  
    @Test//有参构造注入  
    public void test02() {  
        ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext("study/relyOn/springRelyOn.xml");  
        Coder coder = context.getBean("coder", Coder.class);  
        System.out.println(coder);  
    }  
  
}
复制<br>name index type 就是指定构造器的参数类型<br><br>记录时间: 2023/10/24/9:34 AM 当前天气: 晴 🔆 • 当前温度: 23°C • 体感温度: 24°C
开始学习AOP<br><br><a href=".?query=tag:spring" class="tag" target="_blank" rel="noopener">#spring</a>
记录时间: 2023/10/24/3:01 PM 当前天气: 晴 🔆 • 当前温度: 27°C • 体感温度: 27°C<br><br>Javapackage study.aop.bean;  
  
import org.springframework.aop.AfterReturningAdvice;  
import org.springframework.aop.MethodBeforeAdvice;  
  
import java.lang.reflect.Method;  
import java.util.Arrays;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/24 14:44  
 * 文件名 : AopAdvice  
 * 描述 :  
 */public class AopAdvice implements MethodBeforeAdvice, AfterReturningAdvice {  
    @Override  
    public void before(Method method, Object[] args, Object target) throws Throwable {  
        System.out.println("方法名称为" + method.getName());  
        System.out.println("方法参数为" + Arrays.toString(args));  
        System.out.println("方法执行目标对象为" + target);  
    }  
  
    @Override  
    public void afterReturning(Object returnValue, Method method, Object[] args, Object target) throws Throwable {  
        System.out.println("方法名称为" + method.getName());  
        System.out.println("方法参数为" + Arrays.toString(args));  
        System.out.println("方法执行目标对象为" + target);  
        System.out.println("方法返回值为" + returnValue);  
    }  
}
复制<br>Javapackage study.aop.bean;  
  
import lombok.extern.java.Log;  
import org.aspectj.lang.JoinPoint;  
import org.aspectj.lang.ProceedingJoinPoint;  
  
import java.util.Arrays;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/24 9:54  
 * 文件名 : AopTest  
 * 描述 :  
 */@Log  
public class AopTest {  
    /*  
    * joinPoint.getArgs() 获取方法参数  
    * joinPoint.getSignature() 获取方法签名  
    * joinPoint.getTarget() 获取目标对象  
    * joinPoint.getThis() 获取代理对象  
    * joinPoint.getKind() 获取连接点类型  
    * joinPoint.getStaticPart() 获取连接点方法  
    * joinPoint.getSourceLocation() 获取连接点位置  
    * */    public void before(JoinPoint joinPoint) {  
        System.out.print(Arrays.toString(joinPoint.getArgs()));  
        log.info("我是前置方法执行后的日志");  
    }  
  
    public void after(JoinPoint joinPoint) {  
        System.out.println(joinPoint.getThis());  
        log.info("我是后置方法执行后的日志");  
    }  
  
    public void around(ProceedingJoinPoint joinPoint) throws Throwable {  
        joinPoint.proceed();  
        log.info("我是环绕方法执行后的日志");  
    }  
}
复制<br>Javapackage study.aop.bean;  
  
import lombok.Setter;  
import lombok.ToString;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/24 9:46  
 * 文件名 : Student  
 * 描述 :  
 */  
@ToString  
@Setter  
public class Student {  
    private String name;  
    private int age;  
    public Student() {  
        System.out.println("Student被创建了");  
    }  
    public void say(String text) {  
        System.out.println("我叫" + name + "，今年" + age + "岁，我说：" + text);  
    }  
  
    @Deprecated  
    public void test() {  
        System.out.println("我是注解方法");  
    }  
}
复制<br><br>Xml&lt;?xml version="1.0" encoding="UTF-8"?&gt;  
&lt;beans xmlns="http://www.springframework.org/schema/beans"  
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
       xmlns:aop="http://www.springframework.org/schema/aop"  
       xsi:schemaLocation="http://www.springframework.org/schema/beans  
                           http://www.springframework.org/schema/beans/spring-beans.xsd                           http://www.springframework.org/schema/aop                           http://www.springframework.org/schema/aop/spring-aop.xsd"&gt;  
    &lt;!--  
    那么，如何使用AOP呢？首先我们要明确，要实现AOP操作，我们需要知道这些内容：  
1.需要切入的类，类的哪个方法需要被切入  
2.切入之后需要执行什么动作  
3.是在方法执行前切入还是在方法执行后切入  
4.如何告诉Spring需要进行切入  
    --&gt;  
    &lt;!--    aop:config：配置AOP的配置信息  
    expression：切入点表达式，用于告诉Spring哪些类的哪些方法需要被切入  
    execution：用于指定切入点表达式的具体内容 格式：execution(返回值类型 包名.类名.方法名(参数列表))  
    参数列表: 参数列表可以使用通配符 *，表示任意类型的参数  
    --&gt;  
    &lt;aop:config&gt;  
        &lt;!--  
        aspect：切面，用于告诉Spring切入之后需要执行什么动作 ref：切面的id  
        --&gt;        &lt;aop:aspect ref="aopTest" &gt;  
            &lt;aop:pointcut id="test" expression="execution(* study.aop.bean.Student.say(String))"/&gt;  
            &lt;aop:before method="before" pointcut-ref="test"/&gt;  
            &lt;aop:after method="after" pointcut-ref="test"/&gt;  
            &lt;aop:around method="around" pointcut-ref="test"/&gt;  
        &lt;/aop:aspect&gt;  
  
        &lt;!--  
        注解方式  
        --&gt;  
        &lt;aop:aspect ref="aopTest"&gt;  
            &lt;aop:pointcut id="test2" expression="@annotation(Deprecated )"/&gt;  
            &lt;aop:before method="before" pointcut-ref="test2"/&gt;  
            &lt;aop:after method="after" pointcut-ref="test2"/&gt;  
        &lt;/aop:aspect&gt;  
    &lt;/aop:config&gt;  
    &lt;!--  
      通过接口的方式  
      --&gt;  
    &lt;aop:config&gt;  
        &lt;aop:pointcut id="test" expression="execution(* study.aop.bean.Student.say(String))"/&gt;  
        &lt;aop:advisor advice-ref="aopAdvice" pointcut-ref="test"/&gt;  
    &lt;/aop:config&gt;  
  
  
    &lt;bean id="student" class="study.aop.bean.Student"&gt;  
        &lt;property name="name" value="张三"/&gt;  
        &lt;property name="age" value="18"/&gt;  
    &lt;/bean&gt;  
    &lt;bean id="aopTest" class="study.aop.bean.AopTest"/&gt;  
    &lt;bean id="aopAdvice" class="study.aop.bean.AopAdvice"/&gt;  
&lt;/beans&gt;  
  
&lt;!--  
总结：从上到下的顺序  
1.bean被创建  
2.aop前置方法执行  
3.接口before执行  
4.方法执行  
5.接口after执行  
6.aop环绕方法执行  
7.aop后置方法执行  
8.bean被销毁  
  
AOP术语：  
- 通知（Advice):AOP框架中的增强处理，通知描述了切面何时执行以及如何执行增强处理，也就是我们上面编写的方法实现。  
- 连接点（join point):连接点表示应用执行过程中能够插入切面的一个点，这个点可以是方法的调用、异常的抛出，实际上就是我们在方法执行前或是执行后需要做的内容。  
- 切点（PointCut):可以插入增强处理的连接点，可以是方法执行之前也可以方法执行之后，还可以是抛出异常之类的。  
- 切面（Aspect):切面是通知和切点的结合，我们之前在xml中定义的就是切面，包括很多信息。  
- 引入（Introduction):引入允许我们向现有的类添加新的方法或者属性。  
- 织入（Weaving):将增强处理添加到目标对象中，并创建一个被增强的对象，我们之前都是在将我们的增强处理添加到目标对象，也就是织入（这名字挺有文艺范的）  
--&gt;
复制<br><br>Javapackage study.aop;  
  
import org.junit.Test;  
import org.springframework.context.support.ClassPathXmlApplicationContext;  
import study.aop.bean.Student;  
  
/**  
 * @author 叶星痕  
 * @data 2023/10/24 9:48  
 * 文件名 : Test01  
 * 描述 :  
 */public class Test01 {  
    @Test  
    public void test01() {  
        ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext("study/aop/springAop.xml");  
        Student student = context.getBean("student", Student.class);  
        student.say("hello");  
    }  
  
    @Test// 测试注解  
    public void test02() {  
        ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext("study/aop/springAop.xml");  
        Student student = context.getBean("student", Student.class);  
        student.test();  
    }  
  
    @Test// 测试Advice  
    public void test03() {  
        ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext("study/aop/springAop.xml");  
        Student student = context.getBean("student", Student.class);  
        student.say("hello");  
    }  
}
复制<br><br><br>
<br>1.bean被创建  
<br>2.aop前置方法执行  
<br>3.接口before执行  
<br>4.方法执行  
<br>5.接口after执行  
<br>6.aop环绕方法执行  
<br>7.aop后置方法执行  
<br>8.bean被销毁  
<br><br>
<br>通知（Advice):AOP框架中的增强处理，通知描述了切面何时执行以及如何执行增强处理，也就是我们上面编写的方法实现。  
<br>连接点（join point):连接点表示应用执行过程中能够插入切面的一个点，这个点可以是方法的调用、异常的抛出，实际上就是我们在方法执行前或是执行后需要做的内容。  
<br>切点（PointCut):可以插入增强处理的连接点，可以是方法执行之前也可以方法执行之后，还可以是抛出异常之类的。  
<br>切面（Aspect):切面是通知和切点的结合，我们之前在xml中定义的就是切面，包括很多信息。  
<br>引入（Introduction):引入允许我们向现有的类添加新的方法或者属性。  
<br>织入（Weaving):将增强处理添加到目标对象中，并创建一个被增强的对象，我们之前都是在将我们的增强处理添加到目标对象，也就是织入（这名字挺有文艺范的）
<br><br>&lt;&lt; <a data-href="2023-10-23" href="\日记\2023-10-23.html" class="internal-link" target="_self" rel="noopener">2023-10-23</a> | <a data-href="2023-10-25" href="\日记\2023-10-25.html" class="internal-link" target="_self" rel="noopener">2023-10-25</a> &gt;&gt;]]></description><link>日记\2023-10-24.html</link><guid isPermaLink="false">日记/2023-10-24.md</guid><pubDate>Tue, 24 Oct 2023 07:04:27 GMT</pubDate></item><item><title><![CDATA[2023-10-25]]></title><description><![CDATA[<a class="tag" href="?query=tag:java" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#java</a> <a class="tag" href="?query=tag:spring" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#spring</a> 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：18℃  天气：阴转小雨 温度范围：17 ~ 22℃
湿度：86% 风向：南风 3级 紫外线：很弱
空气质量：优 PM: 7 日出: 06:57 日落: 18:17<br>一言
 我喝过你喝的西北风，这算不算很穷。  —— 《网易云》 · 妤儿_moon
<br><br><br>记录时间: 2023/10/25/9:54 AM 当前天气: 阴，多云 ☁️ • 当前温度: 25°C • 体感温度: 25°C
AOP注解方式
<a href=".?query=tag:java" class="tag" target="_blank" rel="noopener">#java</a> <a href=".?query=tag:spring" class="tag" target="_blank" rel="noopener">#spring</a> <br><br><br>&lt;&lt; <a data-href="2023-10-24" href="\日记\2023-10-24.html" class="internal-link" target="_self" rel="noopener">2023-10-24</a> | <a data-href="2023-10-26" href="\日记\2023-10-26.html" class="internal-link" target="_self" rel="noopener">2023-10-26</a> &gt;&gt;]]></description><link>日记\2023-10-25.html</link><guid isPermaLink="false">日记/2023-10-25.md</guid><pubDate>Wed, 25 Oct 2023 01:55:39 GMT</pubDate></item><item><title><![CDATA[2023-10-26]]></title><description><![CDATA[ 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：13℃  天气：雾转中雨 温度范围：13 ~ 17℃
湿度：94% 风向：东北风 2级 紫外线：很弱
空气质量：轻度污染 PM: 36 日出: 06:58 日落: 18:16<br>一言
 银烛秋光冷画屏，轻罗小扇扑流萤。  —— 《秋夕》 · 杜牧
<br><br><br>记录时间: 2023/10/26/9:01 AM 当前天气: 阴，多云 ☁️ • 当前温度: 21°C • 体感温度: 22°C
下雨了<br><br><br>&lt;&lt; <a data-href="2023-10-25" href="\日记\2023-10-25.html" class="internal-link" target="_self" rel="noopener">2023-10-25</a> | <a data-href="2023-10-27" href="\2023-10-27" class="internal-link" target="_self" rel="noopener">2023-10-27</a> &gt;&gt;]]></description><link>日记\2023-10-26.html</link><guid isPermaLink="false">日记/2023-10-26.md</guid><pubDate>Thu, 26 Oct 2023 01:01:37 GMT</pubDate></item><item><title><![CDATA[2024-01-10]]></title><description><![CDATA[<a class="tag" href="?query=tag:hive" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hive</a> <a class="tag" href="?query=tag:先启动metastore服务" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#先启动metastore服务</a> 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：11℃  天气：多云 温度范围：6 ~ 11℃
湿度：74% 风向：东风 2级 紫外线：很弱
空气质量：良 PM: 22 日出: 07:42 日落: 18:18<br>一言
 希望你可以记住我，记住我这样活过，这样在你身边呆过。  —— 《挪威的森林》 · 村上春树
<br><br><br>学习Hive
<a href=".?query=tag:hive" class="tag" target="_blank" rel="noopener">#hive</a> <br><br>我只能说好家伙，视频和ob直接崩<br><br><br><br><img alt="image-20240110231408175" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110231408175.png" referrerpolicy="no-referrer"><br><br>rpm -qa | grep  mariadb<br>
rpm -e mariadb-libs-5.5.68-1.el7.x86_64 --nodeps<br><img alt="image-20240110160358209" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110160358209.png" referrerpolicy="no-referrer"><br><br><img alt="image-20240111180910473" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240111180910473.png" referrerpolicy="no-referrer"><br>cd MySQL<br>
rpm -ivh 01_mysql-community-common-5.7.29-1.el7.x86_64.rpm<br>
rpm -ivh 02_mysql-community-client-plugins-8.0.26-1.el7.x86_64.rpm<br>
rpm -ivh 03_mysql-community-libs-8.0.26-1.el7.x86_64.rpm<br>
rpm -ivh 04_mysql-community-libs-compat-8.0.26-1.el7.x86_64.rpm<br>
rpm -ivh 05_mysql-community-client-8.0.26-1.el7.x86_64.rpm <br><img alt="image-20240110160902103" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110160902103.png" referrerpolicy="no-referrer"><br><img alt="image-20240110161105094" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110161105094.png" referrerpolicy="no-referrer"><br><br>systemctl start mysqld<br>
systemctl enable mysqld<br>
​  <br><br>systemctl status mysqld<br><img alt="image-20240110161510614" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110161510614.png" referrerpolicy="no-referrer"><br><br>grep password /var/log/mysqld.log <br><img alt="image-20240110161808252" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110161808252.png" referrerpolicy="no-referrer"><br><br> mysql -uroot -p<br><img alt="image-20240110161831239" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110161831239.png" referrerpolicy="no-referrer"><br><br>
<br>修改MySQL的密码策略（安全等级）  
<br><br>含⼤⼩写字⺟、数字、特殊字符，以及对位数有要求<br>
show variables like '%validate_password%'; # 查看密<br>
码策略<br>
set global validate_password.policy=LOW; # 修改密码<br>
策略等级为LOW<br>
set global validate_password.length=4; # 密码的最<br>
⼩⻓度<br>
set global validate_password.mixed_case_count=0; # 设置<br>
密码中⾄少要包含0个⼤写字⺟和⼩写字⺟<br>
set global validate_password.number_count=0; # 设置密<br>
码中⾄少要包含0个数字<br>
set global validate_password.special_char_count=0; #<br>
设置密码中⾄少要包含0个特殊字符  <br><br>alter user root@localhost identified by '123456';  <br><br>create user root@'%' identified by '123456';<br>
grant all privileges on . to 'root'@'%' with grant<br>
option；<br><img alt="image-20240110162502329" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110162502329.png" referrerpolicy="no-referrer"><br><br><img alt="image-20240110162531963" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110162531963.png" referrerpolicy="no-referrer"><br><br>Hadoop的core-site.xml中，并分发到其它节点，且重启HDFS集群<br><img alt="image-20240110231857467" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110231857467.png" referrerpolicy="no-referrer"><br><br><br>tar -zxvf apache-hive-3.1.2-b<br><br>​<br>
ln -s /export/server/apache-hive-3.1.3-bin /export/server/hive<br>驱动jar包，放入：Hive安装文件夹的lib目录内<br><img alt="image-20240110231952910" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110231952910.png" referrerpolicy="no-referrer"><br>cd /export/server/MySQL/<br>
cp mysql-connector-java-5.1.48.jar /export/server/hive/lib/<br><br><br>export HADOOP_HOME=/export/server/hadoop<br>
export HIVE_CONF_DIR=/export/server/hive/conf<br>
export HIVE_AUX_JARS_PATH=/export/server/hive/lib<br><img alt="image-20240110232137368" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110232137368.png" referrerpolicy="no-referrer"><br><br><br><br>​<br>
<br>
<br>
javax.jdo.option.ConnectionURL<br>
jdbc:mysql://hadoop1:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8<br>
<br>
​<br>
<br>
javax.jdo.option.ConnectionDriverName<br>
com.mysql.jdbc.Driver<br>
<br>
​<br>
<br>
javax.jdo.option.ConnectionUserName<br>
root<br>
<br>
​<br>
<br>
javax.jdo.option.ConnectionPassword<br>
123456<br>
<br>
​<br>
<br>
hive.server2.thrift.bind.host<br>
hadoop1<br>
<br>
​<br>
<br>
hive.metastore.uris<br>
thrift://hadoop1:9083<br>
<br>
​<br>
<br>
hive.metastore.event.db.notification.api.auth<br>
false<br>
<br>
​<br>
<br>
hive.metastore.warehouse.dir<br>
/user/hive/warehouse<br>
<br>
​<br>
<br>
​<br><img alt="image-20240110232204458" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110232204458.png" referrerpolicy="no-referrer"><br><br>mysql操作<br> mysql -uroot -p123456<br>
mysql&gt; create database hive;<br>
mysql&gt; quit;<br>hive操作<br>cd /export/server/hive<br>
bin/schematool -initSchema -dbType mysql -verbos<br><br><a href=".?query=tag:先启动metastore服务" class="tag" target="_blank" rel="noopener">#先启动metastore服务</a> 然后启动hiveserver2服务•<br>
nohup bin/hive --service metastore &gt;&gt; logs/metastore.log 2&gt;&amp;1 &amp;<br>
nohup bin/hive --service hiveserver2 &gt;&gt; logs/hiveserver2.log 2&gt;&amp;1 &amp;<br><br><br>/export/server/hive/bin/beeline<br>
Beeline version 3.1.2 by Apache Hive<br>
beeline&gt; ! connect jdbc:hive2://hadoop1:10000<br>
Connecting to jdbc:hive2://hadoop1:10000<br>
Enter username for jdbc:hive2://hadoop1:10000: root<br>
Enter password for jdbc:hive2://hadoop1:10000: (可以不写)<br>
Connected to: Apache Hive (version 3.1.2)<br>
Driver: Hive JDBC (version 3.1.2)<br>
Transaction isolation: TRANSACTION_REPEATABLE_READ<br>
0: jdbc:hive2://hadoop1:10000&gt; <br><br><img alt="image-20240111131216192" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240111131216192.png" referrerpolicy="no-referrer"><br><img alt="image-20240111131447469" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240111131447469.png" referrerpolicy="no-referrer"><br><br><br><br>  
 &nbsp; &nbsp;hive.cli.print.header  
 &nbsp; &nbsp;true  
 &nbsp; &nbsp;Whether to print the names of the columns in query output.  
  
  
 &nbsp; &nbsp;hive.cli.print.current.db  
 &nbsp; &nbsp;true  
 &nbsp; &nbsp;Whether to include the current database in the Hive prompt.  
  
​<br><br>cd /export/server/hive/conf<br>
mv hive-log4j2.properties.template hive-log4j2.properties<br>
vim hive-log4j2.properties<br>
/export/server/hive/logs<br><img alt="image-20240111130419540" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240111130419540.png" referrerpolicy="no-referrer"><br><br>vim hive-env.sh<br>
export HADOOP_HEAPSIZE=2048<br><img alt="image-20240111130755760" src="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240111130755760.png" referrerpolicy="no-referrer"><br><br>cd /export/server/hadoop/etc/hadoop/<br>
vim yarn-site.xml<br>
​<br>
<br>
yarn.nodemanager.vmem-check-enabled<br>
false<br>
<br>
​<br>
分发<br>
scp yarn-site.xml  hadoop2:pwd<br>
scp yarn-site.xml  hadoop3:pwd<br><br>cd /export/server/hadoop/sbin<br>
vim Start-All.sh<br>#!/bin/bash<br>
​  <br><br>echo "DFS启动"<br>
start-dfs.sh<br>
sleep 15<br>
​  <br><br>echo "YARN启动"<br>
start-yarn.sh<br>
sleep 15<br>
​  <br><br><br>echo "Metastore启动"<br>
hive --service metastore &amp;<br>
sleep 1<br>
​  <br><br>echo "HiveServer2启动"<br>
hive --service hiveserver2 &amp;<br>
sleep 1<br>
​  <br><br><br>hadoop fs -chmod -R 777 /<br>
echo "HdfsWeb端权限赋予完成"<br>
​  <br><br>echo "hadoop1ZooKeeper启动"<br>
zkServer.sh start<br>
sleep 10<br>
​  <br><br>echo "hadoop2上的ZooKeeper启动"<br>
​<br>
ssh hadoop2 "zkServer.sh start"<br>
echo "hadoop3上的ZooKeeper启动"<br>
ssh hadoop3 "zkServer.sh start"<br>
​<br>
​<br>
echo "所有服务已启动完成。"<br>
jps<br>chmod +x Start-All.sh<br><br>cd /export/server/hadoop/sbin<br>
vim Stop-All.sh<br>#!/bin/bash<br>
​<br>
echo "开始关闭hive"  <br><br>runjar_pids=runjar_pids" ]; then<br>
echo "没有发现RunJar进程在运行。"<br>
else<br>
for pid in pid"<br>
kill -15 $pid &nbsp;# 先尝试优雅地终止进程<br>
sleep 1 &nbsp; &nbsp; &nbsp; # 等待几秒，给进程时间响应<br>
kill -9 $pid &nbsp;# 如果进程仍然运行，强制终止<br>
done<br>
fi<br>
​<br>
sleep 5<br>
​<br>
echo "关闭全部zk"  <br><br>zkServer.sh stop<br>
ssh hadoop2 "zkServer.sh stop"<br>
ssh hadoop3 "zkServer.sh stop"<br>
​<br>
sleep 5<br>
​<br>
echo "关闭hdfs"  <br><br>stop-all.sh<br>
​<br>
echo "所有服务全部关闭"<br>
jps<br>chmod +x Stop-All.sh<br><br>&lt;&lt; <a data-href="2024-01-09" href="\2024-01-09" class="internal-link" target="_self" rel="noopener">2024-01-09</a> | <a data-href="2024-01-11" href="\日记\2024-01-11.html" class="internal-link" target="_self" rel="noopener">2024-01-11</a> &gt;&gt;]]></description><link>日记\2024-01-10.html</link><guid isPermaLink="false">日记/2024-01-10.md</guid><pubDate>Thu, 11 Jan 2024 10:25:35 GMT</pubDate><enclosure url="http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110231408175.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;http://typora-yxh.oss-cn-chengdu.aliyuncs.com/img/image-20240110231408175.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2024-01-11]]></title><description><![CDATA[<a class="tag" href="?query=tag:hive" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hive</a> 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：9℃  天气：多云 温度范围：7 ~ 10℃
湿度：89% 风向：东南风 1级 紫外线：无
空气质量：良 PM: 29 日出: 07:42 日落: 18:19<br>一言
 数人世相逢，百年欢笑，能得几回又。  —— 《摸鱼儿·记年时人人何处》 · 何梦桂
<br><br><br><a href=".?query=tag:hive" class="tag" target="_blank" rel="noopener">#hive</a> <br><br><br>Sqlcreate database if not exists test2 comment "Just For Test" location "/abc"  
    with dbproperties ('aaa' = 'abc');  
  
desc database extended test2;  
  
use test2;  
  
create table student  
(  
    id int  
);  
  
alter database test2 set dbproperties ('aaa' = 'xyz');--数据库只能修改这一属性  
desc database extended test2;  
  
drop database test2 cascade;--删库必须加cascade  
  
use test2;  
  
-- 内部表  
create table student  
(  
    id   int comment "id",  
    name string comment "名字"  
) comment "学生"  
    row format delimited fields terminated by '\t'  
        collection items terminated by '_'  
        map keys terminated by ','  
        lines terminated by '\n'  
    location "/stuxxxx"  
    tblproperties ('aaa' = 'cccxxx');  
  
show tables;  
  
desc test2.student;  
desc formatted test2.student;  
  
-- 外部表  
create external table student2  
(  
    id   int comment "id",  
    name string comment "名字"  
) comment "学生"  
    row format delimited fields terminated by '\t'  
        collection items terminated by '_'  
        map keys terminated by ','  
        lines terminated by '\n'  
    location "/stu2"  
    tblproperties ('aaa' = 'cccxxx');  
  
desc formatted test2.student2;  
  
drop table student;  
drop table student2;  
  
-- 内部表的外部表的区别：  
-- 外部表被删除hdfs的数据不会被删除  
-- 内部表被删除hdfs的数据会被删除  
  
-- 将内部表转换为外部表  
alter table student  
    set tblproperties ('EXTERNAL' = 'TRUE');  
-- EXTERNAL true:外部表  FALSE:内部表  
desc formatted test2.student;  
desc formatted test2.student2;  
  
--修改表名  
alter table student2 rename to student;  
  
--修改表的属性 id 为 idxxx string类型 注解“idxxx"  
alter table student change column id idxxx string comment 'idxxx';  
  
--增加  
alter table student add columns (gender string comment '性别');  
  
--代替原来的属性  
alter table student replace columns (id string,name string);
复制<br><br>重点
1.数据库只能修改dbproperties属性 
2.数据库可以通过location 指定目录,但不能是 根目录   -&gt; "/"
3.在日常测试中最好创建外部表 ( external )
4.对与表的修改 增删改查 都需要关键的 column 进行支撑
<br>记录时间: 2024/01/11/6:34 PM 当前天气: 晴 🔆 • 当前温度: 9°C • 体感温度: 9°C<br><br>Sqlcreate table student(id int,name string)  
row format delimited fields terminated by '\t';  
  
  
-- 本地文件导入  
load data local inpath '/export/testall/03_data/student.txt' into table test.student;  
  
-- 这是清楚表数据的语句  
truncate table test.student;  
  
-- hdfs文件导入 overwrite表示覆盖 这个数据是会被移动的而不是复制  
load data inpath '/xyz/student.txt' overwrite into table test.student;  
  
create table stu2  
(  
    id   int,  
    name string  
)  
    row format delimited fields terminated by '\t';  
  
-- 数据的插入  
  
insert into table stu2 values (1001,'zhangsan'),(1002,'lisi');  
  
-- 从另外一个表中插入数据  
insert into stu2 select id,name from test.student s where id&lt;1010;  
  
-- 从另外一个表中覆盖数据  
insert overwrite table stu2 select id,name from test.student s where id&gt;1010;  
  
-- 创建表的时候插入数据  
create table stu3 as select id,name from test.student s where id&gt;1010;  
  
  
-- 创建表的时候复制表结构  
create table stu4 like test.student;  
  
  
-- 导出数据到hdfs文件(未创建)  
export table test.student to '/student_export';
复制<br>特殊

<br>导入数据常用load 和 insert
<br>load：
<br>1. 本地导入：load data local inpath
<br>2. hdfs导入：load data inpath
<br>最好不要从HdfsWeb端进行上传数据，绕过了Hive导致Hive查询不了

<br><br>&lt;&lt; <a data-href="2024-01-10" href="\日记\2024-01-10.html" class="internal-link" target="_self" rel="noopener">2024-01-10</a> | <a data-href="2024-01-12" href="\日记\2024-01-12.html" class="internal-link" target="_self" rel="noopener">2024-01-12</a> &gt;&gt;]]></description><link>日记\2024-01-11.html</link><guid isPermaLink="false">日记/2024-01-11.md</guid><pubDate>Thu, 11 Jan 2024 14:29:20 GMT</pubDate></item><item><title><![CDATA[2024-01-12]]></title><description><![CDATA[<a class="tag" href="?query=tag:hive" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#hive</a> 
 <br><br>
<br>🥳
<br>😭
<br>🥰
<br>😞
<br>🥱
<br>当前温度：14℃  天气：多云 温度范围：8 ~ 15℃
湿度：67% 风向：东南风 2级 紫外线：很弱
空气质量：良 PM: 41 日出: 07:39 日落: 18:15<br>一言
 我没有梦想，但我可以守护别人的梦想！  —— 《假面骑士faiz》 · 乾巧
<br><br><br><a href=".?query=tag:hive" class="tag" target="_blank" rel="noopener">#hive</a>
写sql的一天<br><br>记录时间: 2024/01/12/5:39 AM 当前天气: 晴 🔆 • 当前温度: 12°C • 体感温度: 11°C<br><br>Sql-- select [All | distinct] select_expr ,...;  
-- FROM table_reference           ---从什么表查  
-- [JOIN table_reference on]  xxxx  ---  
-- [where where_condition]          ---过滤  
-- [group by col_list]              ---分组查询  
-- [having col_list]               ---分组后过滤  
-- [order by col_list]              ---排序统称为hive中4个by  
-- [cluster by col_list            ---  
--   |  [DISTRIBUTE by col_list]  [sort by col_list]  
-- ]  
-- [limit number]                   ---限制输出的行数  
-- [union all]  
  
create table if not exists dept  
(  
    deptno int,  
    dname  string,  
    loc    int  
) row format delimited fields terminated by '\t';  
  
create table if not exists emp  
(  
    empno  int,  
    ename  string,  
    job    string,  
    sal    double,  
    deptno int  
) row format delimited fields terminated by '\t';  
  
load data local inpath '/export/testall/03_data/emp.txt' into table emp;  
load data local inpath '/export/testall/03_data/dept.txt' into table dept;  
  
  
-- 查询所有员工的姓名和工资  
select ename as name, sal as gonzi  
from emp;  
  
--  limit 5意思是只显示5条数据  
select *  
from emp  
limit 5;  
  
--  limit 2,3意思是从第2条开始显示3条数据  
select *  
from emp  
limit 2,3;  
  
-- 查询3000到5000之间的员工  
select *  
from emp  
where sal between 3000 and 5000;  
  
-- in 用法 查询工资为800或者5000的员工  
select *  
from emp  
where sal in (800, 5000);  
  
-- not in 用法 查询工资不为800或者5000的员工  
select *  
from emp  
where sal not in (800, 5000);  
  
-- like 用法 查询姓名以 小 开头的员工  
select *  
from emp  
where ename like "小%";  
  
-- rlike 用法 查询姓名以 小 开头的员工 rlike正则表达式 ^小.*  ^表示以什么开头 .*表示任意字符1到多个  
select *  
from emp  
where ename rlike "^小.*";  
  
-- is null 用法 查询没有工资的员工  
select *  
from emp  
where sal is null;  
  
-- is not null 用法 查询有工资的员工  
select *  
from emp  
where sal is not null;  
  
-- 聚合函数  
-- count(*) 用法 查询员工的总数  
select count(*)  
from emp;  
  
-- count(x) 查询员工工资的总数  
select count(sal)  
from emp;  
  
-- sum(x) 查询员工的工资总和  
select sum(sal)  
from emp;  
  
-- avg(x) 查询员工的平均工资  
select avg(sal)  
from emp;  
  
-- max(x) 查询员工的最大工资  
select max(sal)  
from emp;  
  
-- min(x) 查询员工的最小工资  
select min(sal)  
from emp;  
  
select count(*), count(job), sum(sal), avg(sal), max(sal), min(sal)  
from emp  
group by deptno;  
  
-- group by 用法 查询每个部门的员工的总数  
select deptno, count(*)  
from emp  
group by deptno;  
  
-- group by 用法 查询每个部门的员工的总数,并且按照部门号排序  
select deptno, count(*)  
from emp  
group by deptno  
order by deptno;  
  
-- group by 用法 查询每个部门的员工的总数,并且按照部门号排序,并且按照总数排序  
select deptno, count(*)  
from emp  
group by deptno  
order by deptno, count(*) desc;  
  
-- group by 用法 查询每个部门的员工的总数,并且按照部门号排序,并且按照总数排序,并且只显示前3条  
select deptno, count(*)  
from emp  
group by deptno  
order by deptno, count(*) desc  
limit 3;  
  
-- group by 用法 查询每个部门的员工的总数,并且按照部门号排序,并且按照总数排序,并且只显示前3条,并且只显示部门号和总数  
select deptno, count(*)  
from emp  
group by deptno  
order by deptno, count(*) desc  
limit 3;  
  
-- group by 用法 查询每个部门的员工的总数,并且按照部门号排序,并且按照总数排序,并且只显示前3条,并且只显示部门号和总数,并且部门号为deptno  
select deptno, count(*) as num  
from emp  
group by deptno  
order by deptno, count(*) desc  
limit 3;  
  
-- having 用法 查询每个部门的员工的平均工资大于2000的部门号  
select deptno  
from emp  
group by deptno  
having avg(sal) &gt; 2000;  
  
-- having 用法 查询每个部门的员工的平均工资大于2000的部门号,并且按照部门号排序  
select deptno  
from emp  
group by deptno  
having avg(sal) &gt; 2000  
order by deptno;  
  
-- join 用法 查询每个员工的姓名和部门号  
select e.ename, d.deptno  
from emp e  
         join dept d on e.deptno = d.deptno;  
  
-- join 用法 查询每个员工的姓名和部门号,并且按照部门号排序  
select e.ename, d.deptno  
from emp e  
         join dept d on e.deptno = d.deptno  
order by d.deptno;  
  
select e.*, d.*  
from emp e  
         join dept d on e.deptno = d.deptno;  
  
-- 内连接 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。  
select e.empno,  
       e.ename,  
       d.deptno  
from emp e  
         join dept d  
              on e.deptno = d.deptno;  
  
-- 左外连接 左外连接：join操作符左边表中符合where子句的所有记录将会被返回。  
select e.empno,  
       e.ename,  
       d.deptno  
from emp e  
         left join dept d  
                   on e.deptno = d.deptno;  
  
-- 右外连接 右外连接：join操作符右边表中符合where子句的所有记录将会被返回。  
select e.empno,  
       e.ename,  
       d.deptno  
from emp e  
         right join dept d  
                    on e.deptno = d.deptno;  
  
-- 满外连接 满外连接：将会返回所有表中符合where语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用null值替代。  
select e.empno,  
       e.ename,  
       d.deptno  
from emp e  
         full join dept d  
                   on e.deptno = d.deptno;  
  
  
-- 多表连接  
  
create table if not exists location  
(  
    loc      int,   -- 部门位置id  
    loc_name string -- 部门位置  
)  
    row format delimited fields terminated by '\t';  
  
load data local inpath '/export/testall/03_data/location.txt' into table location;  
  
select e.ename,  
       d.dname,  
       l.loc_name  
from emp e  
         join dept d  
              on d.deptno = e.deptno  
         join location l  
              on d.loc = l.loc;  
  
-- 笛卡尔集  两个表的所有数据都会显示出来  
select empno,  
       dname  
from emp,  
     dept;  
  
--将员工表30部门的员工信息和40部门的员工信息，利用union进行拼接显示。  
select *  
from emp  
where deptno = 30  
union  
select *  
from emp  
where deptno = 40  
order by deptno;  
  
-- Sort By  根据部门编号降序查看员工信息  
select  
    *  
from emp  
sort by deptno desc;  
  
-- 将查询结果导入到文件中（按照部门编号降序排序）  
insert overwrite local directory '/export/testall/03_data/sortby-result'  
 select * from emp sort by deptno desc;
复制<br><br>Sqlset mapreduce.framework.name;  
set mapreduce.framework.name=yarn;  
set hive.stats.column.autogather=false;  
set mapreduce.input.fileinputformat.split.maxsize=10240000;  
  
create database study;  
use study;  
create table if not exists student_info  
(  
    stu_id   string COMMENT '学生id',  
    stu_name string COMMENT '学生姓名',  
    birthday string COMMENT '出生日期',  
    sex      string COMMENT '性别'  
)  
    row format delimited fields terminated by ','  
    stored as textfile;  
create table if not exists course_info  
(  
    course_id   string COMMENT '课程id',  
    course_name string COMMENT '课程名',  
    tea_id      string COMMENT '任课老师id'  
)  
    row format delimited fields terminated by ','  
    stored as textfile;  
create table if not exists teacher_info  
(  
    tea_id   string COMMENT '老师id',  
    tea_name string COMMENT '学生姓名'  
)  
    row format delimited fields terminated by ','  
    stored as textfile;  
create table if not exists score_info  
(  
    stu_id    string COMMENT '学生id',  
    course_id string COMMENT '课程id',  
    score     int COMMENT '成绩'  
)  
    row format delimited fields terminated by ','  
    stored as textfile;  
  
  
load data local inpath '/export/testall/datas/practice/student_info.txt' into table student_info;  
  
load data local inpath '/export/testall/datas/practice/course_info.txt' into table course_info;  
  
load data local inpath '/export/testall/datas/practice/teacher_info.txt' into table teacher_info;  
  
load data local inpath '/export/testall/datas/practice/score_info.txt' into table score_info;  
select *  
from student_info  
limit 5;  
select *  
from course_info  
limit 5;  
select *  
from teacher_info  
limit 5;  
select *  
from score_info  
limit 5;  
  
  
--  查询姓名中带“冰”的学生名单  
select stu_name  
from student_info  
where stu_name like "%冰%";  
  
  
--  查询姓“王”老师的个数  
select count(tea_name) wang_count --别名  
from teacher_info  
where tea_name like "王%";  
  
  
--  检索课程编号为“04”且分数小于60的学生的课程信息，结果按分数降序排列  
select stu_id, course_id, score  
from score_info  
where course_id = '04'  
  and score &lt; 60  
order by score desc;  
  
--  查询数学成绩不及格的学生和其对应的成绩，按照学号升序排序  
select s.stu_id,  
       s.stu_name,  
       t1.score  
from student_info s  
         join (select *  
               from score_info  
               where course_id = (select course_id from course_info where course_name = '数学')  
                 and score &lt; 60) t1 on s.stu_id = t1.stu_id  
order by s.stu_id;  
  
--  查询编号为“02”的课程的总成绩  
select course_id, sum(score) sum_score  
from score_info  
where course_id = '02'  
group by course_id;  
  
--  查询参加考试的学生个数  
select count(distinct stu_id) stu_num  
from score_info;  
  
  
select count(distinct stu_name)  
from student_info;  
  
  
--  查询各科成绩最高和最低的分，以如下的形式显示：课程号，最高分，最低分  
select course_id, max(score), min(score)  
from score_info  
group by course_id;  
  
--  查询每门课程有多少学生参加了考试（有考试成绩）  
select course_id,  
       count(stu_id)  
from score_info  
group by course_id;  
  
--  查询男生、女生人数  
select sex,  
       count(stu_id)  
from student_info  
group by sex;  
  
--  查询平均成绩大于60分的学生的学号和平均成绩  
select stu_id, avg(score) score_avg  
from score_info  
group by stu_id  
having score_avg &gt; 60;  
  
--  查询至少选修四门课程的学生学号  
select stu_id, count(course_id) course_count  
from score_info  
group by stu_id  
having course_count &gt;= 4;  
  
  
--  查询同姓（假设每个学生姓名的第一个字为姓）的学生名单并统计同姓人数大于2的姓  
select t1.first_name,  
       count(*) count_first_name  
from (select stu_id,  
             stu_name,  
             substr(stu_name, 0, 1) first_name  
      from student_info) t1  
group by t1.first_name  
having count_first_name &gt;= 2;  
  
  
--  查询每门课程的平均成绩，结果按平均成绩升序排序，平均成绩相同时，按课程号降序排列  
select course_id, avg(score) avg_score  
from score_info  
group by course_id  
order by avg_score, course_id;  
  
--  统计参加考试人数大于等于15的学科  
select course_id, count(stu_id) count_stu  
from score_info  
group by course_id  
having course_id &gt; 15;  
  
--  查询学生的总成绩并按照总成绩降序排序  
select stu_id,sum(score) sum_score from score_info group by stu_id order by sum_score desc ;  
  
--  按照如下格式显示学生的语文、数学、英语三科成绩，没有成绩的输出为0，按照学生的有效平均成绩降序显示  
select  
  si.stu_id,  
  sum(if(ci.course_name='语文',score,0))  `语文`,  
  sum(if(ci.course_name='数学',score,0))  `数学`,  
  sum(if(ci.course_name='英语',score,0))  `英语`,  
  count(*)  `有效课程数`,  
  avg(si.score)  `平均成绩`  
from  
  score_info si  
join  
  course_info ci  
on  
  si.course_id=ci.course_id  
group by  
  si.stu_id  
order by  
  `平均成绩` desc;  
  
  
--  查询一共参加三门课程且其中一门为语文课程的学生的id和姓名  
select  
    t2.stu_id,  
    s.stu_name  
from (  
         select t1.stu_id  
         from (  
                  select stu_id,  
                         course_id  
                  from score_info  
                  where stu_id in (  
                      select stu_id  
                      from score_info  
                      where course_id = "01"  
                  )  
              ) t1  
         group by t1.stu_id  
         having count(t1.course_id) = 3  
     ) t2  
join student_info s on t2.stu_id = s.stu_id;  
  
  
--  查询所有课程成绩均小于60分的学生的学号、姓名  
select s.stu_id,  
       s.stu_name  
from (  
         select stu_id,  
                sum(if(score &gt;= 60, 1, 0)) flag  
         from score_info  
         group by stu_id  
         having flag = 0  
     ) t1  
         join student_info s on s.stu_id = t1.stu_id;  
  
  
--  查询没有学全所有课的学生的学号、姓名  
select  
    s.stu_id,  
    s.stu_name  
from student_info s  
left join score_info sc on s.stu_id = sc.stu_id  
group by s.stu_id, s.stu_name  
having count(course_id) &lt; (select count(course_id) from course_info);  
  
  
--  查询出只选修了三门课程的全部学生的学号和姓名  
select  
    s.stu_id,  
    s.stu_name  
from student_info s  
join (  
    select  
        stu_id,  
        count(course_id) course_count  
    from score_info  
    group by stu_id  
    having course_count =3  
    ) t1  
on s.stu_id = t1.stu_id;  
  
  
--  查询有两门以上的课程不及格的同学的学号及其平均成绩  
select  
    t1.stu_id,  
    t2.avg_score  
from (  
         select  
             stu_id,  
             sum(if(score &lt; 60,1,0)) flage  
         from score_info  
         group by stu_id  
         having flage &gt;= 2  
) t1  
join (  
    select  
        stu_id,  
        avg(score) avg_score  
    from score_info  
    group by stu_id  
) t2 on t1.stu_id = t2.stu_id;  
  
  
--  查询所有学生的学号、姓名、选课数、总成绩  
select  
    s.stu_id,  
    s.stu_name,  
    count(sc.course_id) count_course,  
    sum(sc.score) sum_score  
from student_info s  
left join score_info sc on s.stu_id = sc.stu_id  
group by s.stu_id,s.stu_name;  
  
  
--  查询平均成绩大于85的所有学生的学号、姓名和平均成绩  
select s.stu_id,  
       s.stu_name,  
       avg(sc.score) avg_score  
from score_info sc  
left join student_info s on s.stu_id = sc.stu_id  
group by s.stu_id, s.stu_name  
having avg_score &gt; 85;  
  
  
--  查询学生的选课情况：学号，姓名，课程号，课程名称  
select  
    s.stu_id,  
    s.stu_name,  
    c.course_id,  
    c.course_name  
from score_info sc  
join course_info c on sc.course_id = c.course_id  
join student_info s on sc.stu_id = s.stu_id;  
  
  
--  查询出每门课程的及格人数和不及格人数  
select  
    c.course_id,  
    c.course_name,  
    t1.`及格人数`,  
    t1.`不及格人数`  
from course_info c  
join (  
    select  
        course_id,  
        sum(if(score &gt;= 60,1,0)) as `及格人数`,  
        sum(if(score &lt; 60,1,0)) as `不及格人数`  
    from score_info  
    group by course_id  
    ) t1 on c.course_id = t1.course_id;  
  
  
--  查询课程编号为03且课程成绩在80分以上的学生的学号和姓名及课程信息  
select  
    s.stu_id,  
    s.stu_name,  
    t1.score,  
    t1.course_id,  
    c.course_name  
from student_info s  
join (  
    select  
        stu_id,  
        score,  
        course_id  
    from score_info  
    where score &gt; 80 and course_id = '03'  
    ) t1  
on s.stu_id = t1.stu_id  
join course_info c on c.course_id = t1.course_id;  
  
  
--  课程编号为"01"且课程分数小于60，按分数降序排列的学生信息  
select  
    s.stu_id,  
    s.stu_name,  
    s.birthday,  
    s.sex,  
    t1.score  
from student_info s  
join (  
    select  
        stu_id,  
        course_id,  
        score  
    from score_info  
    where score &lt; 60 and course_id = '01'  
    ) t1  
on s.stu_id=t1.stu_id  
order by t1.score desc;  
  
  
--  查询所有课程成绩在70分以上的学生的姓名、课程名称和分数，按分数升序排列  
select  
    s.stu_id,  
    s.stu_name,  
    c.course_name,  
    s2.score  
from student_info s  
join (  
    select  
        stu_id,  
        sum(if(score &gt;= 70,0,1)) flage  
    from score_info  
    group by stu_id  
    having flage =0  
    ) t1  
on s.stu_id = t1.stu_id  
left join score_info s2 on s.stu_id = s2.stu_id  
left join course_info c on s2.course_id = c.course_id;  
  
  
--  查询该学生不同课程的成绩相同的学生编号、课程编号、学生成绩  
select  
    sc1.stu_id,  
    sc1.course_id,  
    sc1.score  
from score_info sc1  
join score_info sc2 on sc1.stu_id = sc2.stu_id  
and sc1.course_id &lt;&gt; sc2.course_id  
and sc1.score = sc2.score;  
  
  
--  查询课程编号为“01”的课程比“02”的课程成绩高的所有学生的学号  
select  
    s1.stu_id  
from  
(  
    select  
        sc1.stu_id,  
        sc1.course_id,  
        sc1.score  
    from  score_info sc1  
    where sc1.course_id ='01'  
) s1  
join  
(  
    select  
        sc2.stu_id,  
        sc2.course_id,  
        score  
    from score_info sc2  
    where sc2.course_id ="02"  
)s2  
on s1.stu_id=s2.stu_id  
where s1.score &gt; s2.score;  
  
  
--  查询学过编号为“01”的课程并且也学过编号为“02”的课程的学生的学号、姓名  
select  
    t1.stu_id as `学号`,  
    s.stu_name as `姓名`  
from  
(  
    select  
        stu_id  
    from score_info sc1  
    where sc1.course_id='01'  
    and stu_id in (  
          select  
              stu_id  
          from score_info sc2  
          where sc2.course_id='02'  
          )  
)t1  
join student_info s  
on t1.stu_id = s.stu_id;  
  
  
--  查询学过“李体音”老师所教的所有课的同学的学号、姓名  
select  
    t1.stu_id,  
    si.stu_name  
from  
(  
    select  
        stu_id  
    from score_info si  
    where course_id in  
    (  
        select  
           course_id  
        from course_info c  
        join teacher_info t  
        on c.tea_id = t.tea_id  
        where tea_name='李体音'      --李体音教的所有课程  
    )  
    group by stu_id  
    having count(*)=2       --学习所有课程的学生  
)t1  
left join student_info si  
on t1.stu_id=si.stu_id;  
  
  
--  查询学过“李体音”老师所讲授的任意一门课程的学生的学号、姓名  
select  
    t1.stu_id,  
    si.stu_name  
from  
(  
    select  
        stu_id  
    from score_info si  
    where course_id in  
    (  
        select  
           course_id  
        from course_info c  
        join teacher_info t  
        on c.tea_id = t.tea_id  
        where tea_name='李体音'  
    )  
    group by stu_id  
)t1  
left join student_info si  
on t1.stu_id=si.stu_id;  
  
  
--  查询没学过"李体音"老师讲授的任一门课程的学生姓名  
select  
    stu_id,  
    stu_name  
from student_info  
where stu_id not in  
(  
    select  
        stu_id  
    from score_info si  
    where course_id in  
    (  
        select  
           course_id  
        from course_info c  
        join teacher_info t  
        on c.tea_id = t.tea_id  
        where tea_name='李体音'  
    )  
    group by stu_id  
);  
  
  
--  查询至少有一门课与学号为“001”的学生所学课程相同的学生的学号和姓名  
select  
    si.stu_id,  
    si.stu_name  
from score_info sc  
join student_info si  
on sc.stu_id = si.stu_id  
where sc.course_id in  
(  
    select  
        course_id  
    from score_info  
    where stu_id='001'    --001的课程  
) and sc.stu_id &lt;&gt; '001'  --排除001学生  
group by si.stu_id,si.stu_name;  
  
  
--  按平均成绩从高到低显示所有学生的所有课程的成绩以及平均成绩  
select  
    si.stu_name,  
    ci.course_name,  
    sc.score,  
    t1.avg_score  
from score_info sc  
join student_info si  
on sc.stu_id=si.stu_id  
join course_info ci  
on sc.course_id=ci.course_id  
join  
(  
    select  
        stu_id,  
        avg(score) avg_score  
    from score_info  
    group by stu_id  
)t1  
on sc.stu_id=t1.stu_id  
order by t1.avg_score desc;
复制<br>😭
才会一点点 太TMD难了
<br><br>&lt;&lt; <a data-href="2024-01-11" href="\日记\2024-01-11.html" class="internal-link" target="_self" rel="noopener">2024-01-11</a> | <a data-href="2024-01-13" href="\2024-01-13" class="internal-link" target="_self" rel="noopener">2024-01-13</a> &gt;&gt;]]></description><link>日记\2024-01-12.html</link><guid isPermaLink="false">日记/2024-01-12.md</guid><pubDate>Fri, 12 Jan 2024 09:41:39 GMT</pubDate></item><item><title><![CDATA[1. 反射]]></title><description><![CDATA[ 
 <br><br><br>​	专业的解释（了解一下）：<br>​       是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；<br>​       对于任意一个对象，都能够调用它的任意属性和方法；<br>​       这种动态获取信息以及动态调用对象方法的功能称为Java语言的反射机制。<br>​	通俗的理解：（掌握）<br>
<br>
利用反射创建的对象可以无视修饰符调用类里面的内容

<br>
可以跟配置文件结合起来使用，把要创建的对象信息和方法写在配置文件中。
读取到什么类，就创建什么类的对象
读取到什么方法，就调用什么方法
此时当需求变更的时候不需要修改代码，只要修改配置文件即可。

<br><br>反射都是从class字节码文件中获取的内容。<br>
<br>如何获取class字节码文件的对象
<br>利用反射如何获取构造方法（创建对象）
<br>利用反射如何获取成员变量（赋值，获取值）
<br>利用反射如何获取成员方法（运行）
<br><br>
<br>Class这个类里面的静态方法forName（“全类名”）（最常用）
<br>通过class属性获取  
<br>通过对象获取字节码文件对象
<br>代码示例：<br>Java//1.Class这个类里面的静态方法forName
//Class.forName("类的全类名")： 全类名 = 包名 + 类名
Class clazz1 = Class.forName("com.itheima.reflectdemo.Student");
//源代码阶段获取 --- 先把Student加载到内存中，再获取字节码文件的对象
//clazz 就表示Student这个类的字节码文件对象。
//就是当Student.class这个文件加载到内存之后，产生的字节码文件对象


//2.通过class属性获取
//类名.class
Class clazz2 = Student.class;

//因为class文件在硬盘中是唯一的，所以，当这个文件加载到内存之后产生的对象也是唯一的
System.out.println(clazz1 == clazz2);//true


//3.通过Student对象获取字节码文件对象
Student s = new Student();
Class clazz3 = s.getClass();
System.out.println(clazz1 == clazz2);//true
System.out.println(clazz2 == clazz3);//true
复制<br><br>java文件：就是我们自己编写的java代码。<br>字节码文件：就是通过java文件编译之后的class文件（是在硬盘上真实存在的，用眼睛能看到的）<br>字节码文件对象：当class文件加载到内存之后，虚拟机自动创建出来的对象。<br>​				这个对象里面至少包含了：构造方法，成员变量，成员方法。<br>而我们的反射获取的是什么？字节码文件对象，这个对象在内存中是唯一的。<br><br>规则：<br>​	get表示获取<br>​	Declared表示私有<br>​	最后的s表示所有，复数形式<br>​	如果当前获取到的是私有的，必须要临时修改访问权限，否则无法使用<br><br>代码示例：<br>Javapublic class ReflectDemo2 {
    public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException {
        //1.获得整体（class字节码文件对象）
        Class clazz = Class.forName("com.itheima.reflectdemo.Student");


        //2.获取构造方法对象
        //获取所有构造方法（public）
        Constructor[] constructors1 = clazz.getConstructors();
        for (Constructor constructor : constructors1) {
            System.out.println(constructor);
        }

        System.out.println("=======================");

        //获取所有构造（带私有的）
        Constructor[] constructors2 = clazz.getDeclaredConstructors();

        for (Constructor constructor : constructors2) {
            System.out.println(constructor);
        }
        System.out.println("=======================");

        //获取指定的空参构造
        Constructor con1 = clazz.getConstructor();
        System.out.println(con1);

        Constructor con2 = clazz.getConstructor(String.class,int.class);
        System.out.println(con2);

        System.out.println("=======================");
        //获取指定的构造(所有构造都可以获取到，包括public包括private)
        Constructor con3 = clazz.getDeclaredConstructor();
        System.out.println(con3);
        //了解 System.out.println(con3 == con1);
        //每一次获取构造方法对象的时候，都会新new一个。

        Constructor con4 = clazz.getDeclaredConstructor(String.class);
        System.out.println(con4);
    }
}
复制<br><br>涉及到的方法：newInstance<br>代码示例：<br>Java//首先要有一个javabean类
public class Student {
    private String name;

    private int age;


    public Student() {

    }

    public Student(String name) {
        this.name = name;
    }

    private Student(String name, int age) {
        this.name = name;
        this.age = age;
    }


    /**
     * 获取
     * @return name
     */
    public String getName() {
        return name;
    }

    /**
     * 设置
     * @param name
     */
    public void setName(String name) {
        this.name = name;
    }

    /**
     * 获取
     * @return age
     */
    public int getAge() {
        return age;
    }

    /**
     * 设置
     * @param age
     */
    public void setAge(int age) {
        this.age = age;
    }

    public String toString() {
        return "Student{name = " + name + ", age = " + age + "}";
    }
}



//测试类中的代码：
//需求1：
//获取空参，并创建对象

//1.获取整体的字节码文件对象
Class clazz = Class.forName("com.itheima.a02reflectdemo1.Student");
//2.获取空参的构造方法
Constructor con = clazz.getConstructor();
//3.利用空参构造方法创建对象
Student stu = (Student) con.newInstance();
System.out.println(stu);


System.out.println("=============================================");


//测试类中的代码：
//需求2：
//获取带参构造，并创建对象
//1.获取整体的字节码文件对象
Class clazz = Class.forName("com.itheima.a02reflectdemo1.Student");
//2.获取有参构造方法
Constructor con = clazz.getDeclaredConstructor(String.class, int.class);
//3.临时修改构造方法的访问权限（暴力反射）
con.setAccessible(true);
//4.直接创建对象
Student stu = (Student) con.newInstance("zhangsan", 23);
System.out.println(stu);
复制<br><br>规则：<br>​	get表示获取<br>​	Declared表示私有<br>​	最后的s表示所有，复数形式<br>​	如果当前获取到的是私有的，必须要临时修改访问权限，否则无法使用<br>方法名：<br><br>代码示例：<br>Javapublic class ReflectDemo4 {
    public static void main(String[] args) throws ClassNotFoundException, NoSuchFieldException {
        //获取成员变量对象

        //1.获取class对象
        Class clazz = Class.forName("com.itheima.reflectdemo.Student");

        //2.获取成员变量的对象（Field对象)只能获取public修饰的
        Field[] fields1 = clazz.getFields();
        for (Field field : fields1) {
            System.out.println(field);
        }

        System.out.println("===============================");

        //获取成员变量的对象（public + private）
        Field[] fields2 = clazz.getDeclaredFields();
        for (Field field : fields2) {
            System.out.println(field);
        }

        System.out.println("===============================");
        //获得单个成员变量对象
        //如果获取的属性是不存在的，那么会报异常
        //Field field3 = clazz.getField("aaa");
        //System.out.println(field3);//NoSuchFieldException

        Field field4 = clazz.getField("gender");
        System.out.println(field4);

        System.out.println("===============================");
        //获取单个成员变量（私有）
        Field field5 = clazz.getDeclaredField("name");
        System.out.println(field5);

    }
}



public class Student {
    private String name;

    private int age;

    public String gender;

    public String address;


    public Student() {
    }

    public Student(String name, int age, String address) {
        this.name = name;
        this.age = age;
        this.address = address;
    }


    public Student(String name, int age, String gender, String address) {
        this.name = name;
        this.age = age;
        this.gender = gender;
        this.address = address;
    }

    /**
     * 获取
     * @return name
     */
    public String getName() {
        return name;
    }

    /**
     * 设置
     * @param name
     */
    public void setName(String name) {
        this.name = name;
    }

    /**
     * 获取
     * @return age
     */
    public int getAge() {
        return age;
    }

    /**
     * 设置
     * @param age
     */
    public void setAge(int age) {
        this.age = age;
    }

    /**
     * 获取
     * @return gender
     */
    public String getGender() {
        return gender;
    }

    /**
     * 设置
     * @param gender
     */
    public void setGender(String gender) {
        this.gender = gender;
    }

    /**
     * 获取
     * @return address
     */
    public String getAddress() {
        return address;
    }

    /**
     * 设置
     * @param address
     */
    public void setAddress(String address) {
        this.address = address;
    }

    public String toString() {
        return "Student{name = " + name + ", age = " + age + ", gender = " + gender + ", address = " + address + "}";
    }
}

复制<br><br><br>代码示例：<br>Javapublic class ReflectDemo5 {
    public static void main(String[] args) throws ClassNotFoundException, NoSuchFieldException, IllegalAccessException {
        Student s = new Student("zhangsan",23,"广州");
        Student ss = new Student("lisi",24,"北京");

        //需求：
        //利用反射获取成员变量并获取值和修改值

        //1.获取class对象
        Class clazz = Class.forName("com.itheima.reflectdemo.Student");

        //2.获取name成员变量
        //field就表示name这个属性的对象
        Field field = clazz.getDeclaredField("name");
        //临时修饰他的访问权限
        field.setAccessible(true);

        //3.设置(修改)name的值
        //参数一：表示要修改哪个对象的name？
        //参数二：表示要修改为多少？
        field.set(s,"wangwu");

        //3.获取name的值
        //表示我要获取这个对象的name的值
        String result = (String)field.get(s);

        //4.打印结果
        System.out.println(result);

        System.out.println(s);
        System.out.println(ss);

    }
}


public class Student {
    private String name;
    private int age;
    public String gender;
    public String address;


    public Student() {
    }

    public Student(String name, int age, String address) {
        this.name = name;
        this.age = age;
        this.address = address;
    }


    public Student(String name, int age, String gender, String address) {
        this.name = name;
        this.age = age;
        this.gender = gender;
        this.address = address;
    }

    /**
     * 获取
     * @return name
     */
    public String getName() {
        return name;
    }

    /**
     * 设置
     * @param name
     */
    public void setName(String name) {
        this.name = name;
    }

    /**
     * 获取
     * @return age
     */
    public int getAge() {
        return age;
    }

    /**
     * 设置
     * @param age
     */
    public void setAge(int age) {
        this.age = age;
    }

    /**
     * 获取
     * @return gender
     */
    public String getGender() {
        return gender;
    }

    /**
     * 设置
     * @param gender
     */
    public void setGender(String gender) {
        this.gender = gender;
    }

    /**
     * 获取
     * @return address
     */
    public String getAddress() {
        return address;
    }

    /**
     * 设置
     * @param address
     */
    public void setAddress(String address) {
        this.address = address;
    }

    public String toString() {
        return "Student{name = " + name + ", age = " + age + ", gender = " + gender + ", address = " + address + "}";
    }
}

复制<br><br>规则：<br>​	get表示获取<br>​	Declared表示私有<br>​	最后的s表示所有，复数形式<br>​	如果当前获取到的是私有的，必须要临时修改访问权限，否则无法使用<br><br>代码示例：<br>Javapublic class ReflectDemo6 {
    public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException {
        //1.获取class对象
        Class&lt;?&gt; clazz = Class.forName("com.itheima.reflectdemo.Student");


        //2.获取方法
        //getMethods可以获取父类中public修饰的方法
        Method[] methods1 = clazz.getMethods();
        for (Method method : methods1) {
            System.out.println(method);
        }

        System.out.println("===========================");
        //获取所有的方法（包含私有）
        //但是只能获取自己类中的方法
        Method[] methods2 = clazz.getDeclaredMethods();
        for (Method method : methods2) {
            System.out.println(method);
        }

        System.out.println("===========================");
        //获取指定的方法（空参）
        Method method3 = clazz.getMethod("sleep");
        System.out.println(method3);

        Method method4 = clazz.getMethod("eat",String.class);
        System.out.println(method4);

        //获取指定的私有方法
        Method method5 = clazz.getDeclaredMethod("playGame");
        System.out.println(method5);
    }
}

复制<br><br>方法<br> Object invoke(Object obj, Object... args) ：运行方法<br>参数一：用obj对象调用该方法<br>参数二：调用方法的传递的参数（如果没有就不写）<br>返回值：方法的返回值（如果没有就不写）<br>代码示例：<br>Javapackage com.itheima.a02reflectdemo1;

import java.lang.reflect.InvocationTargetException;
import java.lang.reflect.Method;

public class ReflectDemo6 {
    public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException, InvocationTargetException, IllegalAccessException {
        //1.获取字节码文件对象
        Class clazz = Class.forName("com.itheima.a02reflectdemo1.Student");
		
        //2.获取一个对象
        //需要用这个对象去调用方法
        Student s = new Student();
        
        //3.获取一个指定的方法
        //参数一：方法名
        //参数二：参数列表，如果没有可以不写
        Method eatMethod = clazz.getMethod("eat",String.class);
        
        //运行
        //参数一：表示方法的调用对象
        //参数二：方法在运行时需要的实际参数
        //注意点：如果方法有返回值，那么需要接收invoke的结果
        //如果方法没有返回值，则不需要接收
        String result = (String) eatMethod.invoke(s, "重庆小面");
        System.out.println(result);

    }
}



public class Student {
    private String name;
    private int age;
    public String gender;
    public String address;


    public Student() {

    }

    public Student(String name) {
        this.name = name;
    }

    private Student(String name, int age) {
        this.name = name;
        this.age = age;
    }

    /**
     * 获取
     * @return name
     */
    public String getName() {
        return name;
    }

    /**
     * 设置
     * @param name
     */
    public void setName(String name) {
        this.name = name;
    }

    /**
     * 获取
     * @return age
     */
    public int getAge() {
        return age;
    }

    /**
     * 设置
     * @param age
     */
    public void setAge(int age) {
        this.age = age;
    }

    public String toString() {
        return "Student{name = " + name + ", age = " + age + "}";
    }

    private void study(){
        System.out.println("学生在学习");
    }

    private void sleep(){
        System.out.println("学生在睡觉");
    }

    public String eat(String something){
        System.out.println("学生在吃" + something);
        return "学生已经吃完了，非常happy";
    }
}
复制<br><br>​	你觉得反射好不好？好，有两个方向<br>​	第一个方向：无视修饰符访问类中的内容。但是这种操作在开发中一般不用，都是框架底层来用的。<br>​	第二个方向：反射可以跟配置文件结合起来使用，动态的创建对象，动态的调用方法。<br><br>理解：（掌握）<br>​	集合中的泛型只在java文件中存在，当编译成class文件之后，就没有泛型了。<br>代码示例：（了解）<br>Javapackage com.itheima.reflectdemo;

import java.lang.reflect.InvocationTargetException;
import java.lang.reflect.Method;
import java.util.ArrayList;

public class ReflectDemo8 {
    public static void main(String[] args) throws NoSuchMethodException, InvocationTargetException, IllegalAccessException {
        //1.创建集合对象
        ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;();
        list.add(123);
//        list.add("aaa");

        //2.利用反射运行add方法去添加字符串
        //因为反射使用的是class字节码文件

        //获取class对象
        Class clazz = list.getClass();

        //获取add方法对象
        Method method = clazz.getMethod("add", Object.class);

        //运行方法
        method.invoke(list,"aaa");

        //打印集合
        System.out.println(list);
    }
}

复制<br><br>在这个练习中，我需要你掌握的是字符串不能修改的真正原因。<br>字符串，在底层是一个byte类型的字节数组，名字叫做value<br>Javaprivate final byte[] value;
复制<br>真正不能被修改的原因：final和private<br>final修饰value表示value记录的地址值不能修改。<br>private修饰value而且没有对外提供getvalue和setvalue的方法。所以，在外界不能获取或修改value记录的地址值。<br>如果要强行修改可以用反射：<br>代码示例：（了解）<br>JavaString s = "abc";
String ss = "abc";
// private final byte[] value= {97,98,99};
// 没有对外提供getvalue和setvalue的方法，不能修改value记录的地址值
// 如果我们利用反射获取了value的地址值。
// 也是可以修改的，final修饰的value
// 真正不可变的value数组的地址值，里面的内容利用反射还是可以修改的，比较危险

//1.获取class对象
Class clazz = s.getClass();

//2.获取value成员变量（private）
Field field = clazz.getDeclaredField("value");
//但是这种操作非常危险
//JDK高版本已经屏蔽了这种操作，低版本还是可以的
//临时修改权限
field.setAccessible(true);

//3.获取value记录的地址值
byte[] bytes = (byte[]) field.get(s);
bytes[0] = 100;

System.out.println(s);//dbc
System.out.println(ss);//dbc
复制<br><br>需求: 利用反射根据文件中的不同类名和方法名，创建不同的对象并调用方法。<br>分析:<br>①通过Properties加载配置文件<br>②得到类名和方法名<br>③通过类名反射得到Class对象<br>④通过Class对象创建一个对象<br>⑤通过Class对象得到方法<br>⑥调用方法<br>代码示例：<br>Javapublic class ReflectDemo9 {
    public static void main(String[] args) throws IOException, ClassNotFoundException, NoSuchMethodException, InvocationTargetException, InstantiationException, IllegalAccessException {
        //1.读取配置文件的信息
        Properties prop = new Properties();
        FileInputStream fis = new FileInputStream("day14-code\\prop.properties");
        prop.load(fis);
        fis.close();
        System.out.println(prop);

        String classname = prop.get("classname") + "";
        String methodname = prop.get("methodname") + "";

        //2.获取字节码文件对象
        Class clazz = Class.forName(classname);

        //3.要先创建这个类的对象
        Constructor con = clazz.getDeclaredConstructor();
        con.setAccessible(true);
        Object o = con.newInstance();
        System.out.println(o);

        //4.获取方法的对象
        Method method = clazz.getDeclaredMethod(methodname);
        method.setAccessible(true);

        //5.运行方法
        method.invoke(o);


    }
}

配置文件中的信息：
classname=com.itheima.a02reflectdemo1.Student
methodname=sleep
复制<br><br>Javapublic class MyReflectDemo {
    public static void main(String[] args) throws IllegalAccessException, IOException {
    /*
        对于任意一个对象，都可以把对象所有的字段名和值，保存到文件中去
    */
       Student s = new Student("小A",23,'女',167.5,"睡觉");
       Teacher t = new Teacher("播妞",10000);
       saveObject(s);
    }

    //把对象里面所有的成员变量名和值保存到本地文件中
    public static void saveObject(Object obj) throws IllegalAccessException, IOException {
        //1.获取字节码文件的对象
        Class clazz = obj.getClass();
        //2. 创建IO流
        BufferedWriter bw = new BufferedWriter(new FileWriter("myreflect\\a.txt"));
        //3. 获取所有的成员变量
        Field[] fields = clazz.getDeclaredFields();
        for (Field field : fields) {
            field.setAccessible(true);
            //获取成员变量的名字
            String name = field.getName();
            //获取成员变量的值
            Object value = field.get(obj);
            //写出数据
            bw.write(name + "=" + value);
            bw.newLine();
        }

        bw.close();

    }
}
复制<br>Javapublic class Student {
    private String name;
    private int age;
    private char gender;
    private double height;
    private String hobby;

    public Student() {
    }

    public Student(String name, int age, char gender, double height, String hobby) {
        this.name = name;
        this.age = age;
        this.gender = gender;
        this.height = height;
        this.hobby = hobby;
    }

    /**
     * 获取
     * @return name
     */
    public String getName() {
        return name;
    }

    /**
     * 设置
     * @param name
     */
    public void setName(String name) {
        this.name = name;
    }

    /**
     * 获取
     * @return age
     */
    public int getAge() {
        return age;
    }

    /**
     * 设置
     * @param age
     */
    public void setAge(int age) {
        this.age = age;
    }

    /**
     * 获取
     * @return gender
     */
    public char getGender() {
        return gender;
    }

    /**
     * 设置
     * @param gender
     */
    public void setGender(char gender) {
        this.gender = gender;
    }

    /**
     * 获取
     * @return height
     */
    public double getHeight() {
        return height;
    }

    /**
     * 设置
     * @param height
     */
    public void setHeight(double height) {
        this.height = height;
    }

    /**
     * 获取
     * @return hobby
     */
    public String getHobby() {
        return hobby;
    }

    /**
     * 设置
     * @param hobby
     */
    public void setHobby(String hobby) {
        this.hobby = hobby;
    }

    public String toString() {
        return "Student{name = " + name + ", age = " + age + ", gender = " + gender + ", height = " + height + ", hobby = " + hobby + "}";
    }
}
复制<br>Javapublic class Teacher {
    private String name;
    private double salary;

    public Teacher() {
    }

    public Teacher(String name, double salary) {
        this.name = name;
        this.salary = salary;
    }

    /**
     * 获取
     * @return name
     */
    public String getName() {
        return name;
    }

    /**
     * 设置
     * @param name
     */
    public void setName(String name) {
        this.name = name;
    }

    /**
     * 获取
     * @return salary
     */
    public double getSalary() {
        return salary;
    }

    /**
     * 设置
     * @param salary
     */
    public void setSalary(double salary) {
        this.salary = salary;
    }

    public String toString() {
        return "Teacher{name = " + name + ", salary = " + salary + "}";
    }
}

复制<br><br><br>​	无侵入式的给方法增强功能<br><br>1，真正干活的对象<br>2，代理对象<br>3，利用代理调用方法<br>切记一点：代理可以增强或者拦截的方法都在接口中，接口需要写在newProxyInstance的第二个参数里。<br><br>Javapublic class Test {
    public static void main(String[] args) {
    /*
        需求：
            外面的人想要大明星唱一首歌
             1. 获取代理的对象
                代理对象 = ProxyUtil.createProxy(大明星的对象);
             2. 再调用代理的唱歌方法
                代理对象.唱歌的方法("只因你太美");
     */
        //1. 获取代理的对象
        BigStar bigStar = new BigStar("鸡哥");
        Star proxy = ProxyUtil.createProxy(bigStar);

        //2. 调用唱歌的方法
        String result = proxy.sing("只因你太美");
        System.out.println(result);
    }
}
复制<br>Java/*
*
* 类的作用：
*       创建一个代理
*
* */
public class ProxyUtil {
    /*
    *
    * 方法的作用：
    *       给一个明星的对象，创建一个代理
    *
    *  形参：
    *       被代理的明星对象
    *
    *  返回值：
    *       给明星创建的代理
    *
    *
    *
    * 需求：
    *   外面的人想要大明星唱一首歌
    *   1. 获取代理的对象
    *      代理对象 = ProxyUtil.createProxy(大明星的对象);
    *   2. 再调用代理的唱歌方法
    *      代理对象.唱歌的方法("只因你太美");
    * */
    public static Star createProxy(BigStar bigStar){
       /* java.lang.reflect.Proxy类：提供了为对象产生代理对象的方法：

        public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h)
        参数一：用于指定用哪个类加载器，去加载生成的代理类
        参数二：指定接口，这些接口用于指定生成的代理长什么，也就是有哪些方法
        参数三：用来指定生成的代理对象要干什么事情*/
        Star star = (Star) Proxy.newProxyInstance(
                ProxyUtil.class.getClassLoader(),//参数一：用于指定用哪个类加载器，去加载生成的代理类
                new Class[]{Star.class},//参数二：指定接口，这些接口用于指定生成的代理长什么，也就是有哪些方法
                //参数三：用来指定生成的代理对象要干什么事情
                new InvocationHandler() {
                    @Override
                    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
                        /*
                        * 参数一：代理的对象
                        * 参数二：要运行的方法 sing
                        * 参数三：调用sing方法时，传递的实参
                        * */
                        if("sing".equals(method.getName())){
                            System.out.println("准备话筒，收钱");
                        }else if("dance".equals(method.getName())){
                            System.out.println("准备场地，收钱");
                        }
                        //去找大明星开始唱歌或者跳舞
                        //代码的表现形式：调用大明星里面唱歌或者跳舞的方法
                        return method.invoke(bigStar,args);
                    }
                }
        );
        return star;
    }
}
复制<br>Javapublic interface Star {
    //我们可以把所有想要被代理的方法定义在接口当中
    //唱歌
    public abstract String sing(String name);
    //跳舞
    public abstract void dance();
}
复制<br>Javapublic class BigStar implements Star {
    private String name;


    public BigStar() {
    }

    public BigStar(String name) {
        this.name = name;
    }

    //唱歌
    @Override
    public String sing(String name){
        System.out.println(this.name + "正在唱" + name);
        return "谢谢";
    }

    //跳舞
    @Override
    public void dance(){
        System.out.println(this.name + "正在跳舞");
    }

    /**
     * 获取
     * @return name
     */
    public String getName() {
        return name;
    }

    /**
     * 设置
     * @param name
     */
    public void setName(String name) {
        this.name = name;
    }

    public String toString() {
        return "BigStar{name = " + name + "}";
    }
}

复制<br><br>动态代理，还可以拦截方法<br>比如：<br>​	在这个故事中，经济人作为代理，如果别人让邀请大明星去唱歌，打篮球，经纪人就增强功能。<br>​	但是如果别人让大明星去扫厕所，经纪人就要拦截，不会去调用大明星的方法。<br>Java/*
* 类的作用：
*       创建一个代理
* */
public class ProxyUtil {
    public static Star createProxy(BigStar bigStar){
        public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h)
        Star star = (Star) Proxy.newProxyInstance(
                ProxyUtil.class.getClassLoader(),
                new Class[]{Star.class},
                new InvocationHandler() {
                    @Override
                    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
                        if("cleanWC".equals(method.getName())){
                            System.out.println("拦截，不调用大明星的方法");
                            return null;
                        }
                        //如果是其他方法，正常执行
                        return method.invoke(bigStar,args);
                    }
                }
        );
        return star;
    }
}
复制<br><br>​	 对add方法进行增强，对remove方法进行拦截，对其他方法不拦截也不增强<br>Javapublic class MyProxyDemo1 {
    public static void main(String[] args) {
        //动态代码可以增强也可以拦截
        //1.创建真正干活的人
        ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;();

        //2.创建代理对象
        //参数一：类加载器。当前类名.class.getClassLoader()
        //                 找到是谁，把当前的类，加载到内存中了，我再麻烦他帮我干一件事情，把后面的代理类，也加载到内存

        //参数二：是一个数组，在数组里面写接口的字节码文件对象。
        //                  如果写了List，那么表示代理，可以代理List接口里面所有的方法，对这些方法可以增强或者拦截
        //                  但是，一定要写ArrayList真实实现的接口
        //                  假设在第二个参数中，写了MyInter接口，那么是错误的。
        //                  因为ArrayList并没有实现这个接口，那么就无法对这个接口里面的方法，进行增强或拦截
        //参数三：用来创建代理对象的匿名内部类
        List proxyList = (List) Proxy.newProxyInstance(
                //参数一：类加载器
                MyProxyDemo1.class.getClassLoader(),
                //参数二：是一个数组，表示代理对象能代理的方法范围
                new Class[]{List.class},
                //参数三：本质就是代理对象
                new InvocationHandler() {
                    @Override
                    //invoke方法参数的意义
                    //参数一：表示代理对象，一般不用（了解）
                    //参数二：就是方法名，我们可以对方法名进行判断，是增强还是拦截
                    //参数三：就是下面第三步调用方法时，传递的参数。
                    //举例1：
                    //list.add("阿玮好帅");
                    //此时参数二就是add这个方法名
                    //此时参数三 args[0] 就是 阿玮好帅
                    //举例2：
                    //list.set(1, "aaa");
                    //此时参数二就是set这个方法名
                    //此时参数三  args[0] 就是 1  args[1]"aaa"
                    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
                        //对add方法做一个增强，统计耗时时间
                        if (method.getName().equals("add")) {
                            long start = System.currentTimeMillis();
                            //调用集合的方法，真正的添加数据
                            method.invoke(list, args);
                            long end = System.currentTimeMillis();
                            System.out.println("耗时时间：" + (end - start));
                            //需要进行返回，返回值要跟真正增强或者拦截的方法保持一致
                            return true;
                        }else if(method.getName().equals("remove") &amp;&amp; args[0] instanceof Integer){
                            System.out.println("拦截了按照索引删除的方法");
                            return null;
                        }else if(method.getName().equals("remove")){
                            System.out.println("拦截了按照对象删除的方法");
                            return false;
                        }else{
                            //如果当前调用的是其他方法,我们既不增强，也不拦截
                            method.invoke(list,args);
                            return null;
                        }
                    }
                }
        );

        //3.调用方法
        //如果调用者是list，就好比绕过了第二步的代码，直接添加元素
        //如果调用者是代理对象，此时代理才能帮我们增强或者拦截

        //每次调用方法的时候，都不会直接操作集合
        //而是先调用代理里面的invoke，在invoke方法中进行判断，可以增强或者拦截
        proxyList.add("aaa");
        proxyList.add("bbb");
        proxyList.add("ccc");
        proxyList.add("ddd");

        proxyList.remove(0);
        proxyList.remove("aaa");


        //打印集合
        System.out.println(list);
    }
}
复制]]></description><link>Java\反射&amp;动态代理.html</link><guid isPermaLink="false">Java/反射&amp;动态代理.md</guid><pubDate>Tue, 19 Mar 2024 12:41:23 GMT</pubDate></item><item><title><![CDATA[ColorfulClock]]></title><description><![CDATA[ 
 <br>Noneconst [date, setDate] = useState(new Date)

useEffect(()=&gt;{
	const timerId = setInterval(()=&gt;{
		setDate(new Date)
	},1000)
	return ()=&gt;{clearInterval(timerId)}
})

//moment.locale('en-us');
moment.locale('zh-cn');
let formatDate = moment().format("dddd-MMMM-D-H-mm-ss-a").split("-")
let secProgress = formatDate[5] / 60
let minProgress = (formatDate[4]) / 60
let hourProgress = (formatDate[3]) / 24
let dayProgress = (formatDate[2]) / 31
//console.log(formatDate[2]/31)

return (
&lt;div id="clock" className="progress-clock"&gt;
	&lt;button className="progress-clock__time-date" data-group="d" type="button"&gt;
		&lt;small data-unit="w"&gt;{formatDate[0]}&lt;/small&gt;&lt;br/&gt;
		&lt;span data-unit="mo"&gt;{formatDate[1]}&lt;/span&gt;
		&lt;span data-unit="d"&gt;{formatDate[2]}&lt;/span&gt;
	&lt;/button&gt;
	&lt;button className="progress-clock__time-digit" data-unit="h" data-group="h" type="button"&gt;{formatDate[3]}&lt;/button&gt;&lt;span className="progress-clock__time-colon"&gt;:&lt;/span&gt;&lt;button className="progress-clock__time-digit" data-unit="m" data-group="m" type="button"&gt;{formatDate[4]}&lt;/button&gt;&lt;span className="progress-clock__time-colon"&gt;:&lt;/span&gt;&lt;button className="progress-clock__time-digit" data-unit="s" data-group="s" type="button"&gt;{formatDate[5]}&lt;/button&gt;
	&lt;span className="progress-clock__time-ampm" data-unit="ap"&gt;{formatDate[6]}&lt;/span&gt;
	&lt;svg className="progress-clock__rings" width="256" height="256" viewBox="0 0 256 256"&gt;
		&lt;defs&gt;
			&lt;linearGradient id="pc-red" x1="1" y1="0.5" x2="0" y2="0.5"&gt;
				&lt;stop offset="0%" stopColor="hsl(343,90%,55%)" /&gt;
				&lt;stop offset="100%" stopColor="hsl(323,90%,55%)" /&gt;
			&lt;/linearGradient&gt;
			&lt;linearGradient id="pc-yellow" x1="1" y1="0.5" x2="0" y2="0.5"&gt;
				&lt;stop offset="0%" stopColor="hsl(43,90%,55%)" /&gt;
				&lt;stop offset="100%" stopColor="hsl(23,90%,55%)" /&gt;
			&lt;/linearGradient&gt;
			&lt;linearGradient id="pc-blue" x1="1" y1="0.5" x2="0" y2="0.5"&gt;
				&lt;stop offset="0%" stopColor="hsl(223,90%,55%)" /&gt;
				&lt;stop offset="100%" stopColor="hsl(203,90%,55%)" /&gt;
			&lt;/linearGradient&gt;
			&lt;linearGradient id="pc-purple" x1="1" y1="0.5" x2="0" y2="0.5"&gt;
				&lt;stop offset="0%" stopColor="hsl(283,90%,55%)" /&gt;
				&lt;stop offset="100%" stopColor="hsl(263,90%,55%)" /&gt;
			&lt;/linearGradient&gt;
		&lt;/defs&gt;
		&lt;g data-units="d"&gt;
			&lt;circle className="progress-clock__ring" cx="128" cy="128" r="74" fill="none" opacity="0.1" stroke="#e13e78" strokeWidth="12" /&gt;
			&lt;circle className="progress-clock__ring-fill" data-ring="mo" cx="128" cy="128" r="74" fill="none" stroke="#e13e78" strokeWidth="12" strokeDasharray="465 465" strokeDashoffset={(1-dayProgress)*465} strokeLinecap="round" transform="rotate(-90,128,128)" /&gt;
		&lt;/g&gt;
		&lt;g data-units="h"&gt;
			&lt;circle className="progress-clock__ring" cx="128" cy="128" r="90" fill="none" opacity="0.1" stroke="#e79742" strokeWidth="12" /&gt;
			&lt;circle className="progress-clock__ring-fill" data-ring="d" cx="128" cy="128" r="90" fill="none" stroke="#e79742" strokeWidth="12" strokeDasharray="565.5 565.5" strokeDashoffset={(1-hourProgress)*565.5} strokeLinecap="round" transform="rotate(-90,128,128)" /&gt;
		&lt;/g&gt;
		&lt;g data-units="m"&gt;
			&lt;circle className="progress-clock__ring" cx="128" cy="128" r="106" fill="none" opacity="0.1" stroke="#4483ec" strokeWidth="12" /&gt;
			&lt;circle className="progress-clock__ring-fill" data-ring="h" cx="128" cy="128" r="106" fill="none" stroke="#4483ec" strokeWidth="12" strokeDasharray="666 666" strokeDashoffset={(1-minProgress)*666} strokeLinecap="round" transform="rotate(-90,128,128)" /&gt;
		&lt;/g&gt;
		&lt;g data-units="s"&gt;
			&lt;circle className="progress-clock__ring" cx="128" cy="128" r="122" fill="none" opacity="0.1" stroke="#8f30eb" strokeWidth="12" /&gt;
			&lt;circle className="progress-clock__ring-fill" data-ring="m" cx="128" cy="128" r="122" fill="none" stroke="#8f30eb" strokeWidth="12" strokeDasharray="766.5 766.5" strokeDashoffset={(1-secProgress)*766.5} strokeLinecap="round" transform="rotate(-90,128,128)" /&gt;
		&lt;/g&gt;
	&lt;/svg&gt;
&lt;/div&gt;

)
复制<br><br>]]></description><link>Template\ReactJS\ColorfulClock.html</link><guid isPermaLink="false">Template/ReactJS/ColorfulClock.md</guid><pubDate>Tue, 17 Oct 2023 14:14:43 GMT</pubDate></item><item><title><![CDATA[react_music]]></title><description><![CDATA[ 
 <br>Noneconst musicid = props.src.trim(" ");
const musicsrc='https://music.163.com/outchain/player?type=0&amp;id='+musicid+'&amp;auto=0&amp;height=240';
return (
	&lt;&gt;
		&lt;iframe
      title="iframe"
      src={musicsrc}
      style={{ width: '100%', border: 0, height: 240 }}
      sandbox="allow-same-origin allow-scripts allow-popups allow-forms"
      scrolling="no"
    &gt;&lt;/iframe&gt;
	&lt;/&gt;
)
复制<br>]]></description><link>Template\ReactJS\react_music.html</link><guid isPermaLink="false">Template/ReactJS/react_music.md</guid><pubDate>Tue, 17 Oct 2023 14:02:55 GMT</pubDate></item><item><title><![CDATA[react_weather]]></title><description><![CDATA[ 
 <br>None//First debut by cuman in 🥑Blue Topaz example 
//2022-05-18
//const isDarkMode = app.vault.getConfig("theme") === "obsidian";
const city = props.src.trim(" ");
let color='&amp;color=707271';
// if (!isDarkMode) color='&amp;color=707271';
const weathersrc='https://tianqiapi.com/api.php?style=ta&amp;skin=pear&amp;fontsize=13&amp;align=&amp;paddingtop=2&amp;paddingleft=15&amp;city='+city+color;

return (
	&lt;&gt;
		&lt;iframe
      title="iframe"
      src={weathersrc}
      style={{marginLeft:'10%',width: 245, border: 0, height: 20 }}
      sandbox="allow-same-origin allow-scripts allow-popups allow-forms"
      scrolling="no"
    &gt;&lt;/iframe&gt;
	&lt;/&gt;
)
复制<br>]]></description><link>Template\ReactJS\react_weather.html</link><guid isPermaLink="false">Template/ReactJS/react_weather.md</guid><pubDate>Tue, 17 Oct 2023 14:23:13 GMT</pubDate></item><item><title><![CDATA[场景]]></title><description><![CDATA[ 
 <br><img alt="234343" src="https://w.wallhaven.cc/full/we/wallhaven-we67wr.jpg" referrerpolicy="no-referrer"><br><img alt="12323" src="https://w.wallhaven.cc/full/3l/wallhaven-3lw1l6.jpg" referrerpolicy="no-referrer">
<img alt="1" src="https://w.wallhaven.cc/full/jx/wallhaven-jxkyk5.jpg" referrerpolicy="no-referrer">
<img alt="2" src="https://w.wallhaven.cc/full/d6/wallhaven-d6kjxg.png" referrerpolicy="no-referrer">
<img alt="3" src="https://w.wallhaven.cc/full/6d/wallhaven-6d2577.png" referrerpolicy="no-referrer">
<img alt="4" src="https://w.wallhaven.cc/full/l8/wallhaven-l8k272.png" referrerpolicy="no-referrer">
<img alt="5" src="https://w.wallhaven.cc/full/gp/wallhaven-gpqor7.jpg" referrerpolicy="no-referrer">
<img alt="6" src="https://w.wallhaven.cc/full/x6/wallhaven-x6jd7d.png" referrerpolicy="no-referrer">
<img alt="7" src="https://w.wallhaven.cc/full/qz/wallhaven-qzg3p7.png" referrerpolicy="no-referrer">
<img alt="8" src="https://w.wallhaven.cc/full/zy/wallhaven-zyogjy.png" referrerpolicy="no-referrer">
<img alt="粉色少女" src="https://w.wallhaven.cc/full/l8/wallhaven-l8wl1q.jpg" referrerpolicy="no-referrer">
<img alt="10" src="https://w.wallhaven.cc/full/vq/wallhaven-vqol7p.png" referrerpolicy="no-referrer">
<img alt="11" src="https://w.wallhaven.cc/full/9d/wallhaven-9dzgy1.png" referrerpolicy="no-referrer">
<img alt="12" src="https://w.wallhaven.cc/full/o5/wallhaven-o51r1p.png" referrerpolicy="no-referrer">
<img alt="13" src="https://w.wallhaven.cc/full/l8/wallhaven-l8zek2.png" referrerpolicy="no-referrer">
<img alt="14" src="https://w.wallhaven.cc/full/p9/wallhaven-p9wkwm.jpg" referrerpolicy="no-referrer">
<img alt="15" src="https://w.wallhaven.cc/full/6d/wallhaven-6d5xex.png" referrerpolicy="no-referrer">
<img alt="16" src="https://w.wallhaven.cc/full/x6/wallhaven-x6z5xd.jpg" referrerpolicy="no-referrer"><br><img alt="绿怪" src="https://s1.wallpapermaiden.com/image/2016/11/08/howl-s-moving-castle-hayao-miyazaki-studio-ghibli-landscape-anime-9018-resized.jpg" referrerpolicy="no-referrer">]]></description><link>Typora\精选壁纸\场景.html</link><guid isPermaLink="false">Typora/精选壁纸/场景.md</guid><pubDate>Tue, 30 May 2023 04:42:29 GMT</pubDate><enclosure url="https://w.wallhaven.cc/full/we/wallhaven-we67wr.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://w.wallhaven.cc/full/we/wallhaven-we67wr.jpg&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[动漫]]></title><description><![CDATA[ 
 <br><img alt="231" src="https://w.wallhaven.cc/full/6d/wallhaven-6d56k6.jpg" referrerpolicy="no-referrer"><br><img alt="12312" src="https://w.wallhaven.cc/full/x6/wallhaven-x67mkz.png" referrerpolicy="no-referrer"><br><img alt="2" src="https://w.wallhaven.cc/full/ex/wallhaven-exe76l.png" referrerpolicy="no-referrer">
<img alt="3" src="https://w.wallhaven.cc/full/kx/wallhaven-kxk1vm.jpg" referrerpolicy="no-referrer">
<img alt="4" src="https://w.wallhaven.cc/full/qz/wallhaven-qz3ejr.jpg" referrerpolicy="no-referrer">
<img alt="5" src="https://w.wallhaven.cc/full/vq/wallhaven-vqolg5.jpg" referrerpolicy="no-referrer">
<img alt="6" src="https://w.wallhaven.cc/full/o5/wallhaven-o518xm.jpg" referrerpolicy="no-referrer">
<img alt="7" src="https://w.wallhaven.cc/full/gp/wallhaven-gpqrr7.jpg" referrerpolicy="no-referrer"><br><img alt="9" src="https://w.wallhaven.cc/full/x6/wallhaven-x6zzql.jpg" referrerpolicy="no-referrer">
<img alt="10" src="https://w.wallhaven.cc/full/3l/wallhaven-3l353d.jpg" referrerpolicy="no-referrer">
<img alt="超可爱" src="https://w.wallhaven.cc/full/x6/wallhaven-x6jj1z.png" referrerpolicy="no-referrer">
12
13<br><img alt="aya.jpg" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/aya.jpg" referrerpolicy="no-referrer">]]></description><link>Typora\精选壁纸\动漫.html</link><guid isPermaLink="false">Typora/精选壁纸/动漫.md</guid><pubDate>Mon, 09 Oct 2023 06:40:59 GMT</pubDate><enclosure url="https://w.wallhaven.cc/full/6d/wallhaven-6d56k6.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://w.wallhaven.cc/full/6d/wallhaven-6d56k6.jpg&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[风景]]></title><description><![CDATA[ 
 <br><img alt="3142" src="https://w.wallhaven.cc/full/o5/wallhaven-o51q1p.jpg" referrerpolicy="no-referrer"><br><img alt="小岛白天" src="https://w.wallhaven.cc/full/yx/wallhaven-yxgmll.png" referrerpolicy="no-referrer">
<img alt="小岛夜晚" src="https://w.wallhaven.cc/full/l8/wallhaven-l8kmop.png" referrerpolicy="no-referrer">
<img alt="1" src="https://w.wallhaven.cc/full/vq/wallhaven-vqy5p8.png" referrerpolicy="no-referrer">
<img alt="2" src="https://w.wallhaven.cc/full/3l/wallhaven-3l53kv.jpg" referrerpolicy="no-referrer">
<img alt="3" src="https://w.wallhaven.cc/full/85/wallhaven-85ddpy.png" referrerpolicy="no-referrer">
<img alt="4" src="https://w.wallhaven.cc/full/yx/wallhaven-yxedxg.jpg" referrerpolicy="no-referrer">
<img alt="5" src="https://w.wallhaven.cc/full/2y/wallhaven-2ydprm.jpg" referrerpolicy="no-referrer">
<img alt="6" src="https://w.wallhaven.cc/full/qz/wallhaven-qz3l7d.jpg" referrerpolicy="no-referrer">
<img alt="7" src="https://w.wallhaven.cc/full/7p/wallhaven-7p9v3o.png" referrerpolicy="no-referrer">
<img alt="8" src="https://w.wallhaven.cc/full/m3/wallhaven-m317k1.jpg" referrerpolicy="no-referrer">
<img alt="9" src="https://w.wallhaven.cc/full/6d/wallhaven-6d57ox.jpg" referrerpolicy="no-referrer">
<img alt="10" src="https://w.wallhaven.cc/full/9d/wallhaven-9dzd21.jpg" referrerpolicy="no-referrer">]]></description><link>Typora\精选壁纸\风景.html</link><guid isPermaLink="false">Typora/精选壁纸/风景.md</guid><pubDate>Tue, 30 May 2023 07:58:56 GMT</pubDate><enclosure url="https://w.wallhaven.cc/full/o5/wallhaven-o51q1p.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://w.wallhaven.cc/full/o5/wallhaven-o51q1p.jpg&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[极简]]></title><description><![CDATA[ 
 <br><img alt="cat" src="https://w.wallhaven.cc/full/vq/wallhaven-vqojj8.jpg" referrerpolicy="no-referrer">
<img alt="2" src="https://w.wallhaven.cc/full/kx/wallhaven-kxk7km.jpg" referrerpolicy="no-referrer">
<img alt="3" src="https://w.wallhaven.cc/full/7p/wallhaven-7pw1we.jpg" referrerpolicy="no-referrer">
<img alt="5" src="https://w.wallhaven.cc/full/ex/wallhaven-ex3ovk.png" referrerpolicy="no-referrer">
<img alt="6" src="https://w.wallhaven.cc/full/qz/wallhaven-qzg2xl.png" referrerpolicy="no-referrer">
<img alt="深色-极简三棱镜" src="https://w.wallhaven.cc/full/1p/wallhaven-1pd22w.png" referrerpolicy="no-referrer">
<img alt="9" src="https://w.wallhaven.cc/full/we/wallhaven-we1q2r.png" referrerpolicy="no-referrer">
<img alt="10" src="https://w.wallhaven.cc/full/d6/wallhaven-d63q8o.png" referrerpolicy="no-referrer">
<img alt="8" src="https://w.wallhaven.cc/full/6d/wallhaven-6d53wx.png" referrerpolicy="no-referrer">
<img alt="11" src="http://desktopwallpapers.org.ua/images/201412/desktopwallpapers.org.ua_38240.jpg" referrerpolicy="no-referrer">
<img alt="12" src="https://s1.wallpapermaiden.com/image/2016/10/04/firewatch-landscape-forest-minimalistic-games-6993.jpg" referrerpolicy="no-referrer">
13
14
15
16]]></description><link>Typora\精选壁纸\极简.html</link><guid isPermaLink="false">Typora/精选壁纸/极简.md</guid><pubDate>Tue, 30 May 2023 04:39:15 GMT</pubDate><enclosure url="https://w.wallhaven.cc/full/vq/wallhaven-vqojj8.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://w.wallhaven.cc/full/vq/wallhaven-vqojj8.jpg&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[童话绘本风]]></title><description><![CDATA[ 
 <br><img alt="1" src="https://files.vlad.studio/joy/bird_bird_bird_bird_xmas/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="3" src="https://files.vlad.studio/joy/merry_xmas_moon/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="4" src="https://files.vlad.studio/joy/starry_night/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="5" src="https://files.vlad.studio/joy/library/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="6" src="https://files.vlad.studio/joy/airlines2/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="书房" src="https://files.vlad.studio/joy/googlelibrary/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="8" src="https://files.vlad.studio/joy/treeofbooks/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="9" src="https://files.vlad.studio/joy/chirp_chirp/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="10" src="https://files.vlad.studio/joy/harvest/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="11" src="https://files.vlad.studio/joy/space_gardener/preview/1280x800.jpg" referrerpolicy="no-referrer"><br><img alt="13" src="https://files.vlad.studio/joy/loneliest_house/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="14" src="https://files.vlad.studio/joy/loneliest_house_night/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="15" src="https://files.vlad.studio/joy/flower/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="16" src="https://files.vlad.studio/joy/umbrella/preview/1280x800.jpg" referrerpolicy="no-referrer"><br><img alt="18" src="https://files.vlad.studio/joy/good_morning_sun/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="19" src="https://files.vlad.studio/joy/jazzband/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="20" src="https://files.vlad.studio/joy/tbilisi_2/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="21" src="https://files.vlad.studio/joy/tbilisi/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="22" src="https://files.vlad.studio/joy/kira/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="23" src="https://files.vlad.studio/joy/blue_and_yellow/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="24" src="https://files.vlad.studio/joy/rainbow_plane/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="25" src="https://files.vlad.studio/joy/magic_of_music_piano/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="26" src="https://files.vlad.studio/joy/magic_of_music_guitar/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="27" src="https://files.vlad.studio/joy/xmas_popup/preview/1280x800.jpg" referrerpolicy="no-referrer"><br><img alt="29" src="https://files.vlad.studio/joy/swipe_swipe_swipe_3/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="30" src="https://files.vlad.studio/joy/halloween_pet/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="31" src="https://files.vlad.studio/joy/papercut/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="32" src="https://files.vlad.studio/joy/celestial_cat_dark/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="33" src="https://files.vlad.studio/joy/space_walk/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="34" src="https://files.vlad.studio/joy/stay_in_touch/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="35" src="https://files.vlad.studio/joy/merry_xmas_moon/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="36" src="https://files.vlad.studio/joy/i_feel_good/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="37" src="https://files.vlad.studio/joy/bookworm/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="38" src="https://files.vlad.studio/joy/hug/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="39" src="https://files.vlad.studio/joy/blank_canvas/preview/1280x800.jpg" referrerpolicy="no-referrer">
<img alt="40" src="https://files.vlad.studio/joy/xmas_windows/preview/1280x800.jpg" referrerpolicy="no-referrer">]]></description><link>Typora\精选壁纸\童话绘本风.html</link><guid isPermaLink="false">Typora/精选壁纸/童话绘本风.md</guid><pubDate>Tue, 30 May 2023 04:05:13 GMT</pubDate><enclosure url="https://files.vlad.studio/joy/bird_bird_bird_bird_xmas/preview/1280x800.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://files.vlad.studio/joy/bird_bird_bird_bird_xmas/preview/1280x800.jpg&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[图片]]></title><description><![CDATA[ 
 <br><img src="https://w.wallhaven.cc/full/p9/wallhaven-p9gvvm.jpg" referrerpolicy="no-referrer">
<img src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/aya.jpg" referrerpolicy="no-referrer">
<img alt="藐视.jpg" src="https://typora-yxh.oss-cn-chengdu.aliyuncs.com/%E8%97%90%E8%A7%86.jpg" referrerpolicy="no-referrer">
<img src="https://w.wallhaven.cc/full/d6/wallhaven-d6d7jl.jpg" referrerpolicy="no-referrer">]]></description><link>Typora\精选壁纸\图片.html</link><guid isPermaLink="false">Typora/精选壁纸/图片.md</guid><pubDate>Fri, 13 Oct 2023 06:42:35 GMT</pubDate><enclosure url="https://w.wallhaven.cc/full/p9/wallhaven-p9gvvm.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://w.wallhaven.cc/full/p9/wallhaven-p9gvvm.jpg&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[正则]]></title><description><![CDATA[<a class="tag" href="?query=tag:python" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#python</a> 
 <br><br><a href=".?query=tag:python" class="tag" target="_blank" rel="noopener">#python</a> <br><br>
<br>
需求
判断一个字符串是否是手机号

<br>
解决
编写一个函数，给函数一个字符串，如果是手机号则返回True，否则返回False

<br>
代码
def isPhone(phone):
    # 长度为11
    # 全部都是数字字符
    # 以1开头
    pass

if isPhone("13812345678"):
    print("是手机号")
else:
    print("不是手机号")
复制

<br>
注意
如果使用正则会让这个问题变得简单

<br><br>概述： 正则表达式，又称规则表达式<br>正则表达式(regular expression)描述了一种字符串匹配的模式（pattern）<br>正则匹配是一个 模糊的匹配(不是精确匹配)<br>re：python自1.5版本开始增加了re模块，该模块提供了perl风格的正则表达式模式，re模块是python语言拥有了所有正则表达式的功能<br>
<br>如下四个方法经常使用

<br>match()
<br>search()
<br>findall()
<br>finditer()


<br><br><br><br><br>锚字符:用来判定是否按照规定开始或者结尾<br><br><br>限定符用来指定正则表达式的一个给定组件必须要出现多少次才能满足匹配。有 * 或 + 或 ? 或 {n} 或 {n,} 或 {n,m} 共6种。<br><br><br>通用flags（修正符）<br><br>通用函数<br>
<br>
获取匹配结果

<br>
使用group()方法 获取到匹配的值

<br>
groups()    返回一个包含所有小组字符串的元组(也就是自存储的值)，从 1 到 所含的小组号。



<br><br>
<br>
原型
def match(pattern, string, flags=0)
复制

<br>
功能
匹配成功返回 匹配的对象  
匹配失败 返回 None

<br>
<br>
获取匹配结果

<br>
使用group()方法 获取到匹配的值

<br>
groups()    返回一个包含所有小组字符串的元组，从 1 到 所含的小组号。



<br>
注意：从第一位开始匹配  只匹配一次

<br>
<br>
参数


<br>
代码
import re

res = re.match('\d{2}','123')
print(res.group())

#给当前匹配到的结果起别名
s = '3G4HFD567'
re.match("(?P&lt;value&gt;\d+)",s)
print(x.group(0))
print(x.group('value'))
复制

<br><br>
<br>
原型
def search(pattern, string, flags=0)
复制

<br>
功能
扫描整个字符串string，并返回第一个pattern模式成功的匹配
匹配失败 返回 None

<br>
参数


<br>
注意：
只要字符串包含就可以
只匹配一次

<br>
示例
import re

res = re.search('[a-z]', '131A3ab889s')
print(res)
print(res.group()
复制

<br>
<br>
注意
与search的区别
相同点：
都只匹配一次
不同点：

<br>search是在要匹配的字符串中  包含正则表达式的内容就可以
<br>match 必须第一位就开始匹配  否则匹配失败


<br><br>
<br>
原型
def findall(pattern, string, flags=0)
复制

<br>
功能
扫描整个字符串string，并返回所有匹配的pattern模式结果的字符串列表

<br>
参数


<br>
示例

myStr = """
&lt;a href="http://www.baidu.com"&gt;百度&lt;/a&gt;
&lt;A href="http://www.taobao.com"&gt;淘宝&lt;/A&gt;
&lt;a href="http://www.id97.com"&gt;电
影网站&lt;/a&gt;
&lt;i&gt;我是倾斜1&lt;/i&gt;
&lt;i&gt;我是倾斜2&lt;/i&gt;
&lt;em&gt;我是倾斜2&lt;/em&gt;
"""
# html里是不区分大小写
# （1）给正则里面匹配的 加上圆括号 会将括号里面的内容进行 单独的返回
res = re.findall("(&lt;a href=\"http://www\.(.*?)\.com\"&gt;(.*?)&lt;/a&gt;)",myStr) #[('&lt;a href="http://www.baidu.com"&gt;百度&lt;/a&gt;', 'baidu', '百度')]

# 括号的区别
res = re.findall("&lt;a href=\"http://www\..*?\.com\"&gt;.*?&lt;/a&gt;",myStr) #['&lt;a href="http://www.baidu.com"&gt;百度&lt;/a&gt;']

#(2) 不区分大小写的匹配
res = re.findall("&lt;a href=\"http://www\..*?\.com\"&gt;.*?&lt;/a&gt;",myStr,re.I) #['&lt;a href="http://www.baidu.com"&gt;百度&lt;/a&gt;', '&lt;A href="http://www.taobao.com"&gt;淘宝&lt;/A&gt;']
res = re.findall("&lt;[aA] href=\"http://www\..*?\.com\"&gt;.*?&lt;/[aA]&gt;",myStr) #['&lt;a href="http://www.baidu.com"&gt;百度&lt;/a&gt;']
# (3) 使.支持换行匹配
res = re.findall("&lt;a href="http://www..?.com"&gt;.?&lt;/a&gt;",myStr,re.S) #
# (4) 支持换行 支持不区分大小写匹配
res = re.findall("&lt;a href="http://www..?.com"&gt;.?&lt;/a&gt;",myStr,re.S|re.I) #
print(res)
复制

<br><br>
<br>
原型
def finditer(pattern, string, flags=0)
复制

<br>
功能
与findall()类似，返回一个迭代器

<br>
参数


<br>
代码
import re

res = re.finditer('\w', '12hsakda1')
print(res)
print(next(res))

for i in res:
    print(i)
复制

<br><br>
<br>
作用：切割字符串

<br>
原型：
def split(patter, string, maxsplit=0, flags=0)
复制

<br>
参数
pattern   正则表达式
string      要拆分的字符串
maxsplit   最大拆分次数  默认拆分全部
flags          修正符

<br>
示例
import re
myStr = "asdas\rd&amp;a\ts12d\n*a3sd@a_1sd"
#通过特殊字符 对其进行拆分 成列表
res = re.split("[^a-z]",myStr)
res = re.split("\W",myStr)
复制

<br><br>
<br>
作用
对正则进行修正

<br>
使用
search/match/findall/finditer 等函数 flags参数的使用

<br>
修正符
re.I   不区分大小写匹配
re.S    使.可以匹配换行符   匹配任意字符

<br>
使用
re.I
print(re.findall('[a-z]','AaBb'))
print(re.findall('[a-z]','AaBb', flags=re.I))
复制
re.S
print(re.findall('&lt;b&gt;.*?&lt;/b&gt;','&lt;b&gt;b标签&lt;/b&gt;'))
print(re.findall('&lt;b&gt;.*?&lt;/b&gt;','&lt;b&gt;b标\n签&lt;/b&gt;', flags=re.S))
复制

<br><br><br>
<br>
概念
处理简单的判断是否匹配之外，正则表达式还有提取子串的功能，用()表示的就是要提取的分组

<br>
代码
#给当前匹配到的结果起别名
s = '3G4HFD567'
re.match("(?P&lt;value&gt;\d+)",s)
print(x.group(0))
print(x.group('value'))
复制

<br>
说明

<br>正则表达式中定义了组，就可以在Match对象上用group()方法提取出子串来
<br>group(0)永远是原始字符串，group(1)、group(2)……表示第1、2、……个子串


<br><br>
<br>
概念
当在python中使用正则表达式时，re模块会做两件事，一件是编译正则表达式，如果表达式的字符串本身不合法，会报错。另一件是用编译好的正则表达式提取匹配字符串

<br>
编译优点
如果一个正则表达式要使用几千遍，每一次都会编译，出于效率的考虑进行正则表达式的编译，就不需要每次都编译了，节省了编译的时间，从而提升效率

<br>
compile()函数

<br>
原型
def compile(pattern, flags=0)
复制

<br>
作用
将pattern模式编译成正则对象

<br>
参数


<br>
flags


<br>
返回值
编译好的正则对象

<br>
示例
import re

re_phone = re.compile(r"(0\d{2,3}-\d{7,8})")
print(re_phone, type(re_phone))
复制



<br>
编译后其他方法的使用
原型
def match(self, string, pos=0, endpos=-1)
def search(self, string, pos=0, endpos=-1)
def findall(self, string, pos=0, endpos=-1)
def finditer(self, string, pos=0, endpos=-1)
复制
参数

示例
s1 = "lucky's phone is 010-88888888"
s2 = "kaige's phone is 010-99999999"
ret1 = re_phone.search(s1)
print(ret1, ret1.group(1))
ret2 = re_phone.search(s2)
print(ret2, ret2.group(1))
复制

<br><br>
<br>
贪婪模式  
贪婪概念：匹配尽可能多的字符

<br>.+  匹配换行符以外的字符至少一次
<br>.*  匹配换行符以外的字符任意次

实例
res = re.search('&lt;b&gt;.+&lt;/b&gt;', '&lt;b&gt;&lt;/b&gt;&lt;b&gt;b标签&lt;/b&gt;')
res = re.search('&lt;b&gt;.*&lt;/b&gt;', '&lt;b&gt;b标签&lt;/b&gt;&lt;b&gt;b标签&lt;/b&gt;&lt;b&gt;b标签&lt;/b&gt;&lt;b&gt;b标签&lt;/b&gt;')
复制

<br>
非贪婪模式
非贪婪概念：尽可能少的匹配称为非贪婪匹配，*?、+?即可

<br>
.+?  匹配换行符以外的字符至少一次  拒绝贪婪

<br>.*?   匹配换行符以外的字符任意次      拒绝贪婪

实例
res = re.search('&lt;b&gt;.+?&lt;/b&gt;', '&lt;b&gt;b标签&lt;/b&gt;&lt;b&gt;b标签&lt;/b&gt;')
res = re.search('&lt;b&gt;.*?&lt;/b&gt;', '&lt;b&gt;b标签&lt;/b&gt;&lt;b&gt;b标签&lt;/b&gt;&lt;b&gt;b标签&lt;/b&gt;&lt;b&gt;b标签&lt;/b&gt;')
复制

<br>练习：<br>
<br>
中信证券
# 将产品名称管理人  风险评级 认购金额 起点公示  信息  全部抓到
复制

<br>
练习抓取股票  每一行数据

<br>
豆瓣
抓取标题和图片img标签

<br>
<br>
<br>
lucky
<a href="\#fnref-1-84222bcfa849571c" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>

]]></description><link>Typora\正则.html</link><guid isPermaLink="false">Typora/正则.md</guid><pubDate>Tue, 17 Oct 2023 03:25:02 GMT</pubDate></item><item><title><![CDATA[markdown的使用说明]]></title><description><![CDATA[ 
 <br><br><br>
语法：# (一级标题)  ## (二级标题)  ### (三级标题) ......
<br>
代码：
# 这是一级标题
## 这是二级标题
复制
<br>
效果:  


<br>
快捷键:

<br>Ctrl+数字1~6可以快速将选中的文本调成对应级别的标题
<br>Ctrl+0可以快速将选中的文本调成普通文本
<br>Ctrl+加号/减号对标题级别进行加减

<br><br><br>
代码:  
这是一个段落
这是一个段落
复制
<br>
效果: 
这是一个段落
这是一个段落
<br><br>
语法:  ---或者***+回车
<br>
代码:
---或者***
复制
<br>
效果:

<br><br><br>
语法:

<br>粗体:  用一对双星号包裹
<br>删除线:  用一对双飘号包裹
<br>下划线:  用一对u标签包裹
<br>斜体:  用一对单星号包裹
<br>高亮:  用一对双等号包裹

<br>
代码:
**这是粗体**
~~这是删除线~~
&lt;u&gt;这是下划线&lt;/u&gt;
*这是斜体*
==这是高亮==
复制
<br>
效果:
这是粗体
这是删除线
这是下划线
这是斜体
这是高亮
<br>
快捷键:

<br>加粗:  Ctrl+B
<br>删除线:  Shift+Alt+5
<br>下划线:  Ctrl+U
<br>斜体:  Ctrl+I

<br><br>
代码:
x^2^
H~2~O
复制
<br>
效果:
x^2^
H~2~O
<br><br><br>
代码:
*/-/+ +空格
复制
<br>
效果:
1.只有同一级别:

<br>
苹果

<br>
香蕉

<br>
橘子
2.子集类:

<br>
一级分类

<br>二级分类 

<br>三级分类





<br>
快捷键:  Ctrl+Shift+]
<br><br>
代码:
数字+.+空格
复制
<br>
效果:

<br>
第一个标题

<br>
第二个标题

<br>
第三个标题

<br>子内容1
<br>子内容2


<br>
第四个标题


<br>
快捷键:  Ctrl+Shift+[
<br><br>
代码:
- [ ] 吃早餐
- [x] 背单词
复制
<br>
效果:

<br>吃早餐
<br>背单词

<br><br>
代码:
&gt;+回车
复制
<br>
效果:

这是最外层区块

这是内层区块





这是最内层区块



<br><br><br>
代码:
`int a=0;`（说明：`位于Esc下面）
复制
<br>
效果:
int a=0;
<br>
快捷键:  Ctrl+Shift+`
<br><br>
代码:
```js/java/c#/text
内容
```
复制
<br>
快捷键:  Ctrl+Shift+K
<br><br>
代码:
www.baidu.com
[百度一下](https://www.baidu.com)
[百度一下](https://www.baidu.com "https://www.baidu.com")
复制
<br>
效果:
<a data-tooltip-position="top" aria-label="http://www.baidu.com" rel="noopener" class="external-link" href="http://www.baidu.com" target="_blank">www.baidu.com</a>
<a data-tooltip-position="top" aria-label="https://www.baidu.com" rel="noopener" class="external-link" href="https://www.baidu.com" target="_blank">百度一下</a>
<a data-tooltip-position="top" aria-label="https://www.baidu.com" rel="noopener" class="external-link" title="https://www.baidu.com" href="https://www.baidu.com" target="_blank">百度一下</a>
<br>
快捷键:  Ctrl+K
<br><br>
说明:  对文本进行解释说明。
<br>
代码: 
[^文本]
[^文本]:解释说明
复制
<br>
效果:
这是一个技术<a href="\#fn-1-84222bcfa849571c" class="footnote-link" target="_self" rel="noopener">[1]</a>
<br><br>
代码:
![不显示的文字](图片路径 "图片标题")
复制
<br>
效果:
![This is a picture](C:\Users\asus\Pictures\Saved Pictures\Snipaste_2020-09-03_13-19-11.png "漫步图")
(注：效果路径为C:\Users\asus\Pictures\Saved Pictures\Snipaste_2020-09-03_13-19-11.png。在其他电脑上可能不显示。)
<br>
快捷键:  Ctrl+Shift+I
<br><br>
代码:
|  1   |  2   |  3   |
| :--- | :--: | ---: |
|  4   |  5   |  6   |
|  7   |  8   |  9   |
|  10  |  11  |  12  |
复制
<br>
效果:

<br>
快捷键:  Ctrl+T
<br><br><br>
代码:
```mermaid
graph LR
A[方形]==&gt;B(圆角)
B==&gt;C{条件a}
C--&gt;|a=1|D[结果1]
C--&gt;|a=2|E[结果2]
F[横向流程图]
```
复制
<br>
效果:

<br><br>
代码:
```mermaid
graph TD
A[方形]==&gt;B(圆角)
B==&gt;C{条件a}
C--&gt;|a=1|D[结果1]
C--&gt;|a=2|E[结果2]
F[竖向流程图]
```
复制
<br>
效果:

<br><br>
代码:
:happy:、:cry:、:man:
复制
<br>
效果:
:happy:、 :cry:、 :man:
<br><br><br><br>
代码:
$公式$
复制
<br>
效果:

<br><br>
代码:
$$
公式
$$
复制
<br>
效果:

<br><br>
代码:
$x^{y^z}=(1+e^x)^{-2xy^w}$
$\sideset{^1_2}{^3_4}{\underset{6}\bigotimes}$
复制
<br>
效果:


<br><br>
代码:
$\langle\quad\rangle\quad\lceil\quad\rceil\quad\lfloor\quad\rfloor\quad\lbrace\quad\rbrace\quad\lVert\quad\rVert$
$f(x,y,z)=3y^2z\left(3+\dfrac{7x+5}{1+y^2}\right)$
$\left.\dfrac{\mathrm{d}u}{\mathrm{d}x}\right|_{x=0}$
复制
<br>
效果:



<br><br>
代码:
$\frac{a}{b}\quad\dfrac{a}{b}\quad {a\over b}$
复制
<br>
效果:

<br><br>
代码:
$\sqrt[根指数,省略时为2]{被开方数}$
复制
<br>
效果:

<br><br>
代码:
$\cdots\quad\ldots\quad\vdots\quad\ddots$
复制
<br>
效果:

<br><br>
代码:
$\overrightarrow{E(\vec{r})}\quad\overleftarrow{E(\vec{r})}\quad\overleftrightarrow{E(\vec{r})}\quad\underrightarrow{E(\vec{r})}\quad\underleftarrow{E(\vec{r})}\quad\underleftrightarrow{E(\vec{r})}\quad\overline{v}=\bar{v}\quad\underline{v}$
复制
<br>
效果:

<br><br>
代码:
$$
\iint\limits_D\left(\dfrac{\partial Q}{\partial x}-\dfrac{\partial P}{\partial y}\right){\rm d}x{\rm d}y=\oint\limits_LP{\rm d}x+Q{\rm d}y
$$
复制
<br>
效果:

<br><br>
代码:
$\lim\limits_{n\to\infin}(1+\dfrac{1}{n})^n=e$
复制
<br>
效果:

<br><br>
$\sum\limits_{i=1}^n\dfrac{1}{n^2}\quad and\quad\prod\limits_{i=1}^n\dfrac{1}{n^2}\quad and\quad\bigcup\limits_{i=1}^n\dfrac{1}{n^2}\quad and\quad\bigcap\limits_{i=1}^n\dfrac{1}{n^2}$
复制
<br>
效果:

<br><br><br><br><br>
可以在字符前使用\large或\small以显示更大或更小的字符。
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
代码:
${\字体{需要转换的字符}}$
复制
<br><br><br>
说明:  使用\left和\right来创建自动匹配高度的()、[]、{}、.。在每个公式末尾使用\tag{行标}来实现行标。
<br>
代码:
$$
f\left(
\left[
\dfrac{1+\{x,y\}}{\left(\dfrac{x}{y}+\dfrac{y}{x}\right)(u+1)}+a
\right]
^{\dfrac{3}{2}}
\right)
\tag{行标}
$$
复制
<br>
效果:

<br>
说明:如果你想将行内显示的分隔符也变大,也可以使用\middle命令
<br>
代码:
$$
\left\langle q\middle\|\dfrac{\dfrac{x}{y}}{\dfrac{u}{v}}\middle|p\right\rangle
$$
复制
<br>
效果:

<br><br><br>
代码:
$\text{文字}$
复制
<br>
效果:

<br><br>

<br>适用新旧浏览器
代码:

$\color{颜色}{文字}$
复制
<br><br>

<br>适用新版浏览器
代码:

$\color{#rgb}{文字}$    (注:其中r、g、b可以输入0~9和a~f来分别表示红色、绿色和蓝色的纯度)
复制
<br><br>
说明:  使用\require{cancle}声明，再使用\cancle{字符}、\bcancle{字符}、\xcancle{字符}、\cancleto{字符}{字符}来实现各种片段删除线效果。
<br>
代码:
$$
\require{cancel}\begin{array}{r1}
\verb|y+\cancel{x}|&amp;y+\cancel{x}\\
\verb|y+\cancel{y+x}|&amp;y+\cancel{y+x}\\
\verb|y+\bcancel{x}|&amp;y+\bcancel{x}\\
\verb|y+\xcancel{x}|&amp;y+\xcancel{x}\\
\verb|y+\cancelto{0}{x}|&amp;y+\cancelto{0}{x}\\
\verb+\frac{1\cancel9}{\cancel95}=\frac15+&amp;\frac{1\cancel9}{\cancel95}=\frac15\\
\end{array}
$$
复制
<br>
效果:

<br>
说明:  使用\require{enclose}来允许整段删除线的显示，再使用\enclose{删除线效果}{字符}来使用各种整段删除线效果。其中，删除线效果有horizontalstrike、verticalstrike、updiagonalstrike和downdiagonalstrike,可以叠加使用。
<br>
代码:
$$
\require{enclose}\begin{array}{r1}
\verb|\enclose{horizontalstrike}{x+y}|&amp;\enclose{horizontalstrike}{x+y}\\
\verb|\enclose{verticalstrike}{\frac xy}|&amp;\enclose{verticalstrike}{\frac xy}\\
\verb|\enclose{updiagonalstrike}{x+y}|&amp;\enclose{updiagonalstrike}{x+y}\\
\verb|\enclose{downdiagonalstrike}{x+y}|&amp;\enclose{downdiagonalstrike}{x+y}\\
\verb|\enclose{horizontalstrike,updiagonalstrike}{x+y}|&amp;\enclose{horizontalstrike,updiagonalstrike}{x+y}\\
\end{array}
$$
复制
<br>
效果:

<br><br><br>
代码:
$$
\begin{matrix}
1&amp;x&amp;x^2\\
1&amp;y&amp;y^2\\
1&amp;z&amp;z^2\\
\end{matrix}
$$
复制
<br>
效果:

<br><br>
说明:  在开头将matrix替换为pmatrix、bmatrix、Bmatrix、vmatrix、Vmatrix。
<br><br><br>
说明:  可以使用cc|c来在一个三列矩阵中插入分割线。
<br>
代码:
$$
\left[
\begin{array}{cc|c}
1&amp;2&amp;3\\
4&amp;5&amp;6
\end{array}
\right]
$$
复制
<br>
效果:

<br><br>
代码:
$\bigl(\begin{smallmatrix}a&amp;b\\c&amp;d\end{smallmatrix}\bigr)$
复制
<br>
效果:

<br><br>
说明:  可以使用\begin{align}...\end{align}来创建一列整齐且默认右对齐的方程式序列。请注意{align}是自动编号的，使用{align*}来声明停止自动编号，也可以使用\notag来取消特定行的自动编号。在需要的时候，你可以使用\begin{equation}...\end{equation}来强制表达式自动编号。
<br>
代码:


$$
\begin{align}
\sqrt{37}=\sqrt{\dfrac{73^2-1}{12^2}}\\
&amp;=\sqrt{\dfrac{73^2}{12^2}\cdot\dfrac{73^2-1}{73^2}}\\
&amp;=\sqrt{\dfrac{73^2}{12^2}}\sqrt{\dfrac{73^2-1}{73^2}}\notag\\
&amp;=\dfrac{73}{12}\sqrt{1-\dfrac{1}{73^2}}\\
\approx\dfrac{73}{12}\left(1-\dfrac{1}{2\cdot73^2}\right)\label{A}
\end{align}
$$
***

$$
\begin{align*}
v+m&amp;=0&amp;\text{Given}\tag1\\
-w&amp;=-w+0&amp;\text{additive identity}\tag2\\
-w+0&amp;=-w+(v+w)&amp;\text{equations $(1)$ and $(2)$}
\end{align*}
$$
复制
<br>
效果:



你可以使用\label{标签}来创建一个标签，就如上面的方程式序列中展示的那样，之后使用\eqref{标签}引用你想引用的公式，效果为：。如果不想要括号，可以输入\ref{标签}，效果为：公式 。
公式1和2的不同列之间存在间隔，如果你不想要，可以通过将align替换为alignat{1}来去除列间隔。
<br><br>
说明:  使用\begin{cases}来创造一组默认左对齐的条件表达式,在每一行插入&amp;来指定需要对齐的内容,并在每一行结尾处使用\\,以\end{cases}结尾。
<br>
代码:
$$
f(n)=
\begin{cases}
n/2,&amp;\text{if $n$ is even}\\
3n+1,&amp;\text{if $n$ is odd}
\end{cases}
$$
复制
<br>
效果:

<br><br>
说明:  可以使用\\[2ex]语句替代该行末尾的\\来让编译器适配 , 其中[ex]指一个"X-Height" , 即x字母高度 , 也可以使用[3ex]或[4ex]等。
<br>
代码:
$$
f(n)=
\begin{cases}
\dfrac n2,&amp;\text{if $n$ is even}\\[2ex]
3n+1,&amp;\text{if $n$ is odd}
\end{cases}\tag{适配[2ex]}
$$
***

$$
f(n)=
\begin{cases}
\dfrac n2,&amp;\text{if $n$ is even}\\
3n+1,&amp;\text{if $n$ is odd}
\end{cases}\tag{不适配[2ex]}
$$
复制
<br>
效果:



<br><br>
说明:  数组与表格均以\begin{array}开头,并在其后定义列数及每一列的文本对齐方式,c l r分别代表居中、左对齐及右对齐。若要插入垂直分割线，在定义中插入|，若要插入水平分割线，在定义中加入\hline。
<br>
代码:
$$
\begin{array}{c|lcr}
n&amp;\text{左对齐}&amp;\text{居中对齐}&amp;\text{右对齐}\\
\hline
1&amp;0.24&amp;1&amp;125\\
2&amp;-1&amp;189&amp;-8\\
3&amp;-20&amp;2000&amp;1+10i
\end{array}
$$
复制
<br>
效果:

<br><br>
代码:
$$
% outer vertical array of arrays 外层垂直表格
\begin{array}{c}
% inner horizontal array of arrays 内层水平表格
\begin{array}{cc}
% inner array of minimum values 内层"最小值"数组
\begin{array}{c|cccc}
\text{min}&amp;0&amp;1&amp;2&amp;3\\
\hline
0&amp;0&amp;0&amp;0&amp;0\\
1&amp;0&amp;1&amp;1&amp;1\\
2&amp;0&amp;1&amp;2&amp;2\\
3&amp;0&amp;1&amp;2&amp;3\\
\end{array}
&amp;
% inner array of maximum values 内层"最大值"数组
\begin{array}{c|cccc}
\text{max}&amp;0&amp;1&amp;2&amp;3\\
\hline
0&amp;0&amp;1&amp;2&amp;3\\
1&amp;1&amp;1&amp;2&amp;3\\
2&amp;2&amp;2&amp;2&amp;3\\
3&amp;3&amp;3&amp;3&amp;3
\end{array}
\end{array}
% 内层第一行表格组结束
\\
% inner array of delta values 内层第二行Delta值数组
\begin{array}{c|cccc}
\Delta&amp;0&amp;1&amp;2&amp;3\\
\hline
0&amp;0&amp;1&amp;2&amp;3\\
1&amp;1&amp;0&amp;1&amp;2\\
2&amp;2&amp;1&amp;0&amp;1\\
3&amp;3&amp;2&amp;1&amp;0
\end{array}
% 内层第二行表格组结束
\end{array}
$$
复制
<br>
效果:

<br><br>
说明:  使用\begin{array}...\end{array}和\left\{...\right.来创建一个方程组,或者你也可以使用条件表达式组\begin{cases}...\end{cases}来实现相同效果。
<br>
代码:
$$
\left\{
\begin{array}{l}
a_1x+b_1y+c_1z=d_1\\
a_2x+b_2y+c_2z=d_2\\
a_3x+b_3y+c_1z=d_3
\end{array}
\right.
\quad\text{或者}\quad
\begin{cases}
a_1x+b_1y+c_1z=d_1\\
a_2x+b_2y+c_2z=d_2\\
a_3x+b_3y+c_1z=d_3
\end{cases}
$$
复制
<br>
效果:

<br><br>
说明:  就像\frac一样,使用\cfrac或\dfrac来创建一个连分式,不要使用普通的\frac或\over来创建,否则看起来会很恶心。
<br>
代码:
$$
x=a_0+\cfrac{1^2}{a_1+\cfrac{2^2}{a_2+\cfrac{3^2}{a_3+\cfrac{4^2}{a_4+\cdots}}}}
$$
复制
<br>
效果:

<br>
反例:
x=a_0+\frac{1^2}{a_1+\frac{2^2}{a_2+\frac{3^2}{a_3+\frac{4^2}{a_4+\cdots}}}}
复制
<br>
效果:

<br>
补充:  当然,你可以使用\frac来表达连分数的紧缩记法。
<br>
代码:
$$
x=a_0+\frac{1^2}{a_1+}\frac{2^2}{a_2+}\frac{3^2}{a_3+}\frac{4^2}{a_4+}\cdots
$$
复制
<br>
效果:

<br><br>
说明:  使用一行$\require{AMScd}$语句来允许交换图表的显示,并通过在开头使用\begin{CD},结尾使用\end{CD}来创建。
<br>
代码:
$$
\require{AMScd}
\begin{CD}
A@&gt;a&gt;&gt;B\\
@VbVV\# @VcVV\\
C @&gt;&gt;d&gt; D
\end{CD}
$$
复制
<br>
效果:
\require{AMScd}
\begin{CD}
A@&gt;a&gt;&gt;B\\
@V b V V\# @VV c V\\
C @&gt;&gt;d&gt; D
\end{CD}
<br>
补充:  其中,@&gt;&gt;&gt;代表右箭头、@&lt;&lt;&lt;代表左箭头、@VVV代表下箭头、@AAA代表上箭头、@=代表水平双实线、@|代表竖直双实线、@.代表没有箭头。在@&gt;&gt;&gt;的&gt;&gt;&gt;之间任意插入文字即代表该箭头的注释文字。
<br>
代码:
$$
\begin{CD}
A@&gt;&gt;&gt;B@&gt;{\text{very long label}}&gt;&gt;C\\
@.@AAA@|\\
D@=E@&lt;&lt;&lt;F
\end{CD}
$$
复制
<br>
效果:

<br><br>

<br>搜索

<br><br><br>
代码
&lt;center&gt;内容&lt;/center&gt;
复制
<br>
效果
内容
<br><br>
代码:
&lt;kbd&gt;内容&lt;/kbd&gt;
复制
<br>
效果:
内容
<br><br>
代码:
&lt;b&gt;加粗&lt;/b&gt;
复制
<br>
效果:
加粗
<br><br>
代码:
&lt;i&gt;倾斜&lt;/i&gt;
复制
<br>
效果:
倾斜
<br><br>
代码:
开始&lt;sup&gt;123hi你好&lt;/sup&gt;
开始&lt;sub&gt;321hi你好&lt;/sub&gt;
复制
<br>
效果:
开始123hi你好
开始321hi你好
<br><br>
代码：
&amp;#x27A4;
复制
<br>
效果：
➤
<br>
<br>
<br>这是一个非常好用的框架。<a href="\#fnref-1-84222bcfa849571c" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
]]></description><link>Typora\Typora.html</link><guid isPermaLink="false">Typora/Typora.md</guid><pubDate>Sat, 14 Oct 2023 12:16:45 GMT</pubDate></item><item><title><![CDATA[typore上传图片配置]]></title><description><![CDATA[ 
 <br>area:oss-cn-chengdu<br>bucket:typora-yxh<br><br>AccessKey Secret: RC0zymL0TYr8Oif5HeI6oNGRQCk3QF<br>TextcustomUrl:http://typora-yxh.oss-cn-chengdu.aliyuncs.com
复制<br>Text{
    "picBed": {
        "uploader": "aliyun",
        "aliyun": {
        "accessKeyId": "",
        "accessKeySecret": "RC0zymL0TYr8Oif5HeI6oNGRQCk3QF",
        "bucket": "typora-yxh",
        "area": "oss-cn-chengdu", 
        "path": "img/", 
        "customUrl": "http://typora-yxh.oss-cn-chengdu.aliyuncs.com",
        "options": "" 
      }
    },
    "picgoPlugins": {}
  }
复制]]></description><link>Typora\typore上传图片配置.html</link><guid isPermaLink="false">Typora/typore上传图片配置.md</guid><pubDate>Tue, 26 Sep 2023 06:27:53 GMT</pubDate></item><item><title><![CDATA[学习初始页面]]></title><description><![CDATA[ 
 ]]></description><link>学习初始页面.html</link><guid isPermaLink="false">学习初始页面.md</guid><pubDate>Wed, 18 Oct 2023 01:41:11 GMT</pubDate></item><item><title><![CDATA[AnuPpuccinMD元素设置对照]]></title><description><![CDATA[<a class="tag" href="?query=tag:test1" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#test1</a> <a class="tag" href="?query=tag:test2" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#test2</a> 
 <br><br>Note
Here's a callout block.
It supports markdown and <a data-tooltip-position="top" aria-label="Internal link" data-href="Internal link" href="\Internal link" class="internal-link" target="_self" rel="noopener">wikilinks</a>.
<br>Abstract<br>Todo<br>Info<br>Tip<br>Success<br>Question<br>Warning<br>Failure<br>Danger<br>Bug<br>Example<br>Quote<br><br>
<br>Unchecked - [ ]
<br>Checked - [x]
<br>Rescheduled - [&gt;]
<br>Scheduled - [&lt;]
<br>Important - [!]
<br>Cancelled - [-]
<br>In Progress - [/]
<br>Quote- ["]
<br>Question - [?]
<br>Star - [*]
<br>Note - [n]
<br>Location - [l]
<br>Information - [i]
<br>Idea - [I]
<br>Amount - [S]
<br>Pro - [p]
<br>Con - [c]
<br>Bookmark - [b]
<br><br>
<br>这是000000
<br>这是11111111111111
<br>这是2 2222222222222222222
<br>这是3 33333333333333333333333333
<br>这是4 4444444444444444444444444444444444
<br>这是5 55555555555555555555555555555555555555555
<br>这是6 666666666666666666666666666666666666666666666666
<br>这是7 7777777777777777777777777777777777777777777777777777777
<br>这8888888888888888888888888888888888888888888888888888888888888888888
<br>这是9 9999999999999999999999999999999999999999999999999999999999999999999999
<br><br>
Obsidian实体是紫色鸡蛋Obsidian实体是紫色鸡蛋Obsidian实体是紫色鸡Obsidian实体是紫色鸡蛋Obsidian实体是紫色鸡蛋Obsidian,实体是紫色鸡蛋Obsidian实体是紫色鸡蛋5555
ssss
5555
<br>
真的，不骗人。
<br><br>Test
let x = 4;

for (x &lt; 12) {x++}

复制<br><br><br>
<br>aaa

<br>aaa

<br>4444
<br>aaa

<br>33333

<br>4444

<br>444




<br>4444 


<br>aaa


<br>5555


<br>55555
<br><br>
<br>aaa
<br>aaa

<br>2222

<br>4444

<br>333
<br>5555


<br>55555


<br>2333


<br>2222

<br>222
<br>222
<br>444


<br>aaa

<br>3333
<br>aaa

<br>aaa




<br>ccc
<br>ddd

<br>aaa

<br>22

<br>33
<br>44


<br>44


<br>bbb


<br><br><br><br><br><br><a href=".?query=tag:test1" class="tag" target="_blank" rel="noopener">#test1</a> <a href=".?query=tag:test2" class="tag" target="_blank" rel="noopener">#test2</a><br><br><br><br><br><br><br><br><br><br><br><br><br><br>Bold Test<br><br>Italic Test<br><br>Highlight Test<br>Pythonclass DzxSpider(scrapy.Spider):
    name = "dzx"
    # allowed_domains = ["duanzixing.com"]
    start_urls = ["https://duanzixing.com"]

    def parse(self, response, **kwargs):
        print(response)
        # 获取响应的内容
        # print(response.text)
        # 当前响应的url
        print('response.url:', response.url)
        # 当前响应对应的请求的url
        print('response.request.url:', response.request.url)
        # 响应头
        print('response.headers:', response.headers)
        # 响应的请求头
        print('response.request.headers:', response.request.headers)
        # 响应体
        print('response.body:', response.body)
        # 返回响应的内容
        print('response.text:', response.text)
        # 响应的状态码
        print('response.status:', response.status)
        # 获取响应的json数据 如果响应的内容不是json格式的数据，会报错
        # print('response.json():', response.json())
        
复制]]></description><link>AnuPpuccinMD元素设置对照.html</link><guid isPermaLink="false">AnuPpuccinMD元素设置对照.md</guid><pubDate>Mon, 18 Mar 2024 15:58:37 GMT</pubDate></item><item><title><![CDATA[Home🏡]]></title><description><![CDATA[ 
 <br><br>花溪 最近 7 天的天气如下，参考天气制定你的计划吧！今天是2024年03月19日，🌧阵雨，5~15℃
云朵充盈了60%的天空
顺便，如果有机会看见月亮的话，那么它应该是这样的🌔<br>今天是 2024年3月19日，2024 年已经过去了 78 天<br>20242023MonWedFriDecFebMarAprMayJunJulAugSepOctNovDec<br><br>]]></description><link>Home🏡.html</link><guid isPermaLink="false">Home🏡.md</guid><pubDate>Mon, 18 Mar 2024 15:58:30 GMT</pubDate></item><item><title><![CDATA[Obsidian插件教程]]></title><description><![CDATA[ 
 <br>20242023MonWedFriDecFebMarAprMayJunJulAugSepOctNovDec<br>今天是2024年3月19日，2024 年已经过去了 78 天<br>Dataview: No results to show for table query.<br>gpt链接 <a data-tooltip-position="top" aria-label="https://chat-shared2.zhile.io/?v=2" rel="noopener" class="external-link" href="https://chat-shared2.zhile.io/?v=2" target="_blank">ChatGPT</a><br><br><a href="https://www.bilibili.com" class="rich-link-card" target="_blank" rel="noopener">
	<div class="rich-link-image-container"><div style="background-image: url('https://static.hdslb.com/mobile/img/512.png')" class="rich-link-image"></div></div>
	<div class="rich-link-card-text heading-wrapper"><p class="rich-link-card-description"></p><p class="rich-link-href"></p><div class="heading-children"></div></div>
</a>
		
	
	
		
		
		哔哩哔哩（bilibili.com)是国内知名的视频弹幕网站，这里有及时的动漫新番，活跃的ACG氛围，有创意的Up主。大家可以在这里找到许多欢乐。
		
		
		https://www.bilibili.com
		
	<br>选中链接然后直接 ALT + I<br><br>直接打开网页<br>*<br><br>CTRL + L 就行
<a data-tooltip-position="top" aria-label="https://stentvessel.shop/index.html" rel="noopener" class="external-link" href="https://stentvessel.shop/index.html" target="_blank">最好用的VPN</a><br><br>大纲随便用咯<br><br><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV18a411r7mt/?spm_id_from=333.788.recommend_more_video.1&amp;vd_source=43e56318cc2b14ddb96ad55ad99d99df" rel="noopener" class="external-link" href="https://www.bilibili.com/video/BV18a411r7mt/?spm_id_from=333.788.recommend_more_video.1&amp;vd_source=43e56318cc2b14ddb96ad55ad99d99df" target="_blank">也许是B站最好的Obsidian新手教程！爆肝30天，一站式入门双向链接笔记软件_哔哩哔哩_bilibili</a><br><br>f5 直接生成大纲目录<br><br><a data-tooltip-position="top" aria-label="https://pan.baidu.com/disk/main#/index?category=all&amp;path=%2F%E5%85%B0%E6%96%B9%E9%BE%99" rel="noopener" class="external-link" href="https://pan.baidu.com/disk/main#/index?category=all&amp;path=%2F%E5%85%B0%E6%96%B9%E9%BE%99" target="_blank">Site Unreachable</a><br><br><br>]]></description><link>Obsidian插件教程.html</link><guid isPermaLink="false">Obsidian插件教程.md</guid><pubDate>Wed, 18 Oct 2023 01:30:58 GMT</pubDate></item></channel></rss>